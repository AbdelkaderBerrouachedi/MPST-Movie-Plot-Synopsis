{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import os\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import datetime as dt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14828 rows and 6 columns\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'F:\\mpst_full_data.csv', delimiter=',')\n",
    "nRow, nCol = df.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>title</th>\n",
       "      <th>plot_synopsis</th>\n",
       "      <th>tags</th>\n",
       "      <th>split</th>\n",
       "      <th>synopsis_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0057603</td>\n",
       "      <td>I tre volti della paura</td>\n",
       "      <td>Note: this synopsis is for the orginal Italian...</td>\n",
       "      <td>cult, horror, gothic, murder, atmospheric</td>\n",
       "      <td>train</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt1733125</td>\n",
       "      <td>Dungeons &amp; Dragons: The Book of Vile Darkness</td>\n",
       "      <td>Two thousand years ago, Nhagruul the Foul, a s...</td>\n",
       "      <td>violence</td>\n",
       "      <td>train</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0033045</td>\n",
       "      <td>The Shop Around the Corner</td>\n",
       "      <td>Matuschek's, a gift store in Budapest, is the ...</td>\n",
       "      <td>romantic</td>\n",
       "      <td>test</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0113862</td>\n",
       "      <td>Mr. Holland's Opus</td>\n",
       "      <td>Glenn Holland, not a morning person by anyone'...</td>\n",
       "      <td>inspiring, romantic, stupid, feel-good</td>\n",
       "      <td>train</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0086250</td>\n",
       "      <td>Scarface</td>\n",
       "      <td>In May 1980, a Cuban man named Tony Montana (A...</td>\n",
       "      <td>cruelty, murder, dramatic, cult, violence, atm...</td>\n",
       "      <td>val</td>\n",
       "      <td>imdb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     imdb_id                                          title  \\\n",
       "0  tt0057603                        I tre volti della paura   \n",
       "1  tt1733125  Dungeons & Dragons: The Book of Vile Darkness   \n",
       "2  tt0033045                     The Shop Around the Corner   \n",
       "3  tt0113862                             Mr. Holland's Opus   \n",
       "4  tt0086250                                       Scarface   \n",
       "\n",
       "                                       plot_synopsis  \\\n",
       "0  Note: this synopsis is for the orginal Italian...   \n",
       "1  Two thousand years ago, Nhagruul the Foul, a s...   \n",
       "2  Matuschek's, a gift store in Budapest, is the ...   \n",
       "3  Glenn Holland, not a morning person by anyone'...   \n",
       "4  In May 1980, a Cuban man named Tony Montana (A...   \n",
       "\n",
       "                                                tags  split synopsis_source  \n",
       "0          cult, horror, gothic, murder, atmospheric  train            imdb  \n",
       "1                                           violence  train            imdb  \n",
       "2                                           romantic   test            imdb  \n",
       "3             inspiring, romantic, stupid, feel-good  train            imdb  \n",
       "4  cruelty, murder, dramatic, cult, violence, atm...    val            imdb  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df.drop(columns=[\"split\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['imdb_id', 'title', 'plot_synopsis', 'tags', 'split',\n",
       "       'synopsis_source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['combined']=df['title']+\". \"+ df['plot_synopsis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Sequential()) == Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Convolution1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "# import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from plotly.offline import iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdd(x):\n",
    "    a=x.split()\n",
    "    return len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_synopsis = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in df['plot_synopsis'].values:\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_synopsis.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_plots']=preprocessed_synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['imdb_id', 'title', 'plot_synopsis', 'tags', 'split', 'synopsis_source',\n",
       "       'preprocessed_plots'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(x):\n",
    "    x=x.split(\",\")\n",
    "    nospace=[]\n",
    "    for item in x:\n",
    "        item=item.lstrip()\n",
    "        nospace.append(item)\n",
    "    return (\",\").join(nospace)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags']=df['tags'].apply(remove_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df.loc[df.split=='train']\n",
    "cv=df.loc[df.split==\"val\"]\n",
    "cv=cv.reset_index()\n",
    "train=train.reset_index()\n",
    "test=df.loc[df.split=='test']\n",
    "test=test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(\",\"), binary='true')\n",
    "y_train = vectorizer.fit_transform(train['tags']).toarray()\n",
    "y_test=vectorizer.transform(test['tags']).toarray()\n",
    "y_cv=vectorizer.transform(cv['tags']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "session_conf = tf.ConfigProto(\n",
    " intra_op_parallelism_threads=1, \n",
    "                        inter_op_parallelism_threads=1, \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['imdb_id', 'title', 'plot_synopsis', 'tags', 'split', 'synopsis_source',\n",
       "       'preprocessed_plots'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11406"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['plot_synopsis'].apply(gdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=Tokenizer()\n",
    "vect.fit_on_texts(train['plot_synopsis'])\n",
    "vocab_size = len(vect.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122195"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  779  4660 62208 ...     0     0     0]\n",
      " [   51  4481   143 ...     0     0     0]\n",
      " [ 3063   429   188 ...    75   140  6946]\n",
      " ...\n",
      " [  140  2717   539 ...     0     0     0]\n",
      " [ 5118  2731  3015 ...     0     0     0]\n",
      " [ 1269  2392  2530 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_docs_train = vect.texts_to_sequences(train['preprocessed_plots'])\n",
    "max_length = vocab_size\n",
    "padded_docs_train = pad_sequences(encoded_docs_train, maxlen=1200, padding='post')\n",
    "print(padded_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs_test = vect.texts_to_sequences(test['preprocessed_plots'])\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=1200, padding='post')\n",
    "encoded_docs_cv = vect.texts_to_sequences(cv['preprocessed_plots'])\n",
    "padded_docs_cv = pad_sequences(encoded_docs_cv, maxlen=1200, padding='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_conf = tf.ConfigProto(\n",
    " intra_op_parallelism_threads=1, \n",
    "                        inter_op_parallelism_threads=1, \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1micro(y_true, y_pred):\n",
    "    return tf.py_func(f1_score(y_true, y_pred,average='mirco'),tf.double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, Conv2D, MaxPooling1D, Dropout, Activation,GlobalMaxPool1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time()))\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1micro', verbose=0, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1200, 50)          4939100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1200, 50)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 71)                3621      \n",
      "=================================================================\n",
      "Total params: 4,942,721\n",
      "Trainable params: 4,942,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1micro', verbose=0, save_best_only=True, mode='max')\n",
    "model = Sequential()\n",
    "# model.add(Embedding(max_words, 20, input_length=maxlen))\n",
    "model.add(Embedding(vocab_size, output_dim=50, input_length=1200))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(71, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8540 samples, validate on 949 samples\n",
      "Epoch 1/5\n",
      "8540/8540 [==============================] - ETA: 7:30 - loss: 0.700 - ETA: 4:11 - loss: 0.699 - ETA: 3:04 - loss: 0.698 - ETA: 2:31 - loss: 0.697 - ETA: 2:11 - loss: 0.696 - ETA: 1:58 - loss: 0.695 - ETA: 1:48 - loss: 0.694 - ETA: 1:41 - loss: 0.693 - ETA: 1:35 - loss: 0.692 - ETA: 1:31 - loss: 0.691 - ETA: 1:28 - loss: 0.690 - ETA: 1:24 - loss: 0.689 - ETA: 1:22 - loss: 0.688 - ETA: 1:20 - loss: 0.688 - ETA: 1:18 - loss: 0.687 - ETA: 1:16 - loss: 0.686 - ETA: 1:15 - loss: 0.685 - ETA: 1:13 - loss: 0.684 - ETA: 1:12 - loss: 0.683 - ETA: 1:11 - loss: 0.682 - ETA: 1:10 - loss: 0.681 - ETA: 1:09 - loss: 0.680 - ETA: 1:08 - loss: 0.678 - ETA: 1:07 - loss: 0.677 - ETA: 1:06 - loss: 0.676 - ETA: 1:06 - loss: 0.675 - ETA: 1:05 - loss: 0.674 - ETA: 1:04 - loss: 0.673 - ETA: 1:03 - loss: 0.672 - ETA: 1:03 - loss: 0.671 - ETA: 1:02 - loss: 0.670 - ETA: 1:01 - loss: 0.669 - ETA: 1:01 - loss: 0.668 - ETA: 1:00 - loss: 0.667 - ETA: 59s - loss: 0.666 - ETA: 59s - loss: 0.66 - ETA: 58s - loss: 0.66 - ETA: 58s - loss: 0.66 - ETA: 57s - loss: 0.66 - ETA: 57s - loss: 0.66 - ETA: 56s - loss: 0.65 - ETA: 56s - loss: 0.65 - ETA: 55s - loss: 0.65 - ETA: 55s - loss: 0.65 - ETA: 54s - loss: 0.65 - ETA: 54s - loss: 0.65 - ETA: 53s - loss: 0.65 - ETA: 53s - loss: 0.64 - ETA: 53s - loss: 0.64 - ETA: 52s - loss: 0.64 - ETA: 52s - loss: 0.64 - ETA: 51s - loss: 0.64 - ETA: 51s - loss: 0.64 - ETA: 51s - loss: 0.64 - ETA: 50s - loss: 0.64 - ETA: 50s - loss: 0.63 - ETA: 49s - loss: 0.63 - ETA: 49s - loss: 0.63 - ETA: 49s - loss: 0.63 - ETA: 48s - loss: 0.63 - ETA: 48s - loss: 0.63 - ETA: 48s - loss: 0.63 - ETA: 47s - loss: 0.62 - ETA: 47s - loss: 0.62 - ETA: 47s - loss: 0.62 - ETA: 46s - loss: 0.62 - ETA: 46s - loss: 0.62 - ETA: 46s - loss: 0.62 - ETA: 45s - loss: 0.61 - ETA: 45s - loss: 0.61 - ETA: 45s - loss: 0.61 - ETA: 44s - loss: 0.61 - ETA: 44s - loss: 0.61 - ETA: 44s - loss: 0.61 - ETA: 44s - loss: 0.60 - ETA: 43s - loss: 0.60 - ETA: 43s - loss: 0.60 - ETA: 43s - loss: 0.60 - ETA: 42s - loss: 0.60 - ETA: 42s - loss: 0.60 - ETA: 42s - loss: 0.59 - ETA: 42s - loss: 0.59 - ETA: 41s - loss: 0.59 - ETA: 41s - loss: 0.59 - ETA: 41s - loss: 0.59 - ETA: 41s - loss: 0.58 - ETA: 40s - loss: 0.58 - ETA: 40s - loss: 0.58 - ETA: 40s - loss: 0.58 - ETA: 39s - loss: 0.58 - ETA: 39s - loss: 0.58 - ETA: 39s - loss: 0.57 - ETA: 39s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.57 - ETA: 38s - loss: 0.56 - ETA: 37s - loss: 0.56 - ETA: 37s - loss: 0.56 - ETA: 37s - loss: 0.56 - ETA: 37s - loss: 0.56 - ETA: 36s - loss: 0.56 - ETA: 36s - loss: 0.55 - ETA: 36s - loss: 0.55 - ETA: 35s - loss: 0.55 - ETA: 35s - loss: 0.55 - ETA: 35s - loss: 0.55 - ETA: 35s - loss: 0.54 - ETA: 35s - loss: 0.54 - ETA: 34s - loss: 0.54 - ETA: 34s - loss: 0.54 - ETA: 34s - loss: 0.54 - ETA: 34s - loss: 0.53 - ETA: 33s - loss: 0.53 - ETA: 33s - loss: 0.53 - ETA: 33s - loss: 0.53 - ETA: 33s - loss: 0.53 - ETA: 32s - loss: 0.53 - ETA: 32s - loss: 0.52 - ETA: 32s - loss: 0.52 - ETA: 32s - loss: 0.52 - ETA: 31s - loss: 0.52 - ETA: 31s - loss: 0.52 - ETA: 31s - loss: 0.51 - ETA: 31s - loss: 0.51 - ETA: 30s - loss: 0.51 - ETA: 30s - loss: 0.51 - ETA: 30s - loss: 0.51 - ETA: 30s - loss: 0.51 - ETA: 29s - loss: 0.50 - ETA: 29s - loss: 0.50 - ETA: 29s - loss: 0.50 - ETA: 29s - loss: 0.50 - ETA: 28s - loss: 0.50 - ETA: 28s - loss: 0.49 - ETA: 28s - loss: 0.49 - ETA: 28s - loss: 0.49 - ETA: 28s - loss: 0.49 - ETA: 27s - loss: 0.49 - ETA: 27s - loss: 0.49 - ETA: 27s - loss: 0.48 - ETA: 27s - loss: 0.48 - ETA: 26s - loss: 0.48 - ETA: 26s - loss: 0.48 - ETA: 26s - loss: 0.48 - ETA: 26s - loss: 0.48 - ETA: 25s - loss: 0.47 - ETA: 25s - loss: 0.47 - ETA: 25s - loss: 0.47 - ETA: 25s - loss: 0.47 - ETA: 25s - loss: 0.47 - ETA: 24s - loss: 0.47 - ETA: 24s - loss: 0.46 - ETA: 24s - loss: 0.46 - ETA: 24s - loss: 0.46 - ETA: 23s - loss: 0.46 - ETA: 23s - loss: 0.46 - ETA: 23s - loss: 0.46 - ETA: 23s - loss: 0.45 - ETA: 23s - loss: 0.45 - ETA: 22s - loss: 0.45 - ETA: 22s - loss: 0.45 - ETA: 22s - loss: 0.45 - ETA: 22s - loss: 0.45 - ETA: 21s - loss: 0.45 - ETA: 21s - loss: 0.44 - ETA: 21s - loss: 0.44 - ETA: 21s - loss: 0.44 - ETA: 21s - loss: 0.44 - ETA: 20s - loss: 0.44 - ETA: 20s - loss: 0.44 - ETA: 20s - loss: 0.44 - ETA: 20s - loss: 0.43 - ETA: 19s - loss: 0.43 - ETA: 19s - loss: 0.43 - ETA: 19s - loss: 0.43 - ETA: 19s - loss: 0.43 - ETA: 19s - loss: 0.43 - ETA: 18s - loss: 0.43 - ETA: 18s - loss: 0.43 - ETA: 18s - loss: 0.42 - ETA: 18s - loss: 0.42 - ETA: 17s - loss: 0.42 - ETA: 17s - loss: 0.42 - ETA: 17s - loss: 0.42 - ETA: 17s - loss: 0.42 - ETA: 17s - loss: 0.42 - ETA: 16s - loss: 0.41 - ETA: 16s - loss: 0.41 - ETA: 16s - loss: 0.41 - ETA: 16s - loss: 0.41 - ETA: 16s - loss: 0.41 - ETA: 15s - loss: 0.41 - ETA: 15s - loss: 0.41 - ETA: 15s - loss: 0.41 - ETA: 15s - loss: 0.41 - ETA: 14s - loss: 0.40 - ETA: 14s - loss: 0.40 - ETA: 14s - loss: 0.40 - ETA: 14s - loss: 0.40 - ETA: 14s - loss: 0.40 - ETA: 13s - loss: 0.40 - ETA: 13s - loss: 0.40 - ETA: 13s - loss: 0.40 - ETA: 13s - loss: 0.39 - ETA: 12s - loss: 0.39 - ETA: 12s - loss: 0.39 - ETA: 12s - loss: 0.39 - ETA: 12s - loss: 0.39 - ETA: 12s - loss: 0.39 - ETA: 11s - loss: 0.39 - ETA: 11s - loss: 0.39 - ETA: 11s - loss: 0.39 - ETA: 11s - loss: 0.39 - ETA: 11s - loss: 0.38 - ETA: 10s - loss: 0.38 - ETA: 10s - loss: 0.38 - ETA: 10s - loss: 0.38 - ETA: 10s - loss: 0.38 - ETA: 9s - loss: 0.3840 - ETA: 9s - loss: 0.383 - ETA: 9s - loss: 0.382 - ETA: 9s - loss: 0.381 - ETA: 9s - loss: 0.380 - ETA: 8s - loss: 0.379 - ETA: 8s - loss: 0.378 - ETA: 8s - loss: 0.377 - ETA: 8s - loss: 0.376 - ETA: 8s - loss: 0.375 - ETA: 7s - loss: 0.374 - ETA: 7s - loss: 0.373 - ETA: 7s - loss: 0.372 - ETA: 7s - loss: 0.371 - ETA: 6s - loss: 0.370 - ETA: 6s - loss: 0.369 - ETA: 6s - loss: 0.368 - ETA: 6s - loss: 0.368 - ETA: 6s - loss: 0.367 - ETA: 5s - loss: 0.366 - ETA: 5s - loss: 0.365 - ETA: 5s - loss: 0.364 - ETA: 5s - loss: 0.363 - ETA: 5s - loss: 0.362 - ETA: 4s - loss: 0.361 - ETA: 4s - loss: 0.361 - ETA: 4s - loss: 0.360 - ETA: 4s - loss: 0.359 - ETA: 4s - loss: 0.358 - ETA: 3s - loss: 0.357 - ETA: 3s - loss: 0.356 - ETA: 3s - loss: 0.356 - ETA: 3s - loss: 0.355 - ETA: 2s - loss: 0.354 - ETA: 2s - loss: 0.353 - ETA: 2s - loss: 0.352 - ETA: 2s - loss: 0.352 - ETA: 2s - loss: 0.351 - ETA: 1s - loss: 0.350 - ETA: 1s - loss: 0.349 - ETA: 1s - loss: 0.349 - ETA: 1s - loss: 0.348 - ETA: 1s - loss: 0.347 - ETA: 0s - loss: 0.347 - ETA: 0s - loss: 0.346 - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.344 - 57s 7ms/step - loss: 0.3442 - val_loss: 0.1409\n",
      "Epoch 2/5\n",
      "8540/8540 [==============================] - ETA: 53s - loss: 0.15 - ETA: 55s - loss: 0.15 - ETA: 56s - loss: 0.14 - ETA: 55s - loss: 0.14 - ETA: 55s - loss: 0.14 - ETA: 55s - loss: 0.14 - ETA: 55s - loss: 0.15 - ETA: 55s - loss: 0.15 - ETA: 54s - loss: 0.15 - ETA: 54s - loss: 0.15 - ETA: 54s - loss: 0.15 - ETA: 54s - loss: 0.15 - ETA: 54s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 52s - loss: 0.15 - ETA: 52s - loss: 0.15 - ETA: 52s - loss: 0.15 - ETA: 52s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 51s - loss: 0.15 - ETA: 50s - loss: 0.15 - ETA: 50s - loss: 0.15 - ETA: 50s - loss: 0.15 - ETA: 50s - loss: 0.15 - ETA: 50s - loss: 0.15 - ETA: 50s - loss: 0.15 - ETA: 49s - loss: 0.15 - ETA: 49s - loss: 0.15 - ETA: 49s - loss: 0.15 - ETA: 48s - loss: 0.15 - ETA: 48s - loss: 0.15 - ETA: 48s - loss: 0.15 - ETA: 48s - loss: 0.15 - ETA: 47s - loss: 0.15 - ETA: 47s - loss: 0.15 - ETA: 47s - loss: 0.15 - ETA: 46s - loss: 0.15 - ETA: 46s - loss: 0.15 - ETA: 46s - loss: 0.15 - ETA: 46s - loss: 0.15 - ETA: 45s - loss: 0.15 - ETA: 45s - loss: 0.15 - ETA: 45s - loss: 0.15 - ETA: 45s - loss: 0.15 - ETA: 44s - loss: 0.15 - ETA: 44s - loss: 0.15 - ETA: 44s - loss: 0.15 - ETA: 44s - loss: 0.15 - ETA: 43s - loss: 0.15 - ETA: 43s - loss: 0.15 - ETA: 43s - loss: 0.15 - ETA: 42s - loss: 0.15 - ETA: 42s - loss: 0.15 - ETA: 42s - loss: 0.15 - ETA: 42s - loss: 0.15 - ETA: 42s - loss: 0.15 - ETA: 41s - loss: 0.15 - ETA: 41s - loss: 0.15 - ETA: 41s - loss: 0.15 - ETA: 41s - loss: 0.15 - ETA: 40s - loss: 0.15 - ETA: 40s - loss: 0.15 - ETA: 40s - loss: 0.15 - ETA: 40s - loss: 0.15 - ETA: 39s - loss: 0.15 - ETA: 39s - loss: 0.15 - ETA: 39s - loss: 0.15 - ETA: 39s - loss: 0.15 - ETA: 38s - loss: 0.15 - ETA: 38s - loss: 0.15 - ETA: 38s - loss: 0.15 - ETA: 38s - loss: 0.15 - ETA: 37s - loss: 0.15 - ETA: 37s - loss: 0.15 - ETA: 37s - loss: 0.15 - ETA: 37s - loss: 0.15 - ETA: 37s - loss: 0.15 - ETA: 36s - loss: 0.15 - ETA: 36s - loss: 0.15 - ETA: 36s - loss: 0.15 - ETA: 36s - loss: 0.15 - ETA: 35s - loss: 0.15 - ETA: 35s - loss: 0.15 - ETA: 35s - loss: 0.15 - ETA: 35s - loss: 0.15 - ETA: 34s - loss: 0.15 - ETA: 34s - loss: 0.15 - ETA: 34s - loss: 0.15 - ETA: 34s - loss: 0.15 - ETA: 34s - loss: 0.15 - ETA: 33s - loss: 0.15 - ETA: 33s - loss: 0.15 - ETA: 33s - loss: 0.15 - ETA: 33s - loss: 0.15 - ETA: 32s - loss: 0.15 - ETA: 32s - loss: 0.15 - ETA: 32s - loss: 0.15 - ETA: 32s - loss: 0.15 - ETA: 32s - loss: 0.15 - ETA: 31s - loss: 0.15 - ETA: 31s - loss: 0.15 - ETA: 31s - loss: 0.15 - ETA: 31s - loss: 0.15 - ETA: 30s - loss: 0.15 - ETA: 30s - loss: 0.15 - ETA: 30s - loss: 0.15 - ETA: 30s - loss: 0.15 - ETA: 30s - loss: 0.15 - ETA: 29s - loss: 0.15 - ETA: 29s - loss: 0.15 - ETA: 29s - loss: 0.15 - ETA: 29s - loss: 0.15 - ETA: 29s - loss: 0.15 - ETA: 28s - loss: 0.15 - ETA: 28s - loss: 0.15 - ETA: 28s - loss: 0.15 - ETA: 28s - loss: 0.15 - ETA: 27s - loss: 0.15 - ETA: 27s - loss: 0.15 - ETA: 27s - loss: 0.15 - ETA: 27s - loss: 0.15 - ETA: 27s - loss: 0.15 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.15 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.15 - ETA: 26s - loss: 0.15 - ETA: 25s - loss: 0.15 - ETA: 25s - loss: 0.15 - ETA: 25s - loss: 0.15 - ETA: 25s - loss: 0.15 - ETA: 25s - loss: 0.15 - ETA: 24s - loss: 0.15 - ETA: 24s - loss: 0.15 - ETA: 24s - loss: 0.15 - ETA: 24s - loss: 0.15 - ETA: 23s - loss: 0.15 - ETA: 23s - loss: 0.15 - ETA: 23s - loss: 0.15 - ETA: 23s - loss: 0.15 - ETA: 23s - loss: 0.15 - ETA: 22s - loss: 0.15 - ETA: 22s - loss: 0.15 - ETA: 22s - loss: 0.15 - ETA: 22s - loss: 0.15 - ETA: 22s - loss: 0.15 - ETA: 21s - loss: 0.15 - ETA: 21s - loss: 0.15 - ETA: 21s - loss: 0.15 - ETA: 21s - loss: 0.15 - ETA: 20s - loss: 0.15 - ETA: 20s - loss: 0.15 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.15 - ETA: 16s - loss: 0.15 - ETA: 16s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 9s - loss: 0.1484 - ETA: 9s - loss: 0.148 - ETA: 9s - loss: 0.148 - ETA: 9s - loss: 0.148 - ETA: 9s - loss: 0.148 - ETA: 8s - loss: 0.148 - ETA: 8s - loss: 0.148 - ETA: 8s - loss: 0.148 - ETA: 8s - loss: 0.148 - ETA: 7s - loss: 0.148 - ETA: 7s - loss: 0.148 - ETA: 7s - loss: 0.148 - ETA: 7s - loss: 0.148 - ETA: 7s - loss: 0.148 - ETA: 6s - loss: 0.148 - ETA: 6s - loss: 0.148 - ETA: 6s - loss: 0.148 - ETA: 6s - loss: 0.148 - ETA: 6s - loss: 0.147 - ETA: 5s - loss: 0.147 - ETA: 5s - loss: 0.147 - ETA: 5s - loss: 0.147 - ETA: 5s - loss: 0.147 - ETA: 5s - loss: 0.148 - ETA: 4s - loss: 0.147 - ETA: 4s - loss: 0.147 - ETA: 4s - loss: 0.147 - ETA: 4s - loss: 0.147 - ETA: 3s - loss: 0.147 - ETA: 3s - loss: 0.147 - ETA: 3s - loss: 0.147 - ETA: 3s - loss: 0.147 - ETA: 3s - loss: 0.147 - ETA: 2s - loss: 0.147 - ETA: 2s - loss: 0.147 - ETA: 2s - loss: 0.147 - ETA: 2s - loss: 0.147 - ETA: 2s - loss: 0.147 - ETA: 1s - loss: 0.147 - ETA: 1s - loss: 0.147 - ETA: 1s - loss: 0.147 - ETA: 1s - loss: 0.147 - ETA: 1s - loss: 0.147 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.147 - 56s 7ms/step - loss: 0.1474 - val_loss: 0.1209\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540/8540 [==============================] - ETA: 57s - loss: 0.16 - ETA: 54s - loss: 0.15 - ETA: 53s - loss: 0.16 - ETA: 53s - loss: 0.16 - ETA: 54s - loss: 0.15 - ETA: 54s - loss: 0.15 - ETA: 54s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 52s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 9s - loss: 0.1438 - ETA: 9s - loss: 0.143 - ETA: 9s - loss: 0.143 - ETA: 9s - loss: 0.144 - ETA: 9s - loss: 0.144 - ETA: 8s - loss: 0.144 - ETA: 8s - loss: 0.144 - ETA: 8s - loss: 0.144 - ETA: 8s - loss: 0.143 - ETA: 8s - loss: 0.143 - ETA: 7s - loss: 0.143 - ETA: 7s - loss: 0.143 - ETA: 7s - loss: 0.144 - ETA: 7s - loss: 0.143 - ETA: 7s - loss: 0.143 - ETA: 6s - loss: 0.143 - ETA: 6s - loss: 0.143 - ETA: 6s - loss: 0.143 - ETA: 6s - loss: 0.143 - ETA: 5s - loss: 0.143 - ETA: 5s - loss: 0.143 - ETA: 5s - loss: 0.143 - ETA: 5s - loss: 0.144 - ETA: 5s - loss: 0.143 - ETA: 4s - loss: 0.143 - ETA: 4s - loss: 0.143 - ETA: 4s - loss: 0.143 - ETA: 4s - loss: 0.143 - ETA: 4s - loss: 0.143 - ETA: 3s - loss: 0.143 - ETA: 3s - loss: 0.143 - ETA: 3s - loss: 0.143 - ETA: 3s - loss: 0.143 - ETA: 3s - loss: 0.143 - ETA: 2s - loss: 0.143 - ETA: 2s - loss: 0.143 - ETA: 2s - loss: 0.143 - ETA: 2s - loss: 0.143 - ETA: 2s - loss: 0.143 - ETA: 1s - loss: 0.143 - ETA: 1s - loss: 0.143 - ETA: 1s - loss: 0.143 - ETA: 1s - loss: 0.143 - ETA: 1s - loss: 0.143 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.143 - 56s 7ms/step - loss: 0.1435 - val_loss: 0.1190\n",
      "Epoch 4/5\n",
      "8540/8540 [==============================] - ETA: 53s - loss: 0.14 - ETA: 54s - loss: 0.14 - ETA: 54s - loss: 0.13 - ETA: 53s - loss: 0.13 - ETA: 53s - loss: 0.13 - ETA: 53s - loss: 0.13 - ETA: 53s - loss: 0.13 - ETA: 52s - loss: 0.13 - ETA: 52s - loss: 0.13 - ETA: 52s - loss: 0.13 - ETA: 52s - loss: 0.13 - ETA: 52s - loss: 0.13 - ETA: 52s - loss: 0.13 - ETA: 51s - loss: 0.13 - ETA: 51s - loss: 0.13 - ETA: 51s - loss: 0.13 - ETA: 51s - loss: 0.13 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 47s - loss: 0.13 - ETA: 47s - loss: 0.13 - ETA: 47s - loss: 0.13 - ETA: 46s - loss: 0.13 - ETA: 46s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 46s - loss: 0.13 - ETA: 46s - loss: 0.13 - ETA: 45s - loss: 0.13 - ETA: 45s - loss: 0.13 - ETA: 45s - loss: 0.13 - ETA: 45s - loss: 0.13 - ETA: 45s - loss: 0.13 - ETA: 44s - loss: 0.13 - ETA: 44s - loss: 0.14 - ETA: 44s - loss: 0.13 - ETA: 44s - loss: 0.14 - ETA: 44s - loss: 0.13 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 9s - loss: 0.1420 - ETA: 9s - loss: 0.142 - ETA: 9s - loss: 0.142 - ETA: 9s - loss: 0.142 - ETA: 9s - loss: 0.142 - ETA: 8s - loss: 0.142 - ETA: 8s - loss: 0.142 - ETA: 8s - loss: 0.142 - ETA: 8s - loss: 0.142 - ETA: 8s - loss: 0.142 - ETA: 7s - loss: 0.142 - ETA: 7s - loss: 0.142 - ETA: 7s - loss: 0.142 - ETA: 7s - loss: 0.142 - ETA: 6s - loss: 0.142 - ETA: 6s - loss: 0.142 - ETA: 6s - loss: 0.142 - ETA: 6s - loss: 0.142 - ETA: 6s - loss: 0.142 - ETA: 5s - loss: 0.142 - ETA: 5s - loss: 0.142 - ETA: 5s - loss: 0.142 - ETA: 5s - loss: 0.142 - ETA: 5s - loss: 0.142 - ETA: 4s - loss: 0.142 - ETA: 4s - loss: 0.142 - ETA: 4s - loss: 0.142 - ETA: 4s - loss: 0.142 - ETA: 4s - loss: 0.142 - ETA: 3s - loss: 0.142 - ETA: 3s - loss: 0.142 - ETA: 3s - loss: 0.142 - ETA: 3s - loss: 0.142 - ETA: 3s - loss: 0.142 - ETA: 2s - loss: 0.141 - ETA: 2s - loss: 0.142 - ETA: 2s - loss: 0.141 - ETA: 2s - loss: 0.141 - ETA: 2s - loss: 0.141 - ETA: 1s - loss: 0.141 - ETA: 1s - loss: 0.141 - ETA: 1s - loss: 0.141 - ETA: 1s - loss: 0.141 - ETA: 1s - loss: 0.141 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.142 - 55s 6ms/step - loss: 0.1421 - val_loss: 0.1180\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540/8540 [==============================] - ETA: 54s - loss: 0.16 - ETA: 53s - loss: 0.14 - ETA: 53s - loss: 0.15 - ETA: 52s - loss: 0.15 - ETA: 52s - loss: 0.15 - ETA: 52s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 53s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 9s - loss: 0.1410 - ETA: 9s - loss: 0.141 - ETA: 9s - loss: 0.141 - ETA: 9s - loss: 0.140 - ETA: 9s - loss: 0.141 - ETA: 8s - loss: 0.141 - ETA: 8s - loss: 0.141 - ETA: 8s - loss: 0.141 - ETA: 8s - loss: 0.140 - ETA: 8s - loss: 0.140 - ETA: 7s - loss: 0.140 - ETA: 7s - loss: 0.140 - ETA: 7s - loss: 0.140 - ETA: 7s - loss: 0.140 - ETA: 7s - loss: 0.140 - ETA: 6s - loss: 0.140 - ETA: 6s - loss: 0.140 - ETA: 6s - loss: 0.140 - ETA: 6s - loss: 0.140 - ETA: 6s - loss: 0.140 - ETA: 5s - loss: 0.140 - ETA: 5s - loss: 0.140 - ETA: 5s - loss: 0.140 - ETA: 5s - loss: 0.140 - ETA: 5s - loss: 0.140 - ETA: 4s - loss: 0.140 - ETA: 4s - loss: 0.140 - ETA: 4s - loss: 0.140 - ETA: 4s - loss: 0.140 - ETA: 4s - loss: 0.140 - ETA: 3s - loss: 0.140 - ETA: 3s - loss: 0.140 - ETA: 3s - loss: 0.140 - ETA: 3s - loss: 0.140 - ETA: 3s - loss: 0.140 - ETA: 2s - loss: 0.140 - ETA: 2s - loss: 0.140 - ETA: 2s - loss: 0.140 - ETA: 2s - loss: 0.140 - ETA: 2s - loss: 0.140 - ETA: 1s - loss: 0.140 - ETA: 1s - loss: 0.140 - ETA: 1s - loss: 0.140 - ETA: 1s - loss: 0.140 - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.140 - 55s 6ms/step - loss: 0.1408 - val_loss: 0.1166\n"
     ]
    }
   ],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1micro])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    EarlyStopping(patience=4),\n",
    "    ModelCheckpoint(filepath='model-simple1.hdf5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(padded_docs_train, y_train,\n",
    "                    class_weight='balanced',\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks)\n",
    "# history=model.fit(padded_docs_train, y_train,validation_data=(padded_docs_cv,y_cv), epochs =5,batch_size=32,callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict([padded_docs_test])\n",
    "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.2023, Recall: 0.5908, F1-measure: 0.3014\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2645, Recall: 0.4459, F1-measure: 0.3320\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3940, Recall: 0.2172, F1-measure: 0.2801\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5717, Recall: 0.1070, F1-measure: 0.1802\n",
      "Micro-average quality numbers\n",
      "Precision: 0.6348, Recall: 0.0243, F1-measure: 0.0468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for val in thresholds:\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1200, 50)          4939100   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1200, 50)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 71)                3621      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 71)                0         \n",
      "=================================================================\n",
      "Total params: 4,942,721\n",
      "Trainable params: 4,942,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8540 samples, validate on 949 samples\n",
      "Epoch 1/5\n",
      "8540/8540 [==============================] - ETA: 4:59 - loss: 0.952 - ETA: 2:57 - loss: 0.951 - ETA: 2:17 - loss: 0.952 - ETA: 1:56 - loss: 0.951 - ETA: 1:43 - loss: 0.950 - ETA: 1:35 - loss: 0.950 - ETA: 1:29 - loss: 0.949 - ETA: 1:24 - loss: 0.948 - ETA: 1:20 - loss: 0.948 - ETA: 1:17 - loss: 0.947 - ETA: 1:15 - loss: 0.947 - ETA: 1:12 - loss: 0.947 - ETA: 1:10 - loss: 0.946 - ETA: 1:09 - loss: 0.946 - ETA: 1:08 - loss: 0.946 - ETA: 1:07 - loss: 0.945 - ETA: 1:05 - loss: 0.945 - ETA: 1:04 - loss: 0.944 - ETA: 1:03 - loss: 0.944 - ETA: 1:03 - loss: 0.944 - ETA: 1:02 - loss: 0.943 - ETA: 1:01 - loss: 0.943 - ETA: 1:01 - loss: 0.942 - ETA: 1:00 - loss: 0.942 - ETA: 59s - loss: 0.942 - ETA: 59s - loss: 0.94 - ETA: 58s - loss: 0.94 - ETA: 57s - loss: 0.94 - ETA: 57s - loss: 0.94 - ETA: 56s - loss: 0.94 - ETA: 56s - loss: 0.94 - ETA: 56s - loss: 0.93 - ETA: 55s - loss: 0.93 - ETA: 55s - loss: 0.93 - ETA: 54s - loss: 0.93 - ETA: 54s - loss: 0.93 - ETA: 53s - loss: 0.93 - ETA: 53s - loss: 0.93 - ETA: 53s - loss: 0.93 - ETA: 52s - loss: 0.93 - ETA: 52s - loss: 0.93 - ETA: 52s - loss: 0.93 - ETA: 51s - loss: 0.93 - ETA: 51s - loss: 0.93 - ETA: 51s - loss: 0.93 - ETA: 50s - loss: 0.93 - ETA: 50s - loss: 0.93 - ETA: 50s - loss: 0.93 - ETA: 49s - loss: 0.93 - ETA: 49s - loss: 0.93 - ETA: 49s - loss: 0.93 - ETA: 48s - loss: 0.93 - ETA: 48s - loss: 0.93 - ETA: 48s - loss: 0.92 - ETA: 48s - loss: 0.92 - ETA: 47s - loss: 0.92 - ETA: 47s - loss: 0.92 - ETA: 47s - loss: 0.92 - ETA: 47s - loss: 0.92 - ETA: 46s - loss: 0.92 - ETA: 46s - loss: 0.92 - ETA: 46s - loss: 0.92 - ETA: 46s - loss: 0.92 - ETA: 45s - loss: 0.92 - ETA: 45s - loss: 0.92 - ETA: 45s - loss: 0.92 - ETA: 45s - loss: 0.92 - ETA: 44s - loss: 0.92 - ETA: 44s - loss: 0.92 - ETA: 44s - loss: 0.92 - ETA: 43s - loss: 0.91 - ETA: 43s - loss: 0.91 - ETA: 43s - loss: 0.91 - ETA: 43s - loss: 0.91 - ETA: 42s - loss: 0.91 - ETA: 42s - loss: 0.91 - ETA: 42s - loss: 0.91 - ETA: 42s - loss: 0.91 - ETA: 41s - loss: 0.91 - ETA: 41s - loss: 0.91 - ETA: 41s - loss: 0.91 - ETA: 41s - loss: 0.91 - ETA: 40s - loss: 0.91 - ETA: 40s - loss: 0.91 - ETA: 40s - loss: 0.90 - ETA: 40s - loss: 0.90 - ETA: 39s - loss: 0.90 - ETA: 39s - loss: 0.90 - ETA: 39s - loss: 0.90 - ETA: 39s - loss: 0.90 - ETA: 39s - loss: 0.90 - ETA: 38s - loss: 0.90 - ETA: 38s - loss: 0.90 - ETA: 38s - loss: 0.90 - ETA: 38s - loss: 0.90 - ETA: 37s - loss: 0.90 - ETA: 37s - loss: 0.90 - ETA: 37s - loss: 0.89 - ETA: 37s - loss: 0.89 - ETA: 36s - loss: 0.89 - ETA: 36s - loss: 0.89 - ETA: 36s - loss: 0.89 - ETA: 36s - loss: 0.89 - ETA: 35s - loss: 0.89 - ETA: 35s - loss: 0.89 - ETA: 35s - loss: 0.89 - ETA: 35s - loss: 0.89 - ETA: 34s - loss: 0.89 - ETA: 34s - loss: 0.89 - ETA: 34s - loss: 0.88 - ETA: 34s - loss: 0.88 - ETA: 34s - loss: 0.88 - ETA: 33s - loss: 0.88 - ETA: 33s - loss: 0.88 - ETA: 33s - loss: 0.88 - ETA: 33s - loss: 0.88 - ETA: 32s - loss: 0.88 - ETA: 32s - loss: 0.88 - ETA: 32s - loss: 0.88 - ETA: 32s - loss: 0.88 - ETA: 31s - loss: 0.88 - ETA: 31s - loss: 0.88 - ETA: 31s - loss: 0.87 - ETA: 31s - loss: 0.87 - ETA: 31s - loss: 0.87 - ETA: 30s - loss: 0.87 - ETA: 30s - loss: 0.87 - ETA: 30s - loss: 0.87 - ETA: 30s - loss: 0.87 - ETA: 30s - loss: 0.87 - ETA: 29s - loss: 0.87 - ETA: 29s - loss: 0.87 - ETA: 29s - loss: 0.87 - ETA: 29s - loss: 0.87 - ETA: 28s - loss: 0.86 - ETA: 28s - loss: 0.86 - ETA: 28s - loss: 0.86 - ETA: 28s - loss: 0.86 - ETA: 27s - loss: 0.86 - ETA: 27s - loss: 0.86 - ETA: 27s - loss: 0.86 - ETA: 27s - loss: 0.86 - ETA: 27s - loss: 0.86 - ETA: 26s - loss: 0.86 - ETA: 26s - loss: 0.86 - ETA: 26s - loss: 0.86 - ETA: 26s - loss: 0.86 - ETA: 26s - loss: 0.85 - ETA: 25s - loss: 0.85 - ETA: 25s - loss: 0.85 - ETA: 25s - loss: 0.85 - ETA: 25s - loss: 0.85 - ETA: 24s - loss: 0.85 - ETA: 24s - loss: 0.85 - ETA: 24s - loss: 0.85 - ETA: 24s - loss: 0.85 - ETA: 24s - loss: 0.85 - ETA: 23s - loss: 0.85 - ETA: 23s - loss: 0.85 - ETA: 23s - loss: 0.85 - ETA: 23s - loss: 0.85 - ETA: 23s - loss: 0.84 - ETA: 22s - loss: 0.84 - ETA: 22s - loss: 0.84 - ETA: 22s - loss: 0.84 - ETA: 22s - loss: 0.84 - ETA: 21s - loss: 0.84 - ETA: 21s - loss: 0.84 - ETA: 21s - loss: 0.84 - ETA: 21s - loss: 0.84 - ETA: 21s - loss: 0.84 - ETA: 20s - loss: 0.84 - ETA: 20s - loss: 0.84 - ETA: 20s - loss: 0.84 - ETA: 20s - loss: 0.84 - ETA: 19s - loss: 0.84 - ETA: 19s - loss: 0.83 - ETA: 19s - loss: 0.83 - ETA: 19s - loss: 0.83 - ETA: 18s - loss: 0.83 - ETA: 18s - loss: 0.83 - ETA: 18s - loss: 0.83 - ETA: 18s - loss: 0.83 - ETA: 18s - loss: 0.83 - ETA: 17s - loss: 0.83 - ETA: 17s - loss: 0.83 - ETA: 17s - loss: 0.83 - ETA: 17s - loss: 0.83 - ETA: 17s - loss: 0.83 - ETA: 16s - loss: 0.83 - ETA: 16s - loss: 0.83 - ETA: 16s - loss: 0.83 - ETA: 16s - loss: 0.82 - ETA: 15s - loss: 0.82 - ETA: 15s - loss: 0.82 - ETA: 15s - loss: 0.82 - ETA: 15s - loss: 0.82 - ETA: 15s - loss: 0.82 - ETA: 14s - loss: 0.82 - ETA: 14s - loss: 0.82 - ETA: 14s - loss: 0.82 - ETA: 14s - loss: 0.82 - ETA: 13s - loss: 0.82 - ETA: 13s - loss: 0.82 - ETA: 13s - loss: 0.82 - ETA: 13s - loss: 0.82 - ETA: 13s - loss: 0.82 - ETA: 12s - loss: 0.82 - ETA: 12s - loss: 0.82 - ETA: 12s - loss: 0.82 - ETA: 12s - loss: 0.82 - ETA: 11s - loss: 0.81 - ETA: 11s - loss: 0.81 - ETA: 11s - loss: 0.81 - ETA: 11s - loss: 0.81 - ETA: 11s - loss: 0.81 - ETA: 10s - loss: 0.81 - ETA: 10s - loss: 0.81 - ETA: 10s - loss: 0.81 - ETA: 10s - loss: 0.81 - ETA: 9s - loss: 0.8153 - ETA: 9s - loss: 0.814 - ETA: 9s - loss: 0.814 - ETA: 9s - loss: 0.813 - ETA: 9s - loss: 0.813 - ETA: 8s - loss: 0.813 - ETA: 8s - loss: 0.812 - ETA: 8s - loss: 0.812 - ETA: 8s - loss: 0.811 - ETA: 8s - loss: 0.811 - ETA: 7s - loss: 0.810 - ETA: 7s - loss: 0.810 - ETA: 7s - loss: 0.809 - ETA: 7s - loss: 0.809 - ETA: 6s - loss: 0.809 - ETA: 6s - loss: 0.808 - ETA: 6s - loss: 0.808 - ETA: 6s - loss: 0.807 - ETA: 6s - loss: 0.807 - ETA: 5s - loss: 0.806 - ETA: 5s - loss: 0.806 - ETA: 5s - loss: 0.806 - ETA: 5s - loss: 0.805 - ETA: 4s - loss: 0.805 - ETA: 4s - loss: 0.804 - ETA: 4s - loss: 0.804 - ETA: 4s - loss: 0.804 - ETA: 4s - loss: 0.803 - ETA: 3s - loss: 0.803 - ETA: 3s - loss: 0.802 - ETA: 3s - loss: 0.802 - ETA: 3s - loss: 0.802 - ETA: 3s - loss: 0.801 - ETA: 2s - loss: 0.801 - ETA: 2s - loss: 0.801 - ETA: 2s - loss: 0.800 - ETA: 2s - loss: 0.800 - ETA: 1s - loss: 0.799 - ETA: 1s - loss: 0.799 - ETA: 1s - loss: 0.799 - ETA: 1s - loss: 0.798 - ETA: 1s - loss: 0.798 - ETA: 0s - loss: 0.798 - ETA: 0s - loss: 0.797 - ETA: 0s - loss: 0.797 - ETA: 0s - loss: 0.797 - 58s 7ms/step - loss: 0.7967 - val_loss: 0.7118\n",
      "Epoch 2/5\n",
      "8540/8540 [==============================] - ETA: 58s - loss: 0.70 - ETA: 55s - loss: 0.70 - ETA: 55s - loss: 0.70 - ETA: 55s - loss: 0.70 - ETA: 54s - loss: 0.70 - ETA: 55s - loss: 0.70 - ETA: 55s - loss: 0.70 - ETA: 54s - loss: 0.70 - ETA: 54s - loss: 0.70 - ETA: 54s - loss: 0.70 - ETA: 54s - loss: 0.70 - ETA: 53s - loss: 0.70 - ETA: 53s - loss: 0.70 - ETA: 53s - loss: 0.70 - ETA: 52s - loss: 0.70 - ETA: 53s - loss: 0.70 - ETA: 52s - loss: 0.70 - ETA: 52s - loss: 0.70 - ETA: 52s - loss: 0.70 - ETA: 52s - loss: 0.70 - ETA: 51s - loss: 0.70 - ETA: 51s - loss: 0.70 - ETA: 51s - loss: 0.70 - ETA: 51s - loss: 0.70 - ETA: 51s - loss: 0.70 - ETA: 51s - loss: 0.70 - ETA: 51s - loss: 0.70 - ETA: 50s - loss: 0.70 - ETA: 50s - loss: 0.70 - ETA: 50s - loss: 0.70 - ETA: 50s - loss: 0.70 - ETA: 50s - loss: 0.70 - ETA: 50s - loss: 0.70 - ETA: 49s - loss: 0.70 - ETA: 49s - loss: 0.70 - ETA: 49s - loss: 0.70 - ETA: 49s - loss: 0.70 - ETA: 49s - loss: 0.70 - ETA: 48s - loss: 0.70 - ETA: 48s - loss: 0.70 - ETA: 48s - loss: 0.70 - ETA: 48s - loss: 0.70 - ETA: 47s - loss: 0.70 - ETA: 47s - loss: 0.70 - ETA: 47s - loss: 0.70 - ETA: 47s - loss: 0.70 - ETA: 46s - loss: 0.70 - ETA: 46s - loss: 0.70 - ETA: 46s - loss: 0.70 - ETA: 46s - loss: 0.70 - ETA: 45s - loss: 0.70 - ETA: 45s - loss: 0.70 - ETA: 45s - loss: 0.70 - ETA: 45s - loss: 0.70 - ETA: 45s - loss: 0.70 - ETA: 44s - loss: 0.70 - ETA: 44s - loss: 0.70 - ETA: 44s - loss: 0.70 - ETA: 44s - loss: 0.70 - ETA: 44s - loss: 0.70 - ETA: 43s - loss: 0.70 - ETA: 43s - loss: 0.70 - ETA: 43s - loss: 0.70 - ETA: 43s - loss: 0.70 - ETA: 42s - loss: 0.70 - ETA: 42s - loss: 0.70 - ETA: 42s - loss: 0.70 - ETA: 42s - loss: 0.70 - ETA: 42s - loss: 0.70 - ETA: 41s - loss: 0.70 - ETA: 41s - loss: 0.70 - ETA: 41s - loss: 0.70 - ETA: 41s - loss: 0.70 - ETA: 40s - loss: 0.70 - ETA: 40s - loss: 0.70 - ETA: 40s - loss: 0.70 - ETA: 40s - loss: 0.70 - ETA: 40s - loss: 0.70 - ETA: 39s - loss: 0.70 - ETA: 39s - loss: 0.70 - ETA: 39s - loss: 0.70 - ETA: 39s - loss: 0.70 - ETA: 39s - loss: 0.70 - ETA: 38s - loss: 0.70 - ETA: 38s - loss: 0.70 - ETA: 38s - loss: 0.70 - ETA: 38s - loss: 0.70 - ETA: 37s - loss: 0.70 - ETA: 37s - loss: 0.70 - ETA: 37s - loss: 0.70 - ETA: 37s - loss: 0.70 - ETA: 37s - loss: 0.70 - ETA: 36s - loss: 0.70 - ETA: 36s - loss: 0.70 - ETA: 36s - loss: 0.70 - ETA: 36s - loss: 0.70 - ETA: 35s - loss: 0.70 - ETA: 35s - loss: 0.70 - ETA: 35s - loss: 0.70 - ETA: 35s - loss: 0.70 - ETA: 35s - loss: 0.70 - ETA: 34s - loss: 0.70 - ETA: 34s - loss: 0.70 - ETA: 34s - loss: 0.70 - ETA: 34s - loss: 0.70 - ETA: 34s - loss: 0.70 - ETA: 33s - loss: 0.70 - ETA: 33s - loss: 0.70 - ETA: 33s - loss: 0.70 - ETA: 33s - loss: 0.70 - ETA: 33s - loss: 0.70 - ETA: 32s - loss: 0.70 - ETA: 32s - loss: 0.70 - ETA: 32s - loss: 0.70 - ETA: 32s - loss: 0.70 - ETA: 31s - loss: 0.70 - ETA: 31s - loss: 0.70 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 9s - loss: 0.6982 - ETA: 9s - loss: 0.698 - ETA: 9s - loss: 0.698 - ETA: 9s - loss: 0.698 - ETA: 9s - loss: 0.698 - ETA: 8s - loss: 0.698 - ETA: 8s - loss: 0.698 - ETA: 8s - loss: 0.698 - ETA: 8s - loss: 0.698 - ETA: 7s - loss: 0.698 - ETA: 7s - loss: 0.698 - ETA: 7s - loss: 0.698 - ETA: 7s - loss: 0.698 - ETA: 7s - loss: 0.698 - ETA: 6s - loss: 0.698 - ETA: 6s - loss: 0.698 - ETA: 6s - loss: 0.698 - ETA: 6s - loss: 0.698 - ETA: 6s - loss: 0.698 - ETA: 5s - loss: 0.698 - ETA: 5s - loss: 0.698 - ETA: 5s - loss: 0.698 - ETA: 5s - loss: 0.698 - ETA: 5s - loss: 0.698 - ETA: 4s - loss: 0.698 - ETA: 4s - loss: 0.697 - ETA: 4s - loss: 0.697 - ETA: 4s - loss: 0.697 - ETA: 3s - loss: 0.697 - ETA: 3s - loss: 0.697 - ETA: 3s - loss: 0.697 - ETA: 3s - loss: 0.697 - ETA: 3s - loss: 0.697 - ETA: 2s - loss: 0.697 - ETA: 2s - loss: 0.697 - ETA: 2s - loss: 0.697 - ETA: 2s - loss: 0.697 - ETA: 2s - loss: 0.697 - ETA: 1s - loss: 0.697 - ETA: 1s - loss: 0.697 - ETA: 1s - loss: 0.697 - ETA: 1s - loss: 0.697 - ETA: 1s - loss: 0.697 - ETA: 0s - loss: 0.697 - ETA: 0s - loss: 0.697 - ETA: 0s - loss: 0.697 - ETA: 0s - loss: 0.697 - 56s 7ms/step - loss: 0.6977 - val_loss: 0.6971\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540/8540 [==============================] - ETA: 56s - loss: 0.69 - ETA: 56s - loss: 0.69 - ETA: 55s - loss: 0.69 - ETA: 55s - loss: 0.69 - ETA: 55s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 9s - loss: 0.6943 - ETA: 9s - loss: 0.694 - ETA: 9s - loss: 0.694 - ETA: 9s - loss: 0.694 - ETA: 8s - loss: 0.694 - ETA: 8s - loss: 0.694 - ETA: 8s - loss: 0.694 - ETA: 8s - loss: 0.694 - ETA: 8s - loss: 0.694 - ETA: 7s - loss: 0.694 - ETA: 7s - loss: 0.694 - ETA: 7s - loss: 0.694 - ETA: 7s - loss: 0.694 - ETA: 7s - loss: 0.694 - ETA: 6s - loss: 0.694 - ETA: 6s - loss: 0.694 - ETA: 6s - loss: 0.694 - ETA: 6s - loss: 0.694 - ETA: 6s - loss: 0.694 - ETA: 5s - loss: 0.694 - ETA: 5s - loss: 0.694 - ETA: 5s - loss: 0.694 - ETA: 5s - loss: 0.694 - ETA: 5s - loss: 0.694 - ETA: 4s - loss: 0.694 - ETA: 4s - loss: 0.694 - ETA: 4s - loss: 0.694 - ETA: 4s - loss: 0.694 - ETA: 3s - loss: 0.694 - ETA: 3s - loss: 0.694 - ETA: 3s - loss: 0.694 - ETA: 3s - loss: 0.694 - ETA: 3s - loss: 0.694 - ETA: 2s - loss: 0.694 - ETA: 2s - loss: 0.694 - ETA: 2s - loss: 0.694 - ETA: 2s - loss: 0.694 - ETA: 2s - loss: 0.694 - ETA: 1s - loss: 0.694 - ETA: 1s - loss: 0.694 - ETA: 1s - loss: 0.694 - ETA: 1s - loss: 0.694 - ETA: 1s - loss: 0.694 - ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.694 - 56s 7ms/step - loss: 0.6942 - val_loss: 0.6946\n",
      "Epoch 4/5\n",
      "8540/8540 [==============================] - ETA: 51s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 9s - loss: 0.6936 - ETA: 9s - loss: 0.693 - ETA: 9s - loss: 0.693 - ETA: 9s - loss: 0.693 - ETA: 8s - loss: 0.693 - ETA: 8s - loss: 0.693 - ETA: 8s - loss: 0.693 - ETA: 8s - loss: 0.693 - ETA: 8s - loss: 0.693 - ETA: 7s - loss: 0.693 - ETA: 7s - loss: 0.693 - ETA: 7s - loss: 0.693 - ETA: 7s - loss: 0.693 - ETA: 7s - loss: 0.693 - ETA: 6s - loss: 0.693 - ETA: 6s - loss: 0.693 - ETA: 6s - loss: 0.693 - ETA: 6s - loss: 0.693 - ETA: 6s - loss: 0.693 - ETA: 5s - loss: 0.693 - ETA: 5s - loss: 0.693 - ETA: 5s - loss: 0.693 - ETA: 5s - loss: 0.693 - ETA: 5s - loss: 0.693 - ETA: 4s - loss: 0.693 - ETA: 4s - loss: 0.693 - ETA: 4s - loss: 0.693 - ETA: 4s - loss: 0.693 - ETA: 3s - loss: 0.693 - ETA: 3s - loss: 0.693 - ETA: 3s - loss: 0.693 - ETA: 3s - loss: 0.693 - ETA: 3s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 0s - loss: 0.693 - ETA: 0s - loss: 0.693 - ETA: 0s - loss: 0.693 - ETA: 0s - loss: 0.693 - 56s 7ms/step - loss: 0.6935 - val_loss: 0.6938\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540/8540 [==============================] - ETA: 56s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 55s - loss: 0.69 - ETA: 55s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 54s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 53s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 52s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 51s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 50s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 48s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 47s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 46s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 45s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 43s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 41s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 37s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 33s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 31s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 29s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 27s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 26s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 24s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 23s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 22s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 21s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 19s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 18s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 17s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 15s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 14s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 13s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 12s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 10s - loss: 0.69 - ETA: 9s - loss: 0.6933 - ETA: 9s - loss: 0.693 - ETA: 9s - loss: 0.693 - ETA: 9s - loss: 0.693 - ETA: 9s - loss: 0.693 - ETA: 8s - loss: 0.693 - ETA: 8s - loss: 0.693 - ETA: 8s - loss: 0.693 - ETA: 8s - loss: 0.693 - ETA: 8s - loss: 0.693 - ETA: 7s - loss: 0.693 - ETA: 7s - loss: 0.693 - ETA: 7s - loss: 0.693 - ETA: 7s - loss: 0.693 - ETA: 7s - loss: 0.693 - ETA: 6s - loss: 0.693 - ETA: 6s - loss: 0.693 - ETA: 6s - loss: 0.693 - ETA: 6s - loss: 0.693 - ETA: 5s - loss: 0.693 - ETA: 5s - loss: 0.693 - ETA: 5s - loss: 0.693 - ETA: 5s - loss: 0.693 - ETA: 5s - loss: 0.693 - ETA: 4s - loss: 0.693 - ETA: 4s - loss: 0.693 - ETA: 4s - loss: 0.693 - ETA: 4s - loss: 0.693 - ETA: 4s - loss: 0.693 - ETA: 3s - loss: 0.693 - ETA: 3s - loss: 0.693 - ETA: 3s - loss: 0.693 - ETA: 3s - loss: 0.693 - ETA: 3s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 2s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 1s - loss: 0.693 - ETA: 0s - loss: 0.693 - ETA: 0s - loss: 0.693 - ETA: 0s - loss: 0.693 - ETA: 0s - loss: 0.693 - 56s 7ms/step - loss: 0.6933 - val_loss: 0.6935\n"
     ]
    }
   ],
   "source": [
    "filter_length = 300\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(6119, 71, input_length=1200))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1))\n",
    "# model.add(GlobalMaxPool1D())\n",
    "# model.add(Dense(82))\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1micro', verbose=0, save_best_only=True, mode='max')\n",
    "model = Sequential()\n",
    "# model.add(Embedding(max_words, 20, input_length=maxlen))\n",
    "model.add(Embedding(vocab_size, output_dim=50, input_length=1200))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(71, activation='sigmoid'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "# callbacks = [\n",
    "#     ReduceLROnPlateau(),\n",
    "#     EarlyStopping(patience=4),\n",
    "#     ModelCheckpoint(filepath='model-2.h5', save_best_only=True)\n",
    "# ]\n",
    "history = model.fit(padded_docs_train, y_train,\n",
    "                    class_weight='balanced',\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0428, Recall: 1.0000, F1-measure: 0.0822\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0428, Recall: 1.0000, F1-measure: 0.0822\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0428, Recall: 1.0000, F1-measure: 0.0822\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0428, Recall: 1.0000, F1-measure: 0.0822\n",
      "For threshold:  0.5\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0428, Recall: 1.0000, F1-measure: 0.0822\n",
      "For threshold:  0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict([padded_docs_test])\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [\n",
    "#     ReduceLROnPlateau(),\n",
    "#     EarlyStopping(patience=4),\n",
    "#     ModelCheckpoint(filepath='model-3.h5', save_best_only=True)\n",
    "# ]\n",
    "# history = model.fit(padded_docs_train, y_train,\n",
    "#                     class_weight='balanced',\n",
    "#                     epochs=5,\n",
    "#                     batch_size=32,\n",
    "#                     validation_split=0.1,\n",
    "#                     callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1200, 71)          7013522   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1200, 71)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1198, 300)         64200     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 71)                21371     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 71)                0         \n",
      "=================================================================\n",
      "Total params: 7,099,093\n",
      "Trainable params: 7,099,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# filter_length = 300\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1micro', verbose=0, save_best_only=True, mode='max')\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 71, input_length=1200))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(300, 3, activation='relu'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "# model.add(Conv1D(200, 3, activation='relu'))\n",
    "# model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(71))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7591 samples, validate on 1898 samples\n",
      "Epoch 1/5\n",
      "7591/7591 [==============================] - ETA: 8:41 - loss: 0.703 - ETA: 6:06 - loss: 0.698 - ETA: 5:12 - loss: 0.692 - ETA: 4:44 - loss: 0.686 - ETA: 4:28 - loss: 0.679 - ETA: 4:17 - loss: 0.673 - ETA: 4:08 - loss: 0.666 - ETA: 4:02 - loss: 0.659 - ETA: 3:57 - loss: 0.651 - ETA: 3:52 - loss: 0.644 - ETA: 3:49 - loss: 0.636 - ETA: 3:45 - loss: 0.628 - ETA: 3:42 - loss: 0.621 - ETA: 3:40 - loss: 0.612 - ETA: 3:38 - loss: 0.603 - ETA: 3:35 - loss: 0.595 - ETA: 3:33 - loss: 0.585 - ETA: 3:32 - loss: 0.577 - ETA: 3:31 - loss: 0.567 - ETA: 3:29 - loss: 0.557 - ETA: 3:27 - loss: 0.547 - ETA: 3:26 - loss: 0.537 - ETA: 3:25 - loss: 0.526 - ETA: 3:25 - loss: 0.516 - ETA: 3:23 - loss: 0.506 - ETA: 3:21 - loss: 0.496 - ETA: 3:20 - loss: 0.487 - ETA: 3:18 - loss: 0.477 - ETA: 3:17 - loss: 0.467 - ETA: 3:16 - loss: 0.457 - ETA: 3:14 - loss: 0.448 - ETA: 3:13 - loss: 0.440 - ETA: 3:12 - loss: 0.432 - ETA: 3:10 - loss: 0.424 - ETA: 3:09 - loss: 0.416 - ETA: 3:08 - loss: 0.409 - ETA: 3:07 - loss: 0.402 - ETA: 3:06 - loss: 0.396 - ETA: 3:06 - loss: 0.389 - ETA: 3:05 - loss: 0.383 - ETA: 3:04 - loss: 0.377 - ETA: 3:02 - loss: 0.372 - ETA: 3:02 - loss: 0.366 - ETA: 3:01 - loss: 0.362 - ETA: 3:00 - loss: 0.358 - ETA: 2:59 - loss: 0.354 - ETA: 2:58 - loss: 0.351 - ETA: 2:58 - loss: 0.346 - ETA: 2:57 - loss: 0.341 - ETA: 2:56 - loss: 0.338 - ETA: 2:55 - loss: 0.334 - ETA: 2:54 - loss: 0.332 - ETA: 2:53 - loss: 0.328 - ETA: 2:52 - loss: 0.325 - ETA: 2:51 - loss: 0.322 - ETA: 2:51 - loss: 0.319 - ETA: 2:50 - loss: 0.316 - ETA: 2:49 - loss: 0.313 - ETA: 2:48 - loss: 0.310 - ETA: 2:47 - loss: 0.308 - ETA: 2:46 - loss: 0.305 - ETA: 2:45 - loss: 0.302 - ETA: 2:45 - loss: 0.300 - ETA: 2:44 - loss: 0.297 - ETA: 2:43 - loss: 0.295 - ETA: 2:42 - loss: 0.293 - ETA: 2:41 - loss: 0.291 - ETA: 2:40 - loss: 0.289 - ETA: 2:39 - loss: 0.288 - ETA: 2:39 - loss: 0.286 - ETA: 2:38 - loss: 0.284 - ETA: 2:37 - loss: 0.282 - ETA: 2:36 - loss: 0.280 - ETA: 2:35 - loss: 0.278 - ETA: 2:34 - loss: 0.277 - ETA: 2:33 - loss: 0.275 - ETA: 2:32 - loss: 0.274 - ETA: 2:31 - loss: 0.272 - ETA: 2:30 - loss: 0.271 - ETA: 2:29 - loss: 0.269 - ETA: 2:28 - loss: 0.268 - ETA: 2:28 - loss: 0.266 - ETA: 2:27 - loss: 0.265 - ETA: 2:26 - loss: 0.263 - ETA: 2:25 - loss: 0.261 - ETA: 2:24 - loss: 0.260 - ETA: 2:23 - loss: 0.259 - ETA: 2:22 - loss: 0.258 - ETA: 2:21 - loss: 0.256 - ETA: 2:20 - loss: 0.255 - ETA: 2:19 - loss: 0.254 - ETA: 2:18 - loss: 0.253 - ETA: 2:17 - loss: 0.252 - ETA: 2:16 - loss: 0.251 - ETA: 2:15 - loss: 0.250 - ETA: 2:14 - loss: 0.249 - ETA: 2:13 - loss: 0.248 - ETA: 2:12 - loss: 0.247 - ETA: 2:11 - loss: 0.246 - ETA: 2:10 - loss: 0.245 - ETA: 2:09 - loss: 0.244 - ETA: 2:08 - loss: 0.243 - ETA: 2:07 - loss: 0.242 - ETA: 2:06 - loss: 0.241 - ETA: 2:05 - loss: 0.240 - ETA: 2:03 - loss: 0.239 - ETA: 2:03 - loss: 0.239 - ETA: 2:01 - loss: 0.238 - ETA: 2:01 - loss: 0.237 - ETA: 2:00 - loss: 0.236 - ETA: 1:58 - loss: 0.235 - ETA: 1:58 - loss: 0.234 - ETA: 1:56 - loss: 0.233 - ETA: 1:55 - loss: 0.233 - ETA: 1:54 - loss: 0.232 - ETA: 1:53 - loss: 0.231 - ETA: 1:52 - loss: 0.230 - ETA: 1:52 - loss: 0.229 - ETA: 1:51 - loss: 0.229 - ETA: 1:50 - loss: 0.228 - ETA: 1:49 - loss: 0.227 - ETA: 1:48 - loss: 0.226 - ETA: 1:47 - loss: 0.225 - ETA: 1:46 - loss: 0.225 - ETA: 1:45 - loss: 0.224 - ETA: 1:44 - loss: 0.223 - ETA: 1:43 - loss: 0.222 - ETA: 1:42 - loss: 0.222 - ETA: 1:41 - loss: 0.221 - ETA: 1:40 - loss: 0.221 - ETA: 1:39 - loss: 0.220 - ETA: 1:38 - loss: 0.219 - ETA: 1:37 - loss: 0.219 - ETA: 1:36 - loss: 0.218 - ETA: 1:35 - loss: 0.218 - ETA: 1:34 - loss: 0.217 - ETA: 1:33 - loss: 0.217 - ETA: 1:32 - loss: 0.216 - ETA: 1:31 - loss: 0.215 - ETA: 1:30 - loss: 0.215 - ETA: 1:29 - loss: 0.214 - ETA: 1:28 - loss: 0.214 - ETA: 1:27 - loss: 0.213 - ETA: 1:26 - loss: 0.213 - ETA: 1:25 - loss: 0.212 - ETA: 1:24 - loss: 0.212 - ETA: 1:23 - loss: 0.211 - ETA: 1:22 - loss: 0.211 - ETA: 1:22 - loss: 0.210 - ETA: 1:21 - loss: 0.210 - ETA: 1:20 - loss: 0.209 - ETA: 1:19 - loss: 0.209 - ETA: 1:18 - loss: 0.209 - ETA: 1:17 - loss: 0.208 - ETA: 1:16 - loss: 0.208 - ETA: 1:15 - loss: 0.207 - ETA: 1:14 - loss: 0.207 - ETA: 1:13 - loss: 0.206 - ETA: 1:12 - loss: 0.206 - ETA: 1:11 - loss: 0.205 - ETA: 1:10 - loss: 0.205 - ETA: 1:09 - loss: 0.205 - ETA: 1:08 - loss: 0.204 - ETA: 1:07 - loss: 0.204 - ETA: 1:06 - loss: 0.204 - ETA: 1:05 - loss: 0.203 - ETA: 1:05 - loss: 0.203 - ETA: 1:04 - loss: 0.202 - ETA: 1:03 - loss: 0.202 - ETA: 1:02 - loss: 0.202 - ETA: 1:01 - loss: 0.202 - ETA: 1:00 - loss: 0.201 - ETA: 59s - loss: 0.201 - ETA: 58s - loss: 0.20 - ETA: 57s - loss: 0.20 - ETA: 56s - loss: 0.20 - ETA: 55s - loss: 0.19 - ETA: 54s - loss: 0.19 - ETA: 53s - loss: 0.19 - ETA: 52s - loss: 0.19 - ETA: 51s - loss: 0.19 - ETA: 50s - loss: 0.19 - ETA: 50s - loss: 0.19 - ETA: 49s - loss: 0.19 - ETA: 48s - loss: 0.19 - ETA: 47s - loss: 0.19 - ETA: 46s - loss: 0.19 - ETA: 45s - loss: 0.19 - ETA: 44s - loss: 0.19 - ETA: 43s - loss: 0.19 - ETA: 42s - loss: 0.19 - ETA: 41s - loss: 0.19 - ETA: 40s - loss: 0.19 - ETA: 39s - loss: 0.19 - ETA: 38s - loss: 0.19 - ETA: 37s - loss: 0.19 - ETA: 36s - loss: 0.19 - ETA: 36s - loss: 0.19 - ETA: 35s - loss: 0.19 - ETA: 34s - loss: 0.19 - ETA: 33s - loss: 0.19 - ETA: 32s - loss: 0.19 - ETA: 31s - loss: 0.19 - ETA: 30s - loss: 0.19 - ETA: 29s - loss: 0.19 - ETA: 28s - loss: 0.19 - ETA: 27s - loss: 0.19 - ETA: 26s - loss: 0.19 - ETA: 25s - loss: 0.19 - ETA: 24s - loss: 0.19 - ETA: 24s - loss: 0.19 - ETA: 23s - loss: 0.19 - ETA: 22s - loss: 0.19 - ETA: 21s - loss: 0.19 - ETA: 20s - loss: 0.19 - ETA: 19s - loss: 0.19 - ETA: 18s - loss: 0.19 - ETA: 17s - loss: 0.18 - ETA: 16s - loss: 0.18 - ETA: 15s - loss: 0.18 - ETA: 14s - loss: 0.18 - ETA: 13s - loss: 0.18 - ETA: 13s - loss: 0.18 - ETA: 12s - loss: 0.18 - ETA: 11s - loss: 0.18 - ETA: 10s - loss: 0.18 - ETA: 9s - loss: 0.1878 - ETA: 8s - loss: 0.187 - ETA: 7s - loss: 0.187 - ETA: 6s - loss: 0.187 - ETA: 5s - loss: 0.187 - ETA: 4s - loss: 0.186 - ETA: 3s - loss: 0.186 - ETA: 2s - loss: 0.186 - ETA: 2s - loss: 0.186 - ETA: 1s - loss: 0.186 - ETA: 0s - loss: 0.186 - 227s 30ms/step - loss: 0.1861 - val_loss: 0.1364\n",
      "Epoch 2/5\n",
      "7591/7591 [==============================] - ETA: 4:52 - loss: 0.151 - ETA: 4:49 - loss: 0.154 - ETA: 4:46 - loss: 0.146 - ETA: 4:36 - loss: 0.146 - ETA: 4:20 - loss: 0.148 - ETA: 4:10 - loss: 0.145 - ETA: 4:02 - loss: 0.146 - ETA: 3:56 - loss: 0.144 - ETA: 3:51 - loss: 0.148 - ETA: 3:47 - loss: 0.147 - ETA: 3:44 - loss: 0.145 - ETA: 3:41 - loss: 0.143 - ETA: 3:38 - loss: 0.144 - ETA: 3:36 - loss: 0.144 - ETA: 3:34 - loss: 0.144 - ETA: 3:32 - loss: 0.144 - ETA: 3:30 - loss: 0.144 - ETA: 3:28 - loss: 0.144 - ETA: 3:27 - loss: 0.144 - ETA: 3:25 - loss: 0.144 - ETA: 3:23 - loss: 0.143 - ETA: 3:22 - loss: 0.144 - ETA: 3:20 - loss: 0.144 - ETA: 3:19 - loss: 0.144 - ETA: 3:18 - loss: 0.146 - ETA: 3:17 - loss: 0.146 - ETA: 3:15 - loss: 0.146 - ETA: 3:14 - loss: 0.146 - ETA: 3:13 - loss: 0.145 - ETA: 3:11 - loss: 0.145 - ETA: 3:10 - loss: 0.144 - ETA: 3:09 - loss: 0.144 - ETA: 3:08 - loss: 0.144 - ETA: 3:07 - loss: 0.144 - ETA: 3:06 - loss: 0.145 - ETA: 3:05 - loss: 0.145 - ETA: 3:04 - loss: 0.144 - ETA: 3:03 - loss: 0.144 - ETA: 3:02 - loss: 0.144 - ETA: 3:01 - loss: 0.144 - ETA: 3:00 - loss: 0.144 - ETA: 2:58 - loss: 0.145 - ETA: 2:57 - loss: 0.144 - ETA: 2:56 - loss: 0.145 - ETA: 2:55 - loss: 0.145 - ETA: 2:54 - loss: 0.145 - ETA: 2:53 - loss: 0.145 - ETA: 2:52 - loss: 0.145 - ETA: 2:51 - loss: 0.145 - ETA: 2:50 - loss: 0.145 - ETA: 2:49 - loss: 0.145 - ETA: 2:48 - loss: 0.145 - ETA: 2:47 - loss: 0.145 - ETA: 2:46 - loss: 0.145 - ETA: 2:45 - loss: 0.145 - ETA: 2:44 - loss: 0.145 - ETA: 2:43 - loss: 0.145 - ETA: 2:42 - loss: 0.146 - ETA: 2:41 - loss: 0.146 - ETA: 2:40 - loss: 0.146 - ETA: 2:39 - loss: 0.146 - ETA: 2:38 - loss: 0.146 - ETA: 2:37 - loss: 0.145 - ETA: 2:36 - loss: 0.145 - ETA: 2:35 - loss: 0.145 - ETA: 2:34 - loss: 0.145 - ETA: 2:33 - loss: 0.145 - ETA: 2:33 - loss: 0.145 - ETA: 2:32 - loss: 0.145 - ETA: 2:31 - loss: 0.145 - ETA: 2:30 - loss: 0.145 - ETA: 2:29 - loss: 0.145 - ETA: 2:28 - loss: 0.145 - ETA: 2:27 - loss: 0.145 - ETA: 2:26 - loss: 0.145 - ETA: 2:25 - loss: 0.145 - ETA: 2:24 - loss: 0.145 - ETA: 2:23 - loss: 0.144 - ETA: 2:22 - loss: 0.144 - ETA: 2:21 - loss: 0.144 - ETA: 2:20 - loss: 0.144 - ETA: 2:19 - loss: 0.144 - ETA: 2:18 - loss: 0.144 - ETA: 2:17 - loss: 0.144 - ETA: 2:17 - loss: 0.143 - ETA: 2:16 - loss: 0.144 - ETA: 2:15 - loss: 0.144 - ETA: 2:14 - loss: 0.144 - ETA: 2:13 - loss: 0.144 - ETA: 2:12 - loss: 0.144 - ETA: 2:11 - loss: 0.144 - ETA: 2:11 - loss: 0.143 - ETA: 2:10 - loss: 0.143 - ETA: 2:09 - loss: 0.143 - ETA: 2:08 - loss: 0.143 - ETA: 2:07 - loss: 0.143 - ETA: 2:07 - loss: 0.143 - ETA: 2:06 - loss: 0.143 - ETA: 2:05 - loss: 0.143 - ETA: 2:05 - loss: 0.143 - ETA: 2:04 - loss: 0.143 - ETA: 2:04 - loss: 0.143 - ETA: 2:03 - loss: 0.143 - ETA: 2:02 - loss: 0.143 - ETA: 2:01 - loss: 0.143 - ETA: 2:00 - loss: 0.144 - ETA: 2:00 - loss: 0.144 - ETA: 1:59 - loss: 0.144 - ETA: 1:58 - loss: 0.144 - ETA: 1:57 - loss: 0.143 - ETA: 1:57 - loss: 0.143 - ETA: 1:56 - loss: 0.143 - ETA: 1:55 - loss: 0.144 - ETA: 1:54 - loss: 0.144 - ETA: 1:53 - loss: 0.143 - ETA: 1:52 - loss: 0.143 - ETA: 1:52 - loss: 0.143 - ETA: 1:51 - loss: 0.143 - ETA: 1:50 - loss: 0.143 - ETA: 1:49 - loss: 0.143 - ETA: 1:48 - loss: 0.143 - ETA: 1:47 - loss: 0.143 - ETA: 1:46 - loss: 0.143 - ETA: 1:45 - loss: 0.143 - ETA: 1:44 - loss: 0.143 - ETA: 1:43 - loss: 0.143 - ETA: 1:42 - loss: 0.143 - ETA: 1:41 - loss: 0.143 - ETA: 1:40 - loss: 0.143 - ETA: 1:39 - loss: 0.143 - ETA: 1:38 - loss: 0.143 - ETA: 1:37 - loss: 0.143 - ETA: 1:36 - loss: 0.143 - ETA: 1:35 - loss: 0.143 - ETA: 1:34 - loss: 0.143 - ETA: 1:33 - loss: 0.143 - ETA: 1:32 - loss: 0.143 - ETA: 1:31 - loss: 0.143 - ETA: 1:30 - loss: 0.143 - ETA: 1:29 - loss: 0.143 - ETA: 1:28 - loss: 0.143 - ETA: 1:27 - loss: 0.143 - ETA: 1:26 - loss: 0.144 - ETA: 1:25 - loss: 0.144 - ETA: 1:24 - loss: 0.144 - ETA: 1:23 - loss: 0.144 - ETA: 1:22 - loss: 0.144 - ETA: 1:22 - loss: 0.144 - ETA: 1:21 - loss: 0.144 - ETA: 1:20 - loss: 0.144 - ETA: 1:19 - loss: 0.144 - ETA: 1:18 - loss: 0.144 - ETA: 1:17 - loss: 0.144 - ETA: 1:16 - loss: 0.143 - ETA: 1:15 - loss: 0.144 - ETA: 1:14 - loss: 0.144 - ETA: 1:13 - loss: 0.143 - ETA: 1:12 - loss: 0.143 - ETA: 1:11 - loss: 0.143 - ETA: 1:10 - loss: 0.144 - ETA: 1:09 - loss: 0.143 - ETA: 1:08 - loss: 0.143 - ETA: 1:07 - loss: 0.143 - ETA: 1:07 - loss: 0.143 - ETA: 1:06 - loss: 0.143 - ETA: 1:05 - loss: 0.143 - ETA: 1:04 - loss: 0.144 - ETA: 1:03 - loss: 0.144 - ETA: 1:02 - loss: 0.144 - ETA: 1:02 - loss: 0.144 - ETA: 1:01 - loss: 0.144 - ETA: 1:00 - loss: 0.144 - ETA: 59s - loss: 0.144 - ETA: 58s - loss: 0.14 - ETA: 58s - loss: 0.14 - ETA: 57s - loss: 0.14 - ETA: 56s - loss: 0.14 - ETA: 55s - loss: 0.14 - ETA: 54s - loss: 0.14 - ETA: 53s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 9s - loss: 0.1438 - ETA: 8s - loss: 0.143 - ETA: 7s - loss: 0.143 - ETA: 6s - loss: 0.143 - ETA: 5s - loss: 0.143 - ETA: 4s - loss: 0.143 - ETA: 3s - loss: 0.143 - ETA: 2s - loss: 0.143 - ETA: 2s - loss: 0.143 - ETA: 1s - loss: 0.143 - ETA: 0s - loss: 0.143 - 225s 30ms/step - loss: 0.1436 - val_loss: 0.1335\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7591/7591 [==============================] - ETA: 3:23 - loss: 0.111 - ETA: 3:22 - loss: 0.120 - ETA: 3:22 - loss: 0.119 - ETA: 3:21 - loss: 0.123 - ETA: 3:20 - loss: 0.131 - ETA: 3:19 - loss: 0.139 - ETA: 3:19 - loss: 0.135 - ETA: 3:18 - loss: 0.141 - ETA: 3:17 - loss: 0.143 - ETA: 3:16 - loss: 0.142 - ETA: 3:15 - loss: 0.140 - ETA: 3:14 - loss: 0.141 - ETA: 3:14 - loss: 0.143 - ETA: 3:13 - loss: 0.142 - ETA: 3:13 - loss: 0.141 - ETA: 3:12 - loss: 0.142 - ETA: 3:11 - loss: 0.142 - ETA: 3:10 - loss: 0.144 - ETA: 3:09 - loss: 0.143 - ETA: 3:08 - loss: 0.142 - ETA: 3:08 - loss: 0.142 - ETA: 3:07 - loss: 0.142 - ETA: 3:06 - loss: 0.142 - ETA: 3:05 - loss: 0.143 - ETA: 3:04 - loss: 0.143 - ETA: 3:03 - loss: 0.143 - ETA: 3:02 - loss: 0.143 - ETA: 3:01 - loss: 0.144 - ETA: 3:00 - loss: 0.143 - ETA: 3:00 - loss: 0.143 - ETA: 2:59 - loss: 0.143 - ETA: 2:58 - loss: 0.143 - ETA: 2:57 - loss: 0.143 - ETA: 2:56 - loss: 0.143 - ETA: 2:55 - loss: 0.143 - ETA: 2:54 - loss: 0.143 - ETA: 2:53 - loss: 0.143 - ETA: 2:53 - loss: 0.143 - ETA: 2:52 - loss: 0.143 - ETA: 2:51 - loss: 0.143 - ETA: 2:50 - loss: 0.144 - ETA: 2:49 - loss: 0.144 - ETA: 2:48 - loss: 0.143 - ETA: 2:47 - loss: 0.143 - ETA: 2:47 - loss: 0.143 - ETA: 2:46 - loss: 0.143 - ETA: 2:45 - loss: 0.143 - ETA: 2:44 - loss: 0.143 - ETA: 2:44 - loss: 0.143 - ETA: 2:44 - loss: 0.143 - ETA: 2:44 - loss: 0.143 - ETA: 2:44 - loss: 0.143 - ETA: 2:44 - loss: 0.143 - ETA: 2:43 - loss: 0.143 - ETA: 2:43 - loss: 0.142 - ETA: 2:42 - loss: 0.142 - ETA: 2:42 - loss: 0.142 - ETA: 2:41 - loss: 0.142 - ETA: 2:40 - loss: 0.142 - ETA: 2:40 - loss: 0.142 - ETA: 2:39 - loss: 0.142 - ETA: 2:38 - loss: 0.142 - ETA: 2:37 - loss: 0.141 - ETA: 2:36 - loss: 0.142 - ETA: 2:35 - loss: 0.141 - ETA: 2:35 - loss: 0.141 - ETA: 2:34 - loss: 0.141 - ETA: 2:33 - loss: 0.142 - ETA: 2:32 - loss: 0.142 - ETA: 2:31 - loss: 0.142 - ETA: 2:30 - loss: 0.142 - ETA: 2:29 - loss: 0.142 - ETA: 2:28 - loss: 0.142 - ETA: 2:27 - loss: 0.142 - ETA: 2:26 - loss: 0.142 - ETA: 2:25 - loss: 0.141 - ETA: 2:24 - loss: 0.141 - ETA: 2:23 - loss: 0.141 - ETA: 2:22 - loss: 0.141 - ETA: 2:21 - loss: 0.141 - ETA: 2:20 - loss: 0.141 - ETA: 2:19 - loss: 0.141 - ETA: 2:18 - loss: 0.140 - ETA: 2:17 - loss: 0.140 - ETA: 2:16 - loss: 0.140 - ETA: 2:15 - loss: 0.140 - ETA: 2:14 - loss: 0.140 - ETA: 2:13 - loss: 0.140 - ETA: 2:12 - loss: 0.140 - ETA: 2:12 - loss: 0.140 - ETA: 2:11 - loss: 0.140 - ETA: 2:10 - loss: 0.140 - ETA: 2:09 - loss: 0.140 - ETA: 2:08 - loss: 0.139 - ETA: 2:07 - loss: 0.139 - ETA: 2:06 - loss: 0.139 - ETA: 2:05 - loss: 0.139 - ETA: 2:04 - loss: 0.140 - ETA: 2:03 - loss: 0.140 - ETA: 2:02 - loss: 0.139 - ETA: 2:01 - loss: 0.139 - ETA: 2:00 - loss: 0.139 - ETA: 1:59 - loss: 0.139 - ETA: 1:58 - loss: 0.140 - ETA: 1:57 - loss: 0.140 - ETA: 1:56 - loss: 0.140 - ETA: 1:56 - loss: 0.140 - ETA: 1:55 - loss: 0.140 - ETA: 1:54 - loss: 0.140 - ETA: 1:53 - loss: 0.140 - ETA: 1:52 - loss: 0.140 - ETA: 1:51 - loss: 0.140 - ETA: 1:50 - loss: 0.140 - ETA: 1:49 - loss: 0.140 - ETA: 1:48 - loss: 0.140 - ETA: 1:47 - loss: 0.140 - ETA: 1:46 - loss: 0.140 - ETA: 1:45 - loss: 0.140 - ETA: 1:45 - loss: 0.140 - ETA: 1:44 - loss: 0.140 - ETA: 1:43 - loss: 0.140 - ETA: 1:42 - loss: 0.140 - ETA: 1:41 - loss: 0.140 - ETA: 1:40 - loss: 0.140 - ETA: 1:39 - loss: 0.140 - ETA: 1:38 - loss: 0.140 - ETA: 1:37 - loss: 0.140 - ETA: 1:36 - loss: 0.140 - ETA: 1:35 - loss: 0.140 - ETA: 1:35 - loss: 0.140 - ETA: 1:34 - loss: 0.139 - ETA: 1:33 - loss: 0.139 - ETA: 1:32 - loss: 0.140 - ETA: 1:31 - loss: 0.140 - ETA: 1:30 - loss: 0.139 - ETA: 1:29 - loss: 0.139 - ETA: 1:28 - loss: 0.139 - ETA: 1:27 - loss: 0.139 - ETA: 1:26 - loss: 0.140 - ETA: 1:26 - loss: 0.139 - ETA: 1:25 - loss: 0.140 - ETA: 1:24 - loss: 0.140 - ETA: 1:23 - loss: 0.140 - ETA: 1:22 - loss: 0.140 - ETA: 1:21 - loss: 0.139 - ETA: 1:20 - loss: 0.140 - ETA: 1:19 - loss: 0.140 - ETA: 1:18 - loss: 0.140 - ETA: 1:17 - loss: 0.140 - ETA: 1:17 - loss: 0.140 - ETA: 1:16 - loss: 0.140 - ETA: 1:15 - loss: 0.140 - ETA: 1:14 - loss: 0.140 - ETA: 1:13 - loss: 0.140 - ETA: 1:12 - loss: 0.140 - ETA: 1:11 - loss: 0.140 - ETA: 1:10 - loss: 0.140 - ETA: 1:09 - loss: 0.140 - ETA: 1:09 - loss: 0.141 - ETA: 1:08 - loss: 0.140 - ETA: 1:07 - loss: 0.141 - ETA: 1:06 - loss: 0.140 - ETA: 1:05 - loss: 0.140 - ETA: 1:04 - loss: 0.140 - ETA: 1:03 - loss: 0.141 - ETA: 1:02 - loss: 0.141 - ETA: 1:01 - loss: 0.141 - ETA: 1:00 - loss: 0.141 - ETA: 1:00 - loss: 0.141 - ETA: 59s - loss: 0.141 - ETA: 58s - loss: 0.14 - ETA: 57s - loss: 0.14 - ETA: 56s - loss: 0.14 - ETA: 55s - loss: 0.14 - ETA: 54s - loss: 0.14 - ETA: 53s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 48s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 45s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 31s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 9s - loss: 0.1406 - ETA: 8s - loss: 0.140 - ETA: 8s - loss: 0.140 - ETA: 7s - loss: 0.140 - ETA: 6s - loss: 0.140 - ETA: 5s - loss: 0.140 - ETA: 4s - loss: 0.140 - ETA: 3s - loss: 0.140 - ETA: 2s - loss: 0.140 - ETA: 1s - loss: 0.140 - ETA: 1s - loss: 0.140 - ETA: 0s - loss: 0.140 - 215s 28ms/step - loss: 0.1402 - val_loss: 0.1353\n",
      "Epoch 4/5\n",
      "7591/7591 [==============================] - ETA: 3:31 - loss: 0.140 - ETA: 3:26 - loss: 0.136 - ETA: 3:23 - loss: 0.132 - ETA: 3:22 - loss: 0.136 - ETA: 3:22 - loss: 0.139 - ETA: 3:20 - loss: 0.140 - ETA: 3:19 - loss: 0.139 - ETA: 3:18 - loss: 0.142 - ETA: 3:17 - loss: 0.141 - ETA: 3:15 - loss: 0.143 - ETA: 3:15 - loss: 0.141 - ETA: 3:14 - loss: 0.142 - ETA: 3:13 - loss: 0.142 - ETA: 3:12 - loss: 0.142 - ETA: 3:11 - loss: 0.144 - ETA: 3:10 - loss: 0.144 - ETA: 3:10 - loss: 0.145 - ETA: 3:09 - loss: 0.145 - ETA: 3:08 - loss: 0.144 - ETA: 3:07 - loss: 0.145 - ETA: 3:06 - loss: 0.144 - ETA: 3:05 - loss: 0.144 - ETA: 3:05 - loss: 0.144 - ETA: 3:04 - loss: 0.142 - ETA: 3:03 - loss: 0.141 - ETA: 3:02 - loss: 0.141 - ETA: 3:01 - loss: 0.142 - ETA: 3:00 - loss: 0.142 - ETA: 2:59 - loss: 0.142 - ETA: 2:59 - loss: 0.142 - ETA: 2:58 - loss: 0.142 - ETA: 2:57 - loss: 0.142 - ETA: 2:56 - loss: 0.141 - ETA: 2:55 - loss: 0.141 - ETA: 2:54 - loss: 0.141 - ETA: 2:53 - loss: 0.141 - ETA: 2:52 - loss: 0.141 - ETA: 2:52 - loss: 0.141 - ETA: 2:51 - loss: 0.140 - ETA: 2:50 - loss: 0.140 - ETA: 2:49 - loss: 0.139 - ETA: 2:48 - loss: 0.140 - ETA: 2:48 - loss: 0.141 - ETA: 2:47 - loss: 0.141 - ETA: 2:46 - loss: 0.141 - ETA: 2:45 - loss: 0.141 - ETA: 2:44 - loss: 0.140 - ETA: 2:43 - loss: 0.140 - ETA: 2:42 - loss: 0.140 - ETA: 2:42 - loss: 0.139 - ETA: 2:41 - loss: 0.139 - ETA: 2:40 - loss: 0.139 - ETA: 2:39 - loss: 0.139 - ETA: 2:38 - loss: 0.138 - ETA: 2:37 - loss: 0.138 - ETA: 2:37 - loss: 0.139 - ETA: 2:36 - loss: 0.139 - ETA: 2:35 - loss: 0.140 - ETA: 2:34 - loss: 0.140 - ETA: 2:33 - loss: 0.140 - ETA: 2:32 - loss: 0.140 - ETA: 2:31 - loss: 0.140 - ETA: 2:30 - loss: 0.140 - ETA: 2:29 - loss: 0.140 - ETA: 2:29 - loss: 0.140 - ETA: 2:28 - loss: 0.139 - ETA: 2:27 - loss: 0.140 - ETA: 2:26 - loss: 0.140 - ETA: 2:25 - loss: 0.140 - ETA: 2:24 - loss: 0.140 - ETA: 2:23 - loss: 0.140 - ETA: 2:22 - loss: 0.140 - ETA: 2:22 - loss: 0.140 - ETA: 2:21 - loss: 0.140 - ETA: 2:20 - loss: 0.140 - ETA: 2:19 - loss: 0.140 - ETA: 2:18 - loss: 0.140 - ETA: 2:17 - loss: 0.140 - ETA: 2:17 - loss: 0.140 - ETA: 2:16 - loss: 0.140 - ETA: 2:15 - loss: 0.140 - ETA: 2:14 - loss: 0.140 - ETA: 2:13 - loss: 0.140 - ETA: 2:13 - loss: 0.140 - ETA: 2:12 - loss: 0.140 - ETA: 2:11 - loss: 0.140 - ETA: 2:10 - loss: 0.140 - ETA: 2:09 - loss: 0.140 - ETA: 2:08 - loss: 0.140 - ETA: 2:07 - loss: 0.139 - ETA: 2:06 - loss: 0.139 - ETA: 2:06 - loss: 0.139 - ETA: 2:05 - loss: 0.140 - ETA: 2:04 - loss: 0.140 - ETA: 2:03 - loss: 0.139 - ETA: 2:02 - loss: 0.140 - ETA: 2:01 - loss: 0.140 - ETA: 2:00 - loss: 0.139 - ETA: 2:00 - loss: 0.139 - ETA: 1:59 - loss: 0.140 - ETA: 1:58 - loss: 0.140 - ETA: 1:57 - loss: 0.140 - ETA: 1:56 - loss: 0.139 - ETA: 1:55 - loss: 0.139 - ETA: 1:54 - loss: 0.139 - ETA: 1:53 - loss: 0.140 - ETA: 1:53 - loss: 0.139 - ETA: 1:52 - loss: 0.139 - ETA: 1:51 - loss: 0.139 - ETA: 1:50 - loss: 0.139 - ETA: 1:49 - loss: 0.139 - ETA: 1:48 - loss: 0.139 - ETA: 1:47 - loss: 0.139 - ETA: 1:46 - loss: 0.139 - ETA: 1:46 - loss: 0.139 - ETA: 1:45 - loss: 0.139 - ETA: 1:44 - loss: 0.139 - ETA: 1:43 - loss: 0.139 - ETA: 1:42 - loss: 0.139 - ETA: 1:41 - loss: 0.139 - ETA: 1:40 - loss: 0.139 - ETA: 1:39 - loss: 0.139 - ETA: 1:39 - loss: 0.139 - ETA: 1:38 - loss: 0.139 - ETA: 1:37 - loss: 0.139 - ETA: 1:36 - loss: 0.138 - ETA: 1:35 - loss: 0.138 - ETA: 1:34 - loss: 0.138 - ETA: 1:33 - loss: 0.138 - ETA: 1:32 - loss: 0.138 - ETA: 1:32 - loss: 0.138 - ETA: 1:31 - loss: 0.138 - ETA: 1:30 - loss: 0.138 - ETA: 1:29 - loss: 0.138 - ETA: 1:28 - loss: 0.138 - ETA: 1:27 - loss: 0.138 - ETA: 1:26 - loss: 0.138 - ETA: 1:26 - loss: 0.138 - ETA: 1:25 - loss: 0.138 - ETA: 1:24 - loss: 0.138 - ETA: 1:23 - loss: 0.138 - ETA: 1:22 - loss: 0.138 - ETA: 1:21 - loss: 0.138 - ETA: 1:20 - loss: 0.138 - ETA: 1:19 - loss: 0.138 - ETA: 1:19 - loss: 0.138 - ETA: 1:18 - loss: 0.138 - ETA: 1:17 - loss: 0.138 - ETA: 1:16 - loss: 0.138 - ETA: 1:15 - loss: 0.138 - ETA: 1:14 - loss: 0.138 - ETA: 1:13 - loss: 0.138 - ETA: 1:13 - loss: 0.138 - ETA: 1:12 - loss: 0.138 - ETA: 1:11 - loss: 0.138 - ETA: 1:10 - loss: 0.137 - ETA: 1:09 - loss: 0.137 - ETA: 1:08 - loss: 0.137 - ETA: 1:07 - loss: 0.137 - ETA: 1:06 - loss: 0.137 - ETA: 1:06 - loss: 0.137 - ETA: 1:05 - loss: 0.137 - ETA: 1:04 - loss: 0.137 - ETA: 1:03 - loss: 0.138 - ETA: 1:02 - loss: 0.138 - ETA: 1:01 - loss: 0.138 - ETA: 1:00 - loss: 0.137 - ETA: 59s - loss: 0.138 - ETA: 59s - loss: 0.13 - ETA: 58s - loss: 0.13 - ETA: 57s - loss: 0.13 - ETA: 56s - loss: 0.13 - ETA: 55s - loss: 0.13 - ETA: 54s - loss: 0.13 - ETA: 53s - loss: 0.13 - ETA: 53s - loss: 0.13 - ETA: 52s - loss: 0.13 - ETA: 51s - loss: 0.13 - ETA: 50s - loss: 0.13 - ETA: 49s - loss: 0.13 - ETA: 48s - loss: 0.13 - ETA: 47s - loss: 0.13 - ETA: 46s - loss: 0.13 - ETA: 46s - loss: 0.13 - ETA: 45s - loss: 0.13 - ETA: 44s - loss: 0.13 - ETA: 43s - loss: 0.13 - ETA: 42s - loss: 0.13 - ETA: 41s - loss: 0.13 - ETA: 40s - loss: 0.13 - ETA: 40s - loss: 0.13 - ETA: 39s - loss: 0.13 - ETA: 38s - loss: 0.13 - ETA: 37s - loss: 0.13 - ETA: 36s - loss: 0.13 - ETA: 35s - loss: 0.13 - ETA: 34s - loss: 0.13 - ETA: 33s - loss: 0.13 - ETA: 33s - loss: 0.13 - ETA: 32s - loss: 0.13 - ETA: 31s - loss: 0.13 - ETA: 30s - loss: 0.13 - ETA: 29s - loss: 0.13 - ETA: 28s - loss: 0.13 - ETA: 27s - loss: 0.13 - ETA: 27s - loss: 0.13 - ETA: 26s - loss: 0.13 - ETA: 25s - loss: 0.13 - ETA: 24s - loss: 0.13 - ETA: 23s - loss: 0.13 - ETA: 22s - loss: 0.13 - ETA: 21s - loss: 0.13 - ETA: 20s - loss: 0.13 - ETA: 20s - loss: 0.13 - ETA: 19s - loss: 0.13 - ETA: 18s - loss: 0.13 - ETA: 17s - loss: 0.13 - ETA: 16s - loss: 0.13 - ETA: 15s - loss: 0.13 - ETA: 14s - loss: 0.13 - ETA: 14s - loss: 0.13 - ETA: 13s - loss: 0.13 - ETA: 12s - loss: 0.13 - ETA: 11s - loss: 0.13 - ETA: 10s - loss: 0.13 - ETA: 9s - loss: 0.1379 - ETA: 8s - loss: 0.138 - ETA: 7s - loss: 0.138 - ETA: 7s - loss: 0.137 - ETA: 6s - loss: 0.137 - ETA: 5s - loss: 0.138 - ETA: 4s - loss: 0.137 - ETA: 3s - loss: 0.137 - ETA: 2s - loss: 0.137 - ETA: 1s - loss: 0.137 - ETA: 1s - loss: 0.137 - ETA: 0s - loss: 0.138 - 212s 28ms/step - loss: 0.1379 - val_loss: 0.1372\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7591/7591 [==============================] - ETA: 3:24 - loss: 0.126 - ETA: 3:25 - loss: 0.127 - ETA: 3:24 - loss: 0.125 - ETA: 3:23 - loss: 0.134 - ETA: 3:23 - loss: 0.139 - ETA: 3:22 - loss: 0.137 - ETA: 3:21 - loss: 0.138 - ETA: 3:21 - loss: 0.138 - ETA: 3:20 - loss: 0.139 - ETA: 3:18 - loss: 0.140 - ETA: 3:18 - loss: 0.140 - ETA: 3:16 - loss: 0.140 - ETA: 3:16 - loss: 0.141 - ETA: 3:15 - loss: 0.141 - ETA: 3:14 - loss: 0.142 - ETA: 3:13 - loss: 0.144 - ETA: 3:11 - loss: 0.142 - ETA: 3:10 - loss: 0.143 - ETA: 3:09 - loss: 0.143 - ETA: 3:09 - loss: 0.143 - ETA: 3:08 - loss: 0.143 - ETA: 3:07 - loss: 0.144 - ETA: 3:06 - loss: 0.144 - ETA: 3:05 - loss: 0.144 - ETA: 3:04 - loss: 0.144 - ETA: 3:03 - loss: 0.143 - ETA: 3:02 - loss: 0.142 - ETA: 3:01 - loss: 0.142 - ETA: 3:00 - loss: 0.142 - ETA: 2:59 - loss: 0.141 - ETA: 2:58 - loss: 0.141 - ETA: 2:57 - loss: 0.140 - ETA: 2:56 - loss: 0.140 - ETA: 2:56 - loss: 0.139 - ETA: 2:55 - loss: 0.139 - ETA: 2:54 - loss: 0.139 - ETA: 2:53 - loss: 0.139 - ETA: 2:52 - loss: 0.139 - ETA: 2:51 - loss: 0.138 - ETA: 2:51 - loss: 0.138 - ETA: 2:50 - loss: 0.138 - ETA: 2:49 - loss: 0.138 - ETA: 2:48 - loss: 0.138 - ETA: 2:47 - loss: 0.138 - ETA: 2:46 - loss: 0.138 - ETA: 2:46 - loss: 0.138 - ETA: 2:45 - loss: 0.139 - ETA: 2:44 - loss: 0.140 - ETA: 2:43 - loss: 0.139 - ETA: 2:43 - loss: 0.139 - ETA: 2:42 - loss: 0.139 - ETA: 2:41 - loss: 0.138 - ETA: 2:40 - loss: 0.138 - ETA: 2:39 - loss: 0.138 - ETA: 2:38 - loss: 0.138 - ETA: 2:37 - loss: 0.138 - ETA: 2:36 - loss: 0.138 - ETA: 2:36 - loss: 0.138 - ETA: 2:35 - loss: 0.138 - ETA: 2:34 - loss: 0.138 - ETA: 2:33 - loss: 0.138 - ETA: 2:32 - loss: 0.138 - ETA: 2:31 - loss: 0.138 - ETA: 2:30 - loss: 0.138 - ETA: 2:29 - loss: 0.138 - ETA: 2:28 - loss: 0.138 - ETA: 2:28 - loss: 0.138 - ETA: 2:27 - loss: 0.138 - ETA: 2:26 - loss: 0.138 - ETA: 2:25 - loss: 0.138 - ETA: 2:24 - loss: 0.138 - ETA: 2:24 - loss: 0.138 - ETA: 2:23 - loss: 0.137 - ETA: 2:22 - loss: 0.137 - ETA: 2:21 - loss: 0.137 - ETA: 2:20 - loss: 0.137 - ETA: 2:20 - loss: 0.137 - ETA: 2:19 - loss: 0.137 - ETA: 2:18 - loss: 0.137 - ETA: 2:17 - loss: 0.137 - ETA: 2:17 - loss: 0.137 - ETA: 2:16 - loss: 0.136 - ETA: 2:15 - loss: 0.136 - ETA: 2:14 - loss: 0.136 - ETA: 2:13 - loss: 0.136 - ETA: 2:13 - loss: 0.136 - ETA: 2:12 - loss: 0.136 - ETA: 2:11 - loss: 0.135 - ETA: 2:10 - loss: 0.135 - ETA: 2:09 - loss: 0.135 - ETA: 2:08 - loss: 0.135 - ETA: 2:07 - loss: 0.136 - ETA: 2:07 - loss: 0.135 - ETA: 2:06 - loss: 0.135 - ETA: 2:05 - loss: 0.135 - ETA: 2:04 - loss: 0.135 - ETA: 2:03 - loss: 0.135 - ETA: 2:02 - loss: 0.135 - ETA: 2:01 - loss: 0.135 - ETA: 2:00 - loss: 0.135 - ETA: 1:59 - loss: 0.135 - ETA: 1:59 - loss: 0.135 - ETA: 1:58 - loss: 0.135 - ETA: 1:57 - loss: 0.135 - ETA: 1:56 - loss: 0.135 - ETA: 1:55 - loss: 0.135 - ETA: 1:54 - loss: 0.135 - ETA: 1:53 - loss: 0.135 - ETA: 1:52 - loss: 0.135 - ETA: 1:52 - loss: 0.135 - ETA: 1:51 - loss: 0.135 - ETA: 1:50 - loss: 0.135 - ETA: 1:49 - loss: 0.135 - ETA: 1:48 - loss: 0.135 - ETA: 1:47 - loss: 0.135 - ETA: 1:46 - loss: 0.135 - ETA: 1:45 - loss: 0.135 - ETA: 1:45 - loss: 0.135 - ETA: 1:44 - loss: 0.135 - ETA: 1:43 - loss: 0.135 - ETA: 1:42 - loss: 0.135 - ETA: 1:41 - loss: 0.135 - ETA: 1:40 - loss: 0.135 - ETA: 1:39 - loss: 0.135 - ETA: 1:38 - loss: 0.135 - ETA: 1:37 - loss: 0.135 - ETA: 1:37 - loss: 0.135 - ETA: 1:36 - loss: 0.135 - ETA: 1:35 - loss: 0.135 - ETA: 1:34 - loss: 0.135 - ETA: 1:33 - loss: 0.135 - ETA: 1:32 - loss: 0.135 - ETA: 1:31 - loss: 0.135 - ETA: 1:30 - loss: 0.135 - ETA: 1:30 - loss: 0.135 - ETA: 1:29 - loss: 0.135 - ETA: 1:28 - loss: 0.135 - ETA: 1:27 - loss: 0.135 - ETA: 1:26 - loss: 0.135 - ETA: 1:25 - loss: 0.135 - ETA: 1:24 - loss: 0.135 - ETA: 1:23 - loss: 0.135 - ETA: 1:22 - loss: 0.135 - ETA: 1:22 - loss: 0.135 - ETA: 1:21 - loss: 0.135 - ETA: 1:20 - loss: 0.135 - ETA: 1:19 - loss: 0.135 - ETA: 1:18 - loss: 0.135 - ETA: 1:17 - loss: 0.135 - ETA: 1:16 - loss: 0.135 - ETA: 1:15 - loss: 0.135 - ETA: 1:15 - loss: 0.135 - ETA: 1:14 - loss: 0.135 - ETA: 1:13 - loss: 0.135 - ETA: 1:12 - loss: 0.135 - ETA: 1:11 - loss: 0.135 - ETA: 1:10 - loss: 0.135 - ETA: 1:09 - loss: 0.135 - ETA: 1:08 - loss: 0.135 - ETA: 1:07 - loss: 0.135 - ETA: 1:07 - loss: 0.135 - ETA: 1:06 - loss: 0.135 - ETA: 1:05 - loss: 0.135 - ETA: 1:04 - loss: 0.135 - ETA: 1:03 - loss: 0.135 - ETA: 1:02 - loss: 0.135 - ETA: 1:01 - loss: 0.135 - ETA: 1:00 - loss: 0.135 - ETA: 1:00 - loss: 0.135 - ETA: 59s - loss: 0.135 - ETA: 58s - loss: 0.13 - ETA: 57s - loss: 0.13 - ETA: 56s - loss: 0.13 - ETA: 55s - loss: 0.13 - ETA: 54s - loss: 0.13 - ETA: 53s - loss: 0.13 - ETA: 53s - loss: 0.13 - ETA: 52s - loss: 0.13 - ETA: 51s - loss: 0.13 - ETA: 50s - loss: 0.13 - ETA: 49s - loss: 0.13 - ETA: 48s - loss: 0.13 - ETA: 47s - loss: 0.13 - ETA: 46s - loss: 0.13 - ETA: 46s - loss: 0.13 - ETA: 45s - loss: 0.13 - ETA: 44s - loss: 0.13 - ETA: 43s - loss: 0.13 - ETA: 42s - loss: 0.13 - ETA: 41s - loss: 0.13 - ETA: 40s - loss: 0.13 - ETA: 39s - loss: 0.13 - ETA: 38s - loss: 0.13 - ETA: 38s - loss: 0.13 - ETA: 37s - loss: 0.13 - ETA: 36s - loss: 0.13 - ETA: 35s - loss: 0.13 - ETA: 34s - loss: 0.13 - ETA: 33s - loss: 0.13 - ETA: 32s - loss: 0.13 - ETA: 31s - loss: 0.13 - ETA: 31s - loss: 0.13 - ETA: 30s - loss: 0.13 - ETA: 29s - loss: 0.13 - ETA: 28s - loss: 0.13 - ETA: 27s - loss: 0.13 - ETA: 26s - loss: 0.13 - ETA: 25s - loss: 0.13 - ETA: 24s - loss: 0.13 - ETA: 23s - loss: 0.13 - ETA: 23s - loss: 0.13 - ETA: 22s - loss: 0.13 - ETA: 21s - loss: 0.13 - ETA: 20s - loss: 0.13 - ETA: 19s - loss: 0.13 - ETA: 18s - loss: 0.13 - ETA: 17s - loss: 0.13 - ETA: 16s - loss: 0.13 - ETA: 16s - loss: 0.13 - ETA: 15s - loss: 0.13 - ETA: 14s - loss: 0.13 - ETA: 13s - loss: 0.13 - ETA: 12s - loss: 0.13 - ETA: 11s - loss: 0.13 - ETA: 10s - loss: 0.13 - ETA: 9s - loss: 0.1352 - ETA: 8s - loss: 0.135 - ETA: 8s - loss: 0.135 - ETA: 7s - loss: 0.135 - ETA: 6s - loss: 0.135 - ETA: 5s - loss: 0.135 - ETA: 4s - loss: 0.135 - ETA: 3s - loss: 0.135 - ETA: 2s - loss: 0.135 - ETA: 1s - loss: 0.135 - ETA: 1s - loss: 0.135 - ETA: 0s - loss: 0.135 - 215s 28ms/step - loss: 0.1351 - val_loss: 0.1375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bde1be1908>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "# log_path=str('C:\\Users\\Cyborg\\Documents\\GitHub\\Movie-Plot-Synopses')\n",
    "# model_saver =  ModelCheckpoint(log_path + \"/model.ckpt.{epoch:04d}.hdf5\", monitor='loss', period=1)\n",
    "# callbacks = [tensorboard, TerminateOnNaN(), model_saver]\n",
    "\n",
    "\n",
    "model.fit(padded_docs_train, y_train,\n",
    "          class_weight='balanced',\n",
    "          epochs=5,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2)\n",
    "predictions=model.predict(padded_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1251, Recall: 0.7809, F1-measure: 0.2156\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2226, Recall: 0.5653, F1-measure: 0.3194\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3160, Recall: 0.4000, F1-measure: 0.3531\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4171, Recall: 0.2792, F1-measure: 0.3345\n",
      "For threshold:  0.5\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5096, Recall: 0.2007, F1-measure: 0.2880\n",
      "For threshold:  0.6\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5935, Recall: 0.1492, F1-measure: 0.2384\n",
      "For threshold:  0.7\n",
      "Micro-average quality numbers\n",
      "Precision: 0.6544, Recall: 0.1081, F1-measure: 0.1855\n",
      "For threshold:  0.8\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7083, Recall: 0.0673, F1-measure: 0.1229\n",
      "For threshold:  0.9\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7346, Recall: 0.0252, F1-measure: 0.0487\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict([padded_docs_test])\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1200, 50)          4939100   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 71)                9159      \n",
      "=================================================================\n",
      "Total params: 5,039,907\n",
      "Trainable params: 5,039,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1micro', verbose=0, save_best_only=True, mode='max')\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "# model.add(LSTM(6119, input_shape=(timesteps, input_dim)))\n",
    "model.add(Embedding(vocab_size, output_dim=50, input_length=1200))\n",
    "model.add(LSTM(128))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()\n",
    "# pred=[]\n",
    "# x= model.predict(y_test)\n",
    "# for i in x:\n",
    "#     pred.append(i[:])  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1200, 50)          4939100   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 71)                9159      \n",
      "=================================================================\n",
      "Total params: 5,039,907\n",
      "Trainable params: 5,039,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8540 samples, validate on 949 samples\n",
      "Epoch 1/5\n",
      "8540/8540 [==============================] - ETA: 15:00 - loss: 0.69 - ETA: 11:01 - loss: 0.69 - ETA: 9:44 - loss: 0.6914 - ETA: 9:02 - loss: 0.689 - ETA: 8:38 - loss: 0.688 - ETA: 8:21 - loss: 0.686 - ETA: 8:08 - loss: 0.684 - ETA: 7:58 - loss: 0.681 - ETA: 7:49 - loss: 0.678 - ETA: 7:43 - loss: 0.673 - ETA: 7:38 - loss: 0.665 - ETA: 7:33 - loss: 0.653 - ETA: 7:28 - loss: 0.639 - ETA: 7:25 - loss: 0.623 - ETA: 7:22 - loss: 0.607 - ETA: 7:18 - loss: 0.592 - ETA: 7:15 - loss: 0.576 - ETA: 7:11 - loss: 0.563 - ETA: 7:08 - loss: 0.548 - ETA: 7:06 - loss: 0.534 - ETA: 7:03 - loss: 0.521 - ETA: 7:00 - loss: 0.510 - ETA: 6:57 - loss: 0.499 - ETA: 6:55 - loss: 0.488 - ETA: 6:53 - loss: 0.477 - ETA: 6:51 - loss: 0.467 - ETA: 6:49 - loss: 0.457 - ETA: 6:46 - loss: 0.447 - ETA: 6:44 - loss: 0.437 - ETA: 6:42 - loss: 0.427 - ETA: 6:40 - loss: 0.419 - ETA: 6:38 - loss: 0.411 - ETA: 6:35 - loss: 0.404 - ETA: 6:33 - loss: 0.397 - ETA: 6:31 - loss: 0.390 - ETA: 6:29 - loss: 0.384 - ETA: 6:27 - loss: 0.378 - ETA: 6:25 - loss: 0.372 - ETA: 6:23 - loss: 0.366 - ETA: 6:21 - loss: 0.361 - ETA: 6:19 - loss: 0.356 - ETA: 6:17 - loss: 0.352 - ETA: 6:16 - loss: 0.348 - ETA: 6:14 - loss: 0.345 - ETA: 6:12 - loss: 0.341 - ETA: 6:10 - loss: 0.337 - ETA: 6:08 - loss: 0.333 - ETA: 6:06 - loss: 0.329 - ETA: 6:04 - loss: 0.326 - ETA: 6:02 - loss: 0.322 - ETA: 6:01 - loss: 0.319 - ETA: 5:59 - loss: 0.315 - ETA: 5:57 - loss: 0.312 - ETA: 5:55 - loss: 0.310 - ETA: 5:53 - loss: 0.307 - ETA: 5:52 - loss: 0.304 - ETA: 5:50 - loss: 0.302 - ETA: 5:48 - loss: 0.300 - ETA: 5:46 - loss: 0.297 - ETA: 5:45 - loss: 0.295 - ETA: 5:43 - loss: 0.293 - ETA: 5:41 - loss: 0.291 - ETA: 5:39 - loss: 0.289 - ETA: 5:38 - loss: 0.287 - ETA: 5:36 - loss: 0.285 - ETA: 5:34 - loss: 0.283 - ETA: 5:32 - loss: 0.281 - ETA: 5:31 - loss: 0.279 - ETA: 5:29 - loss: 0.278 - ETA: 5:27 - loss: 0.276 - ETA: 5:25 - loss: 0.275 - ETA: 5:24 - loss: 0.273 - ETA: 5:22 - loss: 0.271 - ETA: 5:20 - loss: 0.270 - ETA: 5:18 - loss: 0.268 - ETA: 5:17 - loss: 0.266 - ETA: 5:15 - loss: 0.265 - ETA: 5:13 - loss: 0.264 - ETA: 5:12 - loss: 0.262 - ETA: 5:10 - loss: 0.261 - ETA: 5:08 - loss: 0.259 - ETA: 5:06 - loss: 0.258 - ETA: 5:05 - loss: 0.256 - ETA: 5:03 - loss: 0.255 - ETA: 5:01 - loss: 0.254 - ETA: 4:59 - loss: 0.252 - ETA: 4:58 - loss: 0.251 - ETA: 4:56 - loss: 0.250 - ETA: 4:54 - loss: 0.249 - ETA: 4:53 - loss: 0.248 - ETA: 4:51 - loss: 0.248 - ETA: 4:49 - loss: 0.246 - ETA: 4:48 - loss: 0.245 - ETA: 4:46 - loss: 0.244 - ETA: 4:44 - loss: 0.243 - ETA: 4:42 - loss: 0.242 - ETA: 4:41 - loss: 0.241 - ETA: 4:39 - loss: 0.241 - ETA: 4:37 - loss: 0.240 - ETA: 4:36 - loss: 0.239 - ETA: 4:34 - loss: 0.238 - ETA: 4:32 - loss: 0.237 - ETA: 4:31 - loss: 0.236 - ETA: 4:29 - loss: 0.235 - ETA: 4:27 - loss: 0.234 - ETA: 4:26 - loss: 0.234 - ETA: 4:24 - loss: 0.233 - ETA: 4:22 - loss: 0.232 - ETA: 4:20 - loss: 0.231 - ETA: 4:19 - loss: 0.231 - ETA: 4:17 - loss: 0.230 - ETA: 4:16 - loss: 0.230 - ETA: 4:14 - loss: 0.229 - ETA: 4:12 - loss: 0.228 - ETA: 4:10 - loss: 0.227 - ETA: 4:09 - loss: 0.227 - ETA: 4:07 - loss: 0.226 - ETA: 4:05 - loss: 0.226 - ETA: 4:04 - loss: 0.225 - ETA: 4:02 - loss: 0.224 - ETA: 4:00 - loss: 0.224 - ETA: 3:59 - loss: 0.223 - ETA: 3:57 - loss: 0.223 - ETA: 3:55 - loss: 0.223 - ETA: 3:54 - loss: 0.222 - ETA: 3:52 - loss: 0.221 - ETA: 3:50 - loss: 0.221 - ETA: 3:49 - loss: 0.220 - ETA: 3:47 - loss: 0.220 - ETA: 3:46 - loss: 0.219 - ETA: 3:44 - loss: 0.219 - ETA: 3:42 - loss: 0.218 - ETA: 3:41 - loss: 0.218 - ETA: 3:39 - loss: 0.217 - ETA: 3:37 - loss: 0.217 - ETA: 3:36 - loss: 0.216 - ETA: 3:34 - loss: 0.216 - ETA: 3:32 - loss: 0.215 - ETA: 3:31 - loss: 0.215 - ETA: 3:29 - loss: 0.215 - ETA: 3:27 - loss: 0.215 - ETA: 3:26 - loss: 0.214 - ETA: 3:24 - loss: 0.214 - ETA: 3:22 - loss: 0.213 - ETA: 3:21 - loss: 0.213 - ETA: 3:19 - loss: 0.213 - ETA: 3:17 - loss: 0.213 - ETA: 3:16 - loss: 0.212 - ETA: 3:14 - loss: 0.212 - ETA: 3:12 - loss: 0.212 - ETA: 3:11 - loss: 0.211 - ETA: 3:09 - loss: 0.211 - ETA: 3:07 - loss: 0.210 - ETA: 3:06 - loss: 0.210 - ETA: 3:04 - loss: 0.209 - ETA: 3:02 - loss: 0.209 - ETA: 3:01 - loss: 0.208 - ETA: 2:59 - loss: 0.208 - ETA: 2:57 - loss: 0.208 - ETA: 2:56 - loss: 0.207 - ETA: 2:54 - loss: 0.207 - ETA: 2:53 - loss: 0.207 - ETA: 2:51 - loss: 0.206 - ETA: 2:49 - loss: 0.206 - ETA: 2:48 - loss: 0.206 - ETA: 2:46 - loss: 0.206 - ETA: 2:44 - loss: 0.205 - ETA: 2:43 - loss: 0.205 - ETA: 2:41 - loss: 0.204 - ETA: 2:39 - loss: 0.204 - ETA: 2:38 - loss: 0.204 - ETA: 2:36 - loss: 0.204 - ETA: 2:34 - loss: 0.204 - ETA: 2:33 - loss: 0.203 - ETA: 2:31 - loss: 0.203 - ETA: 2:29 - loss: 0.203 - ETA: 2:28 - loss: 0.202 - ETA: 2:26 - loss: 0.202 - ETA: 2:24 - loss: 0.202 - ETA: 2:23 - loss: 0.201 - ETA: 2:21 - loss: 0.201 - ETA: 2:19 - loss: 0.201 - ETA: 2:18 - loss: 0.200 - ETA: 2:16 - loss: 0.200 - ETA: 2:14 - loss: 0.200 - ETA: 2:13 - loss: 0.200 - ETA: 2:11 - loss: 0.199 - ETA: 2:09 - loss: 0.199 - ETA: 2:08 - loss: 0.199 - ETA: 2:06 - loss: 0.199 - ETA: 2:04 - loss: 0.199 - ETA: 2:03 - loss: 0.199 - ETA: 2:01 - loss: 0.198 - ETA: 2:00 - loss: 0.198 - ETA: 1:58 - loss: 0.197 - ETA: 1:56 - loss: 0.197 - ETA: 1:55 - loss: 0.197 - ETA: 1:53 - loss: 0.197 - ETA: 1:51 - loss: 0.197 - ETA: 1:50 - loss: 0.196 - ETA: 1:48 - loss: 0.196 - ETA: 1:46 - loss: 0.196 - ETA: 1:45 - loss: 0.196 - ETA: 1:43 - loss: 0.196 - ETA: 1:41 - loss: 0.195 - ETA: 1:40 - loss: 0.195 - ETA: 1:38 - loss: 0.195 - ETA: 1:36 - loss: 0.195 - ETA: 1:35 - loss: 0.195 - ETA: 1:33 - loss: 0.195 - ETA: 1:31 - loss: 0.194 - ETA: 1:30 - loss: 0.194 - ETA: 1:28 - loss: 0.194 - ETA: 1:27 - loss: 0.194 - ETA: 1:25 - loss: 0.194 - ETA: 1:23 - loss: 0.194 - ETA: 1:22 - loss: 0.193 - ETA: 1:20 - loss: 0.193 - ETA: 1:18 - loss: 0.193 - ETA: 1:17 - loss: 0.193 - ETA: 1:15 - loss: 0.193 - ETA: 1:13 - loss: 0.193 - ETA: 1:12 - loss: 0.193 - ETA: 1:10 - loss: 0.192 - ETA: 1:08 - loss: 0.192 - ETA: 1:07 - loss: 0.192 - ETA: 1:05 - loss: 0.192 - ETA: 1:03 - loss: 0.192 - ETA: 1:02 - loss: 0.191 - ETA: 1:00 - loss: 0.191 - ETA: 59s - loss: 0.191 - ETA: 57s - loss: 0.19 - ETA: 55s - loss: 0.19 - ETA: 54s - loss: 0.19 - ETA: 52s - loss: 0.19 - ETA: 50s - loss: 0.19 - ETA: 49s - loss: 0.19 - ETA: 47s - loss: 0.19 - ETA: 45s - loss: 0.18 - ETA: 44s - loss: 0.18 - ETA: 42s - loss: 0.18 - ETA: 40s - loss: 0.18 - ETA: 39s - loss: 0.18 - ETA: 37s - loss: 0.18 - ETA: 35s - loss: 0.18 - ETA: 34s - loss: 0.18 - ETA: 32s - loss: 0.18 - ETA: 31s - loss: 0.18 - ETA: 29s - loss: 0.18 - ETA: 27s - loss: 0.18 - ETA: 26s - loss: 0.18 - ETA: 24s - loss: 0.18 - ETA: 22s - loss: 0.18 - ETA: 21s - loss: 0.18 - ETA: 19s - loss: 0.18 - ETA: 17s - loss: 0.18 - ETA: 16s - loss: 0.18 - ETA: 14s - loss: 0.18 - ETA: 12s - loss: 0.18 - ETA: 11s - loss: 0.18 - ETA: 9s - loss: 0.1872 - ETA: 8s - loss: 0.187 - ETA: 6s - loss: 0.186 - ETA: 4s - loss: 0.186 - ETA: 3s - loss: 0.186 - ETA: 1s - loss: 0.186 - 446s 52ms/step - loss: 0.1865 - val_loss: 0.1104\n",
      "Epoch 2/5\n",
      "8540/8540 [==============================] - ETA: 6:19 - loss: 0.166 - ETA: 6:16 - loss: 0.160 - ETA: 6:13 - loss: 0.170 - ETA: 6:12 - loss: 0.160 - ETA: 6:09 - loss: 0.154 - ETA: 6:07 - loss: 0.156 - ETA: 6:06 - loss: 0.154 - ETA: 6:04 - loss: 0.153 - ETA: 6:02 - loss: 0.153 - ETA: 6:01 - loss: 0.154 - ETA: 6:00 - loss: 0.153 - ETA: 5:59 - loss: 0.154 - ETA: 5:59 - loss: 0.153 - ETA: 5:57 - loss: 0.155 - ETA: 5:56 - loss: 0.156 - ETA: 5:54 - loss: 0.157 - ETA: 5:53 - loss: 0.158 - ETA: 5:52 - loss: 0.157 - ETA: 5:50 - loss: 0.158 - ETA: 5:49 - loss: 0.157 - ETA: 5:47 - loss: 0.158 - ETA: 5:46 - loss: 0.159 - ETA: 5:44 - loss: 0.158 - ETA: 5:43 - loss: 0.159 - ETA: 5:42 - loss: 0.157 - ETA: 5:40 - loss: 0.156 - ETA: 5:38 - loss: 0.156 - ETA: 5:37 - loss: 0.156 - ETA: 5:35 - loss: 0.156 - ETA: 5:34 - loss: 0.156 - ETA: 5:32 - loss: 0.157 - ETA: 5:31 - loss: 0.156 - ETA: 5:30 - loss: 0.155 - ETA: 5:28 - loss: 0.156 - ETA: 5:26 - loss: 0.156 - ETA: 5:25 - loss: 0.155 - ETA: 5:23 - loss: 0.155 - ETA: 5:22 - loss: 0.156 - ETA: 5:20 - loss: 0.155 - ETA: 5:19 - loss: 0.155 - ETA: 5:17 - loss: 0.155 - ETA: 5:16 - loss: 0.154 - ETA: 5:14 - loss: 0.154 - ETA: 5:13 - loss: 0.154 - ETA: 5:12 - loss: 0.154 - ETA: 5:10 - loss: 0.154 - ETA: 5:09 - loss: 0.155 - ETA: 5:08 - loss: 0.154 - ETA: 5:07 - loss: 0.154 - ETA: 5:05 - loss: 0.153 - ETA: 5:04 - loss: 0.153 - ETA: 5:03 - loss: 0.153 - ETA: 5:01 - loss: 0.153 - ETA: 5:00 - loss: 0.152 - ETA: 4:58 - loss: 0.152 - ETA: 4:57 - loss: 0.152 - ETA: 4:55 - loss: 0.152 - ETA: 4:54 - loss: 0.152 - ETA: 4:53 - loss: 0.152 - ETA: 4:51 - loss: 0.152 - ETA: 4:50 - loss: 0.152 - ETA: 4:48 - loss: 0.152 - ETA: 4:47 - loss: 0.151 - ETA: 4:46 - loss: 0.152 - ETA: 4:44 - loss: 0.151 - ETA: 4:43 - loss: 0.151 - ETA: 4:41 - loss: 0.152 - ETA: 4:40 - loss: 0.152 - ETA: 4:38 - loss: 0.151 - ETA: 4:37 - loss: 0.151 - ETA: 4:36 - loss: 0.152 - ETA: 4:34 - loss: 0.152 - ETA: 4:33 - loss: 0.152 - ETA: 4:31 - loss: 0.152 - ETA: 4:30 - loss: 0.152 - ETA: 4:28 - loss: 0.152 - ETA: 4:27 - loss: 0.151 - ETA: 4:26 - loss: 0.151 - ETA: 4:24 - loss: 0.152 - ETA: 4:23 - loss: 0.152 - ETA: 4:21 - loss: 0.152 - ETA: 4:20 - loss: 0.152 - ETA: 4:19 - loss: 0.151 - ETA: 4:17 - loss: 0.152 - ETA: 4:16 - loss: 0.152 - ETA: 4:14 - loss: 0.151 - ETA: 4:13 - loss: 0.151 - ETA: 4:11 - loss: 0.151 - ETA: 4:10 - loss: 0.151 - ETA: 4:09 - loss: 0.151 - ETA: 4:07 - loss: 0.151 - ETA: 4:06 - loss: 0.151 - ETA: 4:04 - loss: 0.151 - ETA: 4:03 - loss: 0.151 - ETA: 4:02 - loss: 0.151 - ETA: 4:00 - loss: 0.151 - ETA: 3:59 - loss: 0.151 - ETA: 3:57 - loss: 0.151 - ETA: 3:56 - loss: 0.151 - ETA: 3:55 - loss: 0.151 - ETA: 3:53 - loss: 0.151 - ETA: 3:52 - loss: 0.151 - ETA: 3:50 - loss: 0.151 - ETA: 3:49 - loss: 0.151 - ETA: 3:48 - loss: 0.151 - ETA: 3:46 - loss: 0.151 - ETA: 3:45 - loss: 0.151 - ETA: 3:43 - loss: 0.151 - ETA: 3:42 - loss: 0.151 - ETA: 3:40 - loss: 0.151 - ETA: 3:39 - loss: 0.151 - ETA: 3:38 - loss: 0.151 - ETA: 3:36 - loss: 0.151 - ETA: 3:35 - loss: 0.151 - ETA: 3:33 - loss: 0.151 - ETA: 3:32 - loss: 0.151 - ETA: 3:31 - loss: 0.151 - ETA: 3:29 - loss: 0.151 - ETA: 3:28 - loss: 0.151 - ETA: 3:26 - loss: 0.151 - ETA: 3:25 - loss: 0.151 - ETA: 3:23 - loss: 0.151 - ETA: 3:22 - loss: 0.151 - ETA: 3:21 - loss: 0.151 - ETA: 3:19 - loss: 0.150 - ETA: 3:18 - loss: 0.150 - ETA: 3:16 - loss: 0.150 - ETA: 3:15 - loss: 0.150 - ETA: 3:14 - loss: 0.150 - ETA: 3:12 - loss: 0.150 - ETA: 3:11 - loss: 0.150 - ETA: 3:09 - loss: 0.150 - ETA: 3:08 - loss: 0.150 - ETA: 3:07 - loss: 0.150 - ETA: 3:05 - loss: 0.150 - ETA: 3:04 - loss: 0.150 - ETA: 3:02 - loss: 0.150 - ETA: 3:01 - loss: 0.150 - ETA: 2:59 - loss: 0.150 - ETA: 2:58 - loss: 0.150 - ETA: 2:57 - loss: 0.150 - ETA: 2:55 - loss: 0.150 - ETA: 2:54 - loss: 0.150 - ETA: 2:52 - loss: 0.150 - ETA: 2:51 - loss: 0.150 - ETA: 2:50 - loss: 0.150 - ETA: 2:48 - loss: 0.150 - ETA: 2:47 - loss: 0.150 - ETA: 2:45 - loss: 0.150 - ETA: 2:44 - loss: 0.150 - ETA: 2:43 - loss: 0.150 - ETA: 2:41 - loss: 0.150 - ETA: 2:40 - loss: 0.150 - ETA: 2:38 - loss: 0.150 - ETA: 2:37 - loss: 0.150 - ETA: 2:36 - loss: 0.150 - ETA: 2:34 - loss: 0.150 - ETA: 2:33 - loss: 0.149 - ETA: 2:31 - loss: 0.150 - ETA: 2:30 - loss: 0.149 - ETA: 2:28 - loss: 0.149 - ETA: 2:27 - loss: 0.150 - ETA: 2:26 - loss: 0.150 - ETA: 2:24 - loss: 0.149 - ETA: 2:23 - loss: 0.149 - ETA: 2:21 - loss: 0.149 - ETA: 2:20 - loss: 0.149 - ETA: 2:19 - loss: 0.149 - ETA: 2:17 - loss: 0.149 - ETA: 2:16 - loss: 0.149 - ETA: 2:14 - loss: 0.149 - ETA: 2:13 - loss: 0.149 - ETA: 2:12 - loss: 0.149 - ETA: 2:10 - loss: 0.149 - ETA: 2:09 - loss: 0.149 - ETA: 2:07 - loss: 0.149 - ETA: 2:06 - loss: 0.149 - ETA: 2:05 - loss: 0.149 - ETA: 2:03 - loss: 0.149 - ETA: 2:02 - loss: 0.149 - ETA: 2:00 - loss: 0.149 - ETA: 1:59 - loss: 0.149 - ETA: 1:58 - loss: 0.149 - ETA: 1:56 - loss: 0.149 - ETA: 1:55 - loss: 0.149 - ETA: 1:53 - loss: 0.149 - ETA: 1:52 - loss: 0.149 - ETA: 1:51 - loss: 0.149 - ETA: 1:49 - loss: 0.149 - ETA: 1:48 - loss: 0.149 - ETA: 1:47 - loss: 0.149 - ETA: 1:45 - loss: 0.149 - ETA: 1:44 - loss: 0.149 - ETA: 1:42 - loss: 0.149 - ETA: 1:41 - loss: 0.149 - ETA: 1:40 - loss: 0.149 - ETA: 1:38 - loss: 0.149 - ETA: 1:37 - loss: 0.150 - ETA: 1:35 - loss: 0.150 - ETA: 1:34 - loss: 0.149 - ETA: 1:32 - loss: 0.149 - ETA: 1:31 - loss: 0.150 - ETA: 1:30 - loss: 0.150 - ETA: 1:28 - loss: 0.149 - ETA: 1:27 - loss: 0.149 - ETA: 1:25 - loss: 0.149 - ETA: 1:24 - loss: 0.149 - ETA: 1:23 - loss: 0.149 - ETA: 1:21 - loss: 0.149 - ETA: 1:20 - loss: 0.149 - ETA: 1:18 - loss: 0.149 - ETA: 1:17 - loss: 0.149 - ETA: 1:16 - loss: 0.149 - ETA: 1:14 - loss: 0.149 - ETA: 1:13 - loss: 0.149 - ETA: 1:11 - loss: 0.149 - ETA: 1:10 - loss: 0.149 - ETA: 1:08 - loss: 0.149 - ETA: 1:07 - loss: 0.149 - ETA: 1:06 - loss: 0.149 - ETA: 1:04 - loss: 0.149 - ETA: 1:03 - loss: 0.149 - ETA: 1:01 - loss: 0.149 - ETA: 1:00 - loss: 0.149 - ETA: 59s - loss: 0.149 - ETA: 57s - loss: 0.14 - ETA: 56s - loss: 0.14 - ETA: 54s - loss: 0.14 - ETA: 53s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 9s - loss: 0.1491 - ETA: 8s - loss: 0.149 - ETA: 6s - loss: 0.149 - ETA: 5s - loss: 0.149 - ETA: 4s - loss: 0.149 - ETA: 2s - loss: 0.149 - ETA: 1s - loss: 0.148 - 384s 45ms/step - loss: 0.1489 - val_loss: 0.1095\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540/8540 [==============================] - ETA: 6:20 - loss: 0.168 - ETA: 6:13 - loss: 0.159 - ETA: 6:13 - loss: 0.152 - ETA: 6:12 - loss: 0.149 - ETA: 6:10 - loss: 0.148 - ETA: 6:08 - loss: 0.150 - ETA: 6:06 - loss: 0.146 - ETA: 6:04 - loss: 0.144 - ETA: 6:02 - loss: 0.142 - ETA: 6:01 - loss: 0.142 - ETA: 5:59 - loss: 0.145 - ETA: 5:58 - loss: 0.146 - ETA: 5:57 - loss: 0.146 - ETA: 5:55 - loss: 0.148 - ETA: 5:54 - loss: 0.148 - ETA: 5:53 - loss: 0.147 - ETA: 5:51 - loss: 0.146 - ETA: 5:50 - loss: 0.147 - ETA: 5:49 - loss: 0.147 - ETA: 5:47 - loss: 0.148 - ETA: 5:46 - loss: 0.149 - ETA: 5:44 - loss: 0.148 - ETA: 5:43 - loss: 0.149 - ETA: 5:41 - loss: 0.149 - ETA: 5:40 - loss: 0.149 - ETA: 5:38 - loss: 0.148 - ETA: 5:37 - loss: 0.149 - ETA: 5:36 - loss: 0.149 - ETA: 5:34 - loss: 0.149 - ETA: 5:33 - loss: 0.149 - ETA: 5:31 - loss: 0.150 - ETA: 5:30 - loss: 0.150 - ETA: 5:29 - loss: 0.150 - ETA: 5:27 - loss: 0.150 - ETA: 5:26 - loss: 0.149 - ETA: 5:24 - loss: 0.149 - ETA: 5:23 - loss: 0.148 - ETA: 5:22 - loss: 0.150 - ETA: 5:20 - loss: 0.149 - ETA: 5:19 - loss: 0.149 - ETA: 5:17 - loss: 0.148 - ETA: 5:16 - loss: 0.148 - ETA: 5:14 - loss: 0.149 - ETA: 5:13 - loss: 0.148 - ETA: 5:12 - loss: 0.149 - ETA: 5:10 - loss: 0.149 - ETA: 5:09 - loss: 0.148 - ETA: 5:07 - loss: 0.149 - ETA: 5:06 - loss: 0.149 - ETA: 5:05 - loss: 0.150 - ETA: 5:03 - loss: 0.149 - ETA: 5:02 - loss: 0.149 - ETA: 5:00 - loss: 0.149 - ETA: 4:59 - loss: 0.149 - ETA: 4:57 - loss: 0.149 - ETA: 4:56 - loss: 0.149 - ETA: 4:55 - loss: 0.149 - ETA: 4:53 - loss: 0.149 - ETA: 4:52 - loss: 0.150 - ETA: 4:50 - loss: 0.150 - ETA: 4:49 - loss: 0.150 - ETA: 4:47 - loss: 0.149 - ETA: 4:46 - loss: 0.149 - ETA: 4:45 - loss: 0.149 - ETA: 4:43 - loss: 0.149 - ETA: 4:42 - loss: 0.149 - ETA: 4:40 - loss: 0.149 - ETA: 4:39 - loss: 0.149 - ETA: 4:38 - loss: 0.149 - ETA: 4:36 - loss: 0.149 - ETA: 4:35 - loss: 0.149 - ETA: 4:34 - loss: 0.148 - ETA: 4:32 - loss: 0.148 - ETA: 4:31 - loss: 0.148 - ETA: 4:29 - loss: 0.148 - ETA: 4:28 - loss: 0.148 - ETA: 4:26 - loss: 0.148 - ETA: 4:25 - loss: 0.148 - ETA: 4:24 - loss: 0.148 - ETA: 4:22 - loss: 0.148 - ETA: 4:21 - loss: 0.148 - ETA: 4:20 - loss: 0.148 - ETA: 4:18 - loss: 0.148 - ETA: 4:17 - loss: 0.148 - ETA: 4:15 - loss: 0.148 - ETA: 4:14 - loss: 0.148 - ETA: 4:13 - loss: 0.148 - ETA: 4:11 - loss: 0.148 - ETA: 4:10 - loss: 0.148 - ETA: 4:08 - loss: 0.147 - ETA: 4:07 - loss: 0.147 - ETA: 4:06 - loss: 0.148 - ETA: 4:04 - loss: 0.148 - ETA: 4:03 - loss: 0.149 - ETA: 4:01 - loss: 0.148 - ETA: 4:00 - loss: 0.148 - ETA: 3:59 - loss: 0.148 - ETA: 3:57 - loss: 0.148 - ETA: 3:56 - loss: 0.148 - ETA: 3:54 - loss: 0.149 - ETA: 3:53 - loss: 0.148 - ETA: 3:52 - loss: 0.148 - ETA: 3:50 - loss: 0.148 - ETA: 3:49 - loss: 0.148 - ETA: 3:47 - loss: 0.148 - ETA: 3:46 - loss: 0.148 - ETA: 3:44 - loss: 0.148 - ETA: 3:43 - loss: 0.148 - ETA: 3:42 - loss: 0.148 - ETA: 3:40 - loss: 0.148 - ETA: 3:39 - loss: 0.148 - ETA: 3:37 - loss: 0.148 - ETA: 3:36 - loss: 0.148 - ETA: 3:35 - loss: 0.147 - ETA: 3:33 - loss: 0.147 - ETA: 3:32 - loss: 0.148 - ETA: 3:30 - loss: 0.147 - ETA: 3:29 - loss: 0.147 - ETA: 3:28 - loss: 0.147 - ETA: 3:26 - loss: 0.147 - ETA: 3:25 - loss: 0.147 - ETA: 3:23 - loss: 0.147 - ETA: 3:22 - loss: 0.146 - ETA: 3:21 - loss: 0.146 - ETA: 3:19 - loss: 0.147 - ETA: 3:18 - loss: 0.147 - ETA: 3:16 - loss: 0.147 - ETA: 3:15 - loss: 0.147 - ETA: 3:14 - loss: 0.147 - ETA: 3:12 - loss: 0.147 - ETA: 3:11 - loss: 0.147 - ETA: 3:09 - loss: 0.146 - ETA: 3:08 - loss: 0.146 - ETA: 3:07 - loss: 0.147 - ETA: 3:05 - loss: 0.147 - ETA: 3:04 - loss: 0.147 - ETA: 3:02 - loss: 0.147 - ETA: 3:01 - loss: 0.147 - ETA: 3:00 - loss: 0.147 - ETA: 2:58 - loss: 0.147 - ETA: 2:57 - loss: 0.147 - ETA: 2:55 - loss: 0.147 - ETA: 2:54 - loss: 0.147 - ETA: 2:53 - loss: 0.147 - ETA: 2:51 - loss: 0.147 - ETA: 2:50 - loss: 0.147 - ETA: 2:48 - loss: 0.147 - ETA: 2:47 - loss: 0.147 - ETA: 2:46 - loss: 0.147 - ETA: 2:44 - loss: 0.147 - ETA: 2:43 - loss: 0.147 - ETA: 2:41 - loss: 0.147 - ETA: 2:40 - loss: 0.147 - ETA: 2:38 - loss: 0.147 - ETA: 2:37 - loss: 0.147 - ETA: 2:36 - loss: 0.147 - ETA: 2:34 - loss: 0.147 - ETA: 2:33 - loss: 0.147 - ETA: 2:31 - loss: 0.147 - ETA: 2:30 - loss: 0.147 - ETA: 2:29 - loss: 0.147 - ETA: 2:27 - loss: 0.147 - ETA: 2:26 - loss: 0.147 - ETA: 2:24 - loss: 0.147 - ETA: 2:23 - loss: 0.147 - ETA: 2:22 - loss: 0.147 - ETA: 2:20 - loss: 0.146 - ETA: 2:19 - loss: 0.147 - ETA: 2:17 - loss: 0.146 - ETA: 2:16 - loss: 0.146 - ETA: 2:15 - loss: 0.146 - ETA: 2:13 - loss: 0.146 - ETA: 2:12 - loss: 0.146 - ETA: 2:10 - loss: 0.146 - ETA: 2:09 - loss: 0.146 - ETA: 2:08 - loss: 0.146 - ETA: 2:06 - loss: 0.146 - ETA: 2:05 - loss: 0.146 - ETA: 2:03 - loss: 0.146 - ETA: 2:02 - loss: 0.146 - ETA: 2:00 - loss: 0.146 - ETA: 1:59 - loss: 0.146 - ETA: 1:58 - loss: 0.146 - ETA: 1:56 - loss: 0.146 - ETA: 1:55 - loss: 0.146 - ETA: 1:53 - loss: 0.146 - ETA: 1:52 - loss: 0.146 - ETA: 1:51 - loss: 0.146 - ETA: 1:49 - loss: 0.146 - ETA: 1:48 - loss: 0.146 - ETA: 1:46 - loss: 0.146 - ETA: 1:45 - loss: 0.146 - ETA: 1:44 - loss: 0.146 - ETA: 1:42 - loss: 0.146 - ETA: 1:41 - loss: 0.146 - ETA: 1:39 - loss: 0.146 - ETA: 1:38 - loss: 0.146 - ETA: 1:37 - loss: 0.147 - ETA: 1:35 - loss: 0.147 - ETA: 1:34 - loss: 0.147 - ETA: 1:32 - loss: 0.146 - ETA: 1:31 - loss: 0.147 - ETA: 1:30 - loss: 0.147 - ETA: 1:28 - loss: 0.147 - ETA: 1:27 - loss: 0.147 - ETA: 1:25 - loss: 0.147 - ETA: 1:24 - loss: 0.147 - ETA: 1:23 - loss: 0.147 - ETA: 1:21 - loss: 0.147 - ETA: 1:20 - loss: 0.147 - ETA: 1:18 - loss: 0.146 - ETA: 1:17 - loss: 0.146 - ETA: 1:15 - loss: 0.146 - ETA: 1:14 - loss: 0.146 - ETA: 1:13 - loss: 0.146 - ETA: 1:11 - loss: 0.146 - ETA: 1:10 - loss: 0.146 - ETA: 1:08 - loss: 0.146 - ETA: 1:07 - loss: 0.146 - ETA: 1:06 - loss: 0.146 - ETA: 1:04 - loss: 0.146 - ETA: 1:03 - loss: 0.146 - ETA: 1:01 - loss: 0.147 - ETA: 1:00 - loss: 0.147 - ETA: 59s - loss: 0.147 - ETA: 57s - loss: 0.14 - ETA: 56s - loss: 0.14 - ETA: 54s - loss: 0.14 - ETA: 53s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 28s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 9s - loss: 0.1463 - ETA: 8s - loss: 0.146 - ETA: 6s - loss: 0.146 - ETA: 5s - loss: 0.146 - ETA: 4s - loss: 0.146 - ETA: 2s - loss: 0.146 - ETA: 1s - loss: 0.146 - 383s 45ms/step - loss: 0.1459 - val_loss: 0.1093\n",
      "Epoch 4/5\n",
      "8540/8540 [==============================] - ETA: 6:05 - loss: 0.141 - ETA: 6:10 - loss: 0.135 - ETA: 6:10 - loss: 0.130 - ETA: 6:08 - loss: 0.135 - ETA: 6:07 - loss: 0.142 - ETA: 6:05 - loss: 0.139 - ETA: 6:04 - loss: 0.142 - ETA: 6:03 - loss: 0.141 - ETA: 6:01 - loss: 0.141 - ETA: 6:00 - loss: 0.139 - ETA: 5:59 - loss: 0.140 - ETA: 5:58 - loss: 0.138 - ETA: 5:56 - loss: 0.137 - ETA: 5:55 - loss: 0.140 - ETA: 5:53 - loss: 0.142 - ETA: 5:52 - loss: 0.144 - ETA: 5:51 - loss: 0.145 - ETA: 5:49 - loss: 0.145 - ETA: 5:48 - loss: 0.146 - ETA: 5:47 - loss: 0.145 - ETA: 5:45 - loss: 0.145 - ETA: 5:44 - loss: 0.144 - ETA: 5:42 - loss: 0.145 - ETA: 5:41 - loss: 0.144 - ETA: 5:40 - loss: 0.143 - ETA: 5:38 - loss: 0.143 - ETA: 5:37 - loss: 0.143 - ETA: 5:35 - loss: 0.144 - ETA: 5:34 - loss: 0.143 - ETA: 5:33 - loss: 0.143 - ETA: 5:31 - loss: 0.142 - ETA: 5:30 - loss: 0.142 - ETA: 5:28 - loss: 0.142 - ETA: 5:27 - loss: 0.141 - ETA: 5:26 - loss: 0.141 - ETA: 5:24 - loss: 0.141 - ETA: 5:23 - loss: 0.140 - ETA: 5:21 - loss: 0.140 - ETA: 5:20 - loss: 0.140 - ETA: 5:19 - loss: 0.140 - ETA: 5:17 - loss: 0.140 - ETA: 5:16 - loss: 0.141 - ETA: 5:15 - loss: 0.140 - ETA: 5:13 - loss: 0.141 - ETA: 5:12 - loss: 0.140 - ETA: 5:10 - loss: 0.140 - ETA: 5:09 - loss: 0.140 - ETA: 5:08 - loss: 0.142 - ETA: 5:06 - loss: 0.141 - ETA: 5:05 - loss: 0.141 - ETA: 5:04 - loss: 0.142 - ETA: 5:02 - loss: 0.142 - ETA: 5:01 - loss: 0.142 - ETA: 4:59 - loss: 0.143 - ETA: 4:58 - loss: 0.143 - ETA: 4:57 - loss: 0.143 - ETA: 4:55 - loss: 0.143 - ETA: 4:54 - loss: 0.143 - ETA: 4:52 - loss: 0.143 - ETA: 4:51 - loss: 0.143 - ETA: 4:49 - loss: 0.143 - ETA: 4:48 - loss: 0.143 - ETA: 4:47 - loss: 0.143 - ETA: 4:45 - loss: 0.143 - ETA: 4:44 - loss: 0.143 - ETA: 4:43 - loss: 0.143 - ETA: 4:41 - loss: 0.142 - ETA: 4:40 - loss: 0.143 - ETA: 4:38 - loss: 0.143 - ETA: 4:37 - loss: 0.143 - ETA: 4:36 - loss: 0.143 - ETA: 4:34 - loss: 0.143 - ETA: 4:33 - loss: 0.143 - ETA: 4:31 - loss: 0.142 - ETA: 4:30 - loss: 0.143 - ETA: 4:29 - loss: 0.143 - ETA: 4:27 - loss: 0.143 - ETA: 4:26 - loss: 0.142 - ETA: 4:24 - loss: 0.143 - ETA: 4:23 - loss: 0.142 - ETA: 4:22 - loss: 0.142 - ETA: 4:20 - loss: 0.143 - ETA: 4:19 - loss: 0.142 - ETA: 4:17 - loss: 0.143 - ETA: 4:16 - loss: 0.143 - ETA: 4:14 - loss: 0.143 - ETA: 4:13 - loss: 0.143 - ETA: 4:12 - loss: 0.142 - ETA: 4:10 - loss: 0.142 - ETA: 4:09 - loss: 0.142 - ETA: 4:07 - loss: 0.142 - ETA: 4:06 - loss: 0.143 - ETA: 4:05 - loss: 0.143 - ETA: 4:03 - loss: 0.143 - ETA: 4:02 - loss: 0.143 - ETA: 4:00 - loss: 0.143 - ETA: 3:59 - loss: 0.143 - ETA: 3:57 - loss: 0.143 - ETA: 3:56 - loss: 0.143 - ETA: 3:55 - loss: 0.143 - ETA: 3:53 - loss: 0.143 - ETA: 3:52 - loss: 0.144 - ETA: 3:50 - loss: 0.143 - ETA: 3:49 - loss: 0.143 - ETA: 3:48 - loss: 0.143 - ETA: 3:46 - loss: 0.143 - ETA: 3:45 - loss: 0.143 - ETA: 3:43 - loss: 0.143 - ETA: 3:42 - loss: 0.144 - ETA: 3:40 - loss: 0.143 - ETA: 3:39 - loss: 0.143 - ETA: 3:38 - loss: 0.143 - ETA: 3:36 - loss: 0.143 - ETA: 3:35 - loss: 0.144 - ETA: 3:33 - loss: 0.143 - ETA: 3:32 - loss: 0.143 - ETA: 3:31 - loss: 0.144 - ETA: 3:29 - loss: 0.143 - ETA: 3:28 - loss: 0.144 - ETA: 3:26 - loss: 0.143 - ETA: 3:25 - loss: 0.143 - ETA: 3:23 - loss: 0.144 - ETA: 3:22 - loss: 0.143 - ETA: 3:21 - loss: 0.143 - ETA: 3:19 - loss: 0.144 - ETA: 3:18 - loss: 0.143 - ETA: 3:16 - loss: 0.143 - ETA: 3:15 - loss: 0.143 - ETA: 3:14 - loss: 0.143 - ETA: 3:12 - loss: 0.143 - ETA: 3:11 - loss: 0.143 - ETA: 3:09 - loss: 0.143 - ETA: 3:08 - loss: 0.144 - ETA: 3:06 - loss: 0.144 - ETA: 3:05 - loss: 0.144 - ETA: 3:04 - loss: 0.144 - ETA: 3:02 - loss: 0.144 - ETA: 3:01 - loss: 0.144 - ETA: 2:59 - loss: 0.144 - ETA: 2:58 - loss: 0.144 - ETA: 2:57 - loss: 0.144 - ETA: 2:55 - loss: 0.144 - ETA: 2:54 - loss: 0.144 - ETA: 2:52 - loss: 0.144 - ETA: 2:51 - loss: 0.144 - ETA: 2:50 - loss: 0.144 - ETA: 2:48 - loss: 0.144 - ETA: 2:47 - loss: 0.144 - ETA: 2:45 - loss: 0.144 - ETA: 2:44 - loss: 0.144 - ETA: 2:43 - loss: 0.144 - ETA: 2:41 - loss: 0.144 - ETA: 2:40 - loss: 0.143 - ETA: 2:38 - loss: 0.144 - ETA: 2:37 - loss: 0.144 - ETA: 2:36 - loss: 0.144 - ETA: 2:34 - loss: 0.144 - ETA: 2:33 - loss: 0.144 - ETA: 2:31 - loss: 0.144 - ETA: 2:30 - loss: 0.144 - ETA: 2:28 - loss: 0.144 - ETA: 2:27 - loss: 0.144 - ETA: 2:26 - loss: 0.144 - ETA: 2:24 - loss: 0.144 - ETA: 2:23 - loss: 0.144 - ETA: 2:21 - loss: 0.144 - ETA: 2:20 - loss: 0.144 - ETA: 2:19 - loss: 0.144 - ETA: 2:17 - loss: 0.144 - ETA: 2:16 - loss: 0.144 - ETA: 2:14 - loss: 0.144 - ETA: 2:13 - loss: 0.144 - ETA: 2:12 - loss: 0.144 - ETA: 2:10 - loss: 0.144 - ETA: 2:09 - loss: 0.144 - ETA: 2:07 - loss: 0.144 - ETA: 2:06 - loss: 0.144 - ETA: 2:05 - loss: 0.144 - ETA: 2:03 - loss: 0.144 - ETA: 2:02 - loss: 0.144 - ETA: 2:00 - loss: 0.144 - ETA: 1:59 - loss: 0.144 - ETA: 1:57 - loss: 0.144 - ETA: 1:56 - loss: 0.144 - ETA: 1:55 - loss: 0.144 - ETA: 1:53 - loss: 0.144 - ETA: 1:52 - loss: 0.144 - ETA: 1:51 - loss: 0.144 - ETA: 1:49 - loss: 0.144 - ETA: 1:48 - loss: 0.144 - ETA: 1:46 - loss: 0.144 - ETA: 1:45 - loss: 0.144 - ETA: 1:43 - loss: 0.144 - ETA: 1:42 - loss: 0.144 - ETA: 1:41 - loss: 0.144 - ETA: 1:39 - loss: 0.144 - ETA: 1:38 - loss: 0.143 - ETA: 1:36 - loss: 0.144 - ETA: 1:35 - loss: 0.144 - ETA: 1:34 - loss: 0.144 - ETA: 1:32 - loss: 0.143 - ETA: 1:31 - loss: 0.143 - ETA: 1:29 - loss: 0.143 - ETA: 1:28 - loss: 0.143 - ETA: 1:27 - loss: 0.143 - ETA: 1:25 - loss: 0.143 - ETA: 1:24 - loss: 0.144 - ETA: 1:22 - loss: 0.144 - ETA: 1:21 - loss: 0.144 - ETA: 1:20 - loss: 0.144 - ETA: 1:18 - loss: 0.144 - ETA: 1:17 - loss: 0.144 - ETA: 1:15 - loss: 0.144 - ETA: 1:14 - loss: 0.144 - ETA: 1:13 - loss: 0.144 - ETA: 1:11 - loss: 0.144 - ETA: 1:10 - loss: 0.144 - ETA: 1:08 - loss: 0.144 - ETA: 1:07 - loss: 0.144 - ETA: 1:05 - loss: 0.144 - ETA: 1:04 - loss: 0.144 - ETA: 1:03 - loss: 0.144 - ETA: 1:01 - loss: 0.144 - ETA: 1:00 - loss: 0.144 - ETA: 58s - loss: 0.144 - ETA: 57s - loss: 0.14 - ETA: 56s - loss: 0.14 - ETA: 54s - loss: 0.14 - ETA: 53s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 9s - loss: 0.1443 - ETA: 8s - loss: 0.144 - ETA: 6s - loss: 0.144 - ETA: 5s - loss: 0.144 - ETA: 4s - loss: 0.144 - ETA: 2s - loss: 0.144 - ETA: 1s - loss: 0.144 - 382s 45ms/step - loss: 0.1442 - val_loss: 0.1098\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540/8540 [==============================] - ETA: 6:12 - loss: 0.105 - ETA: 6:24 - loss: 0.121 - ETA: 6:18 - loss: 0.130 - ETA: 6:13 - loss: 0.133 - ETA: 6:12 - loss: 0.134 - ETA: 6:10 - loss: 0.135 - ETA: 6:09 - loss: 0.137 - ETA: 6:07 - loss: 0.136 - ETA: 6:06 - loss: 0.135 - ETA: 6:04 - loss: 0.136 - ETA: 6:02 - loss: 0.138 - ETA: 6:01 - loss: 0.135 - ETA: 5:59 - loss: 0.135 - ETA: 5:58 - loss: 0.136 - ETA: 5:56 - loss: 0.136 - ETA: 5:54 - loss: 0.137 - ETA: 5:53 - loss: 0.139 - ETA: 5:51 - loss: 0.140 - ETA: 5:50 - loss: 0.140 - ETA: 5:48 - loss: 0.141 - ETA: 5:47 - loss: 0.143 - ETA: 5:45 - loss: 0.142 - ETA: 5:43 - loss: 0.143 - ETA: 5:42 - loss: 0.143 - ETA: 5:41 - loss: 0.143 - ETA: 5:39 - loss: 0.144 - ETA: 5:37 - loss: 0.146 - ETA: 5:36 - loss: 0.147 - ETA: 5:35 - loss: 0.147 - ETA: 5:33 - loss: 0.148 - ETA: 5:31 - loss: 0.147 - ETA: 5:30 - loss: 0.146 - ETA: 5:28 - loss: 0.146 - ETA: 5:27 - loss: 0.146 - ETA: 5:26 - loss: 0.147 - ETA: 5:24 - loss: 0.147 - ETA: 5:23 - loss: 0.147 - ETA: 5:22 - loss: 0.147 - ETA: 5:21 - loss: 0.146 - ETA: 5:20 - loss: 0.146 - ETA: 5:18 - loss: 0.146 - ETA: 5:17 - loss: 0.146 - ETA: 5:16 - loss: 0.146 - ETA: 5:14 - loss: 0.147 - ETA: 5:13 - loss: 0.147 - ETA: 5:11 - loss: 0.148 - ETA: 5:10 - loss: 0.148 - ETA: 5:08 - loss: 0.147 - ETA: 5:07 - loss: 0.147 - ETA: 5:06 - loss: 0.147 - ETA: 5:04 - loss: 0.146 - ETA: 5:03 - loss: 0.146 - ETA: 5:01 - loss: 0.146 - ETA: 5:00 - loss: 0.146 - ETA: 4:59 - loss: 0.146 - ETA: 4:57 - loss: 0.146 - ETA: 4:56 - loss: 0.146 - ETA: 4:54 - loss: 0.146 - ETA: 4:53 - loss: 0.146 - ETA: 4:51 - loss: 0.146 - ETA: 4:50 - loss: 0.146 - ETA: 4:48 - loss: 0.146 - ETA: 4:47 - loss: 0.146 - ETA: 4:46 - loss: 0.146 - ETA: 4:44 - loss: 0.145 - ETA: 4:43 - loss: 0.145 - ETA: 4:41 - loss: 0.145 - ETA: 4:40 - loss: 0.145 - ETA: 4:38 - loss: 0.145 - ETA: 4:37 - loss: 0.144 - ETA: 4:36 - loss: 0.145 - ETA: 4:34 - loss: 0.146 - ETA: 4:33 - loss: 0.146 - ETA: 4:31 - loss: 0.146 - ETA: 4:30 - loss: 0.146 - ETA: 4:28 - loss: 0.146 - ETA: 4:27 - loss: 0.146 - ETA: 4:26 - loss: 0.147 - ETA: 4:24 - loss: 0.147 - ETA: 4:23 - loss: 0.146 - ETA: 4:21 - loss: 0.146 - ETA: 4:20 - loss: 0.147 - ETA: 4:18 - loss: 0.147 - ETA: 4:17 - loss: 0.146 - ETA: 4:16 - loss: 0.147 - ETA: 4:14 - loss: 0.146 - ETA: 4:13 - loss: 0.147 - ETA: 4:12 - loss: 0.147 - ETA: 4:10 - loss: 0.147 - ETA: 4:09 - loss: 0.147 - ETA: 4:07 - loss: 0.147 - ETA: 4:06 - loss: 0.147 - ETA: 4:04 - loss: 0.147 - ETA: 4:03 - loss: 0.147 - ETA: 4:02 - loss: 0.147 - ETA: 4:00 - loss: 0.147 - ETA: 3:59 - loss: 0.147 - ETA: 3:57 - loss: 0.147 - ETA: 3:56 - loss: 0.147 - ETA: 3:54 - loss: 0.147 - ETA: 3:53 - loss: 0.147 - ETA: 3:52 - loss: 0.147 - ETA: 3:50 - loss: 0.147 - ETA: 3:49 - loss: 0.147 - ETA: 3:47 - loss: 0.147 - ETA: 3:46 - loss: 0.147 - ETA: 3:45 - loss: 0.146 - ETA: 3:43 - loss: 0.146 - ETA: 3:42 - loss: 0.146 - ETA: 3:40 - loss: 0.146 - ETA: 3:39 - loss: 0.146 - ETA: 3:38 - loss: 0.146 - ETA: 3:36 - loss: 0.146 - ETA: 3:35 - loss: 0.145 - ETA: 3:33 - loss: 0.145 - ETA: 3:32 - loss: 0.145 - ETA: 3:30 - loss: 0.145 - ETA: 3:29 - loss: 0.146 - ETA: 3:28 - loss: 0.145 - ETA: 3:26 - loss: 0.145 - ETA: 3:25 - loss: 0.146 - ETA: 3:23 - loss: 0.146 - ETA: 3:22 - loss: 0.146 - ETA: 3:21 - loss: 0.146 - ETA: 3:19 - loss: 0.146 - ETA: 3:18 - loss: 0.145 - ETA: 3:16 - loss: 0.145 - ETA: 3:15 - loss: 0.146 - ETA: 3:14 - loss: 0.145 - ETA: 3:12 - loss: 0.145 - ETA: 3:11 - loss: 0.145 - ETA: 3:09 - loss: 0.145 - ETA: 3:08 - loss: 0.145 - ETA: 3:07 - loss: 0.145 - ETA: 3:05 - loss: 0.145 - ETA: 3:04 - loss: 0.145 - ETA: 3:02 - loss: 0.144 - ETA: 3:01 - loss: 0.144 - ETA: 2:59 - loss: 0.144 - ETA: 2:58 - loss: 0.144 - ETA: 2:57 - loss: 0.144 - ETA: 2:55 - loss: 0.144 - ETA: 2:54 - loss: 0.144 - ETA: 2:52 - loss: 0.144 - ETA: 2:51 - loss: 0.144 - ETA: 2:50 - loss: 0.144 - ETA: 2:48 - loss: 0.144 - ETA: 2:47 - loss: 0.144 - ETA: 2:45 - loss: 0.144 - ETA: 2:44 - loss: 0.144 - ETA: 2:43 - loss: 0.144 - ETA: 2:41 - loss: 0.144 - ETA: 2:40 - loss: 0.144 - ETA: 2:38 - loss: 0.143 - ETA: 2:37 - loss: 0.143 - ETA: 2:35 - loss: 0.143 - ETA: 2:34 - loss: 0.143 - ETA: 2:33 - loss: 0.143 - ETA: 2:31 - loss: 0.143 - ETA: 2:30 - loss: 0.143 - ETA: 2:28 - loss: 0.143 - ETA: 2:27 - loss: 0.143 - ETA: 2:26 - loss: 0.143 - ETA: 2:24 - loss: 0.143 - ETA: 2:23 - loss: 0.143 - ETA: 2:21 - loss: 0.143 - ETA: 2:20 - loss: 0.143 - ETA: 2:19 - loss: 0.143 - ETA: 2:17 - loss: 0.143 - ETA: 2:16 - loss: 0.143 - ETA: 2:14 - loss: 0.143 - ETA: 2:13 - loss: 0.143 - ETA: 2:12 - loss: 0.142 - ETA: 2:10 - loss: 0.143 - ETA: 2:09 - loss: 0.143 - ETA: 2:07 - loss: 0.143 - ETA: 2:06 - loss: 0.143 - ETA: 2:04 - loss: 0.143 - ETA: 2:03 - loss: 0.143 - ETA: 2:02 - loss: 0.143 - ETA: 2:00 - loss: 0.143 - ETA: 1:59 - loss: 0.143 - ETA: 1:57 - loss: 0.143 - ETA: 1:56 - loss: 0.143 - ETA: 1:55 - loss: 0.143 - ETA: 1:53 - loss: 0.143 - ETA: 1:52 - loss: 0.142 - ETA: 1:50 - loss: 0.142 - ETA: 1:49 - loss: 0.142 - ETA: 1:48 - loss: 0.142 - ETA: 1:46 - loss: 0.142 - ETA: 1:45 - loss: 0.142 - ETA: 1:43 - loss: 0.142 - ETA: 1:42 - loss: 0.142 - ETA: 1:41 - loss: 0.142 - ETA: 1:39 - loss: 0.142 - ETA: 1:38 - loss: 0.142 - ETA: 1:36 - loss: 0.142 - ETA: 1:35 - loss: 0.142 - ETA: 1:34 - loss: 0.142 - ETA: 1:32 - loss: 0.142 - ETA: 1:31 - loss: 0.142 - ETA: 1:29 - loss: 0.142 - ETA: 1:28 - loss: 0.142 - ETA: 1:26 - loss: 0.142 - ETA: 1:25 - loss: 0.142 - ETA: 1:24 - loss: 0.142 - ETA: 1:22 - loss: 0.142 - ETA: 1:21 - loss: 0.142 - ETA: 1:19 - loss: 0.142 - ETA: 1:18 - loss: 0.142 - ETA: 1:17 - loss: 0.142 - ETA: 1:15 - loss: 0.142 - ETA: 1:14 - loss: 0.142 - ETA: 1:12 - loss: 0.142 - ETA: 1:11 - loss: 0.142 - ETA: 1:10 - loss: 0.142 - ETA: 1:08 - loss: 0.142 - ETA: 1:07 - loss: 0.142 - ETA: 1:05 - loss: 0.142 - ETA: 1:04 - loss: 0.142 - ETA: 1:03 - loss: 0.142 - ETA: 1:01 - loss: 0.142 - ETA: 1:00 - loss: 0.142 - ETA: 58s - loss: 0.142 - ETA: 57s - loss: 0.14 - ETA: 56s - loss: 0.14 - ETA: 54s - loss: 0.14 - ETA: 53s - loss: 0.14 - ETA: 51s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 26s - loss: 0.14 - ETA: 25s - loss: 0.14 - ETA: 23s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 20s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 11s - loss: 0.14 - ETA: 9s - loss: 0.1428 - ETA: 8s - loss: 0.142 - ETA: 6s - loss: 0.142 - ETA: 5s - loss: 0.142 - ETA: 4s - loss: 0.143 - ETA: 2s - loss: 0.143 - ETA: 1s - loss: 0.143 - 382s 45ms/step - loss: 0.1432 - val_loss: 0.1104\n"
     ]
    }
   ],
   "source": [
    "# model = Sequential()\n",
    "# # model.add(Embedding(max_words, 20, input_length=maxlen))\n",
    "# model.add(Embedding(vocab_size, output_dim=50, input_length=1200))\n",
    "# model.add(LSTM(128))\n",
    "# model.add(Dropout(0.15))\n",
    "# model.add(GlobalMaxPool1D())\n",
    "# model.add(Dense(71, activation='sigmoid'))\n",
    "\n",
    "# # model.summary()\n",
    "\n",
    "\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "# callbacks = [\n",
    "#     ReduceLROnPlateau(),\n",
    "#     EarlyStopping(patience=4),\n",
    "#     ModelCheckpoint(filepath='model-2.h5', save_best_only=True)\n",
    "# ]\n",
    "history = model.fit(padded_docs_train, y_train,\n",
    "                    class_weight='balanced',\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2114, Recall: 0.5668, F1-measure: 0.3080\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2917, Recall: 0.3030, F1-measure: 0.2973\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3903, Recall: 0.1367, F1-measure: 0.2024\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4897, Recall: 0.0079, F1-measure: 0.0155\n",
      "For threshold:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict([padded_docs_test])\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1200, 50)          4939100   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 1200, 128)         91648     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1200, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 71)                4615      \n",
      "=================================================================\n",
      "Total params: 5,084,771\n",
      "Trainable params: 5,084,771\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1micro', verbose=0, save_best_only=True, mode='max')\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "# model.add(LSTM(6119, input_shape=(timesteps, input_dim)))\n",
    "model.add(Embedding(vocab_size, output_dim=50, input_length=1200))\n",
    "# model.add(LSTM(128))\n",
    "model.add(LSTM(128, return_sequences=True))  \n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8540 samples, validate on 949 samples\n",
      "Epoch 1/5\n",
      "8540/8540 [==============================] - ETA: 23:30 - loss: 0.69 - ETA: 17:48 - loss: 0.69 - ETA: 16:00 - loss: 0.69 - ETA: 14:58 - loss: 0.68 - ETA: 14:21 - loss: 0.68 - ETA: 13:55 - loss: 0.68 - ETA: 13:34 - loss: 0.68 - ETA: 13:20 - loss: 0.67 - ETA: 13:10 - loss: 0.67 - ETA: 13:02 - loss: 0.66 - ETA: 12:46 - loss: 0.65 - ETA: 12:37 - loss: 0.64 - ETA: 12:26 - loss: 0.63 - ETA: 12:18 - loss: 0.62 - ETA: 12:10 - loss: 0.61 - ETA: 12:03 - loss: 0.60 - ETA: 11:55 - loss: 0.59 - ETA: 11:49 - loss: 0.58 - ETA: 11:45 - loss: 0.57 - ETA: 11:38 - loss: 0.56 - ETA: 11:35 - loss: 0.56 - ETA: 11:31 - loss: 0.55 - ETA: 11:26 - loss: 0.54 - ETA: 11:20 - loss: 0.53 - ETA: 11:16 - loss: 0.52 - ETA: 11:10 - loss: 0.51 - ETA: 11:05 - loss: 0.50 - ETA: 11:01 - loss: 0.50 - ETA: 10:56 - loss: 0.49 - ETA: 10:51 - loss: 0.48 - ETA: 10:46 - loss: 0.48 - ETA: 10:41 - loss: 0.47 - ETA: 10:37 - loss: 0.46 - ETA: 10:32 - loss: 0.46 - ETA: 10:28 - loss: 0.45 - ETA: 10:24 - loss: 0.44 - ETA: 10:20 - loss: 0.44 - ETA: 10:16 - loss: 0.43 - ETA: 10:12 - loss: 0.43 - ETA: 10:09 - loss: 0.42 - ETA: 10:05 - loss: 0.42 - ETA: 10:01 - loss: 0.41 - ETA: 9:58 - loss: 0.4102 - ETA: 9:54 - loss: 0.406 - ETA: 9:50 - loss: 0.401 - ETA: 9:47 - loss: 0.397 - ETA: 9:43 - loss: 0.392 - ETA: 9:40 - loss: 0.388 - ETA: 9:37 - loss: 0.384 - ETA: 9:33 - loss: 0.380 - ETA: 9:30 - loss: 0.377 - ETA: 9:27 - loss: 0.373 - ETA: 9:24 - loss: 0.370 - ETA: 9:20 - loss: 0.366 - ETA: 9:17 - loss: 0.363 - ETA: 9:14 - loss: 0.359 - ETA: 9:11 - loss: 0.356 - ETA: 9:08 - loss: 0.352 - ETA: 9:06 - loss: 0.350 - ETA: 9:04 - loss: 0.347 - ETA: 9:01 - loss: 0.344 - ETA: 8:59 - loss: 0.341 - ETA: 8:57 - loss: 0.338 - ETA: 8:54 - loss: 0.335 - ETA: 8:51 - loss: 0.333 - ETA: 8:48 - loss: 0.331 - ETA: 8:45 - loss: 0.328 - ETA: 8:42 - loss: 0.326 - ETA: 8:39 - loss: 0.324 - ETA: 8:36 - loss: 0.322 - ETA: 8:33 - loss: 0.320 - ETA: 8:30 - loss: 0.318 - ETA: 8:27 - loss: 0.316 - ETA: 8:24 - loss: 0.314 - ETA: 8:22 - loss: 0.312 - ETA: 8:19 - loss: 0.310 - ETA: 8:16 - loss: 0.308 - ETA: 8:13 - loss: 0.306 - ETA: 8:10 - loss: 0.304 - ETA: 8:07 - loss: 0.302 - ETA: 8:04 - loss: 0.301 - ETA: 8:02 - loss: 0.299 - ETA: 7:59 - loss: 0.297 - ETA: 7:56 - loss: 0.296 - ETA: 7:53 - loss: 0.295 - ETA: 7:50 - loss: 0.293 - ETA: 7:47 - loss: 0.292 - ETA: 7:45 - loss: 0.290 - ETA: 7:42 - loss: 0.288 - ETA: 7:39 - loss: 0.287 - ETA: 7:36 - loss: 0.285 - ETA: 7:33 - loss: 0.284 - ETA: 7:30 - loss: 0.282 - ETA: 7:28 - loss: 0.281 - ETA: 7:25 - loss: 0.280 - ETA: 7:22 - loss: 0.278 - ETA: 7:19 - loss: 0.277 - ETA: 7:17 - loss: 0.276 - ETA: 7:14 - loss: 0.275 - ETA: 7:11 - loss: 0.273 - ETA: 7:08 - loss: 0.272 - ETA: 7:06 - loss: 0.271 - ETA: 7:03 - loss: 0.271 - ETA: 7:00 - loss: 0.270 - ETA: 6:58 - loss: 0.269 - ETA: 6:55 - loss: 0.268 - ETA: 6:52 - loss: 0.267 - ETA: 6:50 - loss: 0.266 - ETA: 6:47 - loss: 0.265 - ETA: 6:44 - loss: 0.264 - ETA: 6:41 - loss: 0.263 - ETA: 6:39 - loss: 0.262 - ETA: 6:36 - loss: 0.262 - ETA: 6:33 - loss: 0.261 - ETA: 6:31 - loss: 0.260 - ETA: 6:28 - loss: 0.259 - ETA: 6:25 - loss: 0.258 - ETA: 6:23 - loss: 0.257 - ETA: 6:20 - loss: 0.256 - ETA: 6:17 - loss: 0.256 - ETA: 6:15 - loss: 0.255 - ETA: 6:12 - loss: 0.254 - ETA: 6:10 - loss: 0.253 - ETA: 6:07 - loss: 0.252 - ETA: 6:05 - loss: 0.251 - ETA: 6:03 - loss: 0.251 - ETA: 6:00 - loss: 0.250 - ETA: 5:58 - loss: 0.249 - ETA: 5:56 - loss: 0.249 - ETA: 5:53 - loss: 0.248 - ETA: 5:51 - loss: 0.248 - ETA: 5:48 - loss: 0.247 - ETA: 5:45 - loss: 0.246 - ETA: 5:43 - loss: 0.245 - ETA: 5:40 - loss: 0.245 - ETA: 5:37 - loss: 0.244 - ETA: 5:35 - loss: 0.244 - ETA: 5:32 - loss: 0.243 - ETA: 5:30 - loss: 0.243 - ETA: 5:28 - loss: 0.242 - ETA: 5:25 - loss: 0.242 - ETA: 5:23 - loss: 0.241 - ETA: 5:20 - loss: 0.241 - ETA: 5:18 - loss: 0.240 - ETA: 5:16 - loss: 0.239 - ETA: 5:13 - loss: 0.239 - ETA: 5:11 - loss: 0.238 - ETA: 5:08 - loss: 0.238 - ETA: 5:06 - loss: 0.237 - ETA: 5:03 - loss: 0.237 - ETA: 5:01 - loss: 0.236 - ETA: 4:58 - loss: 0.235 - ETA: 4:56 - loss: 0.235 - ETA: 4:54 - loss: 0.235 - ETA: 4:51 - loss: 0.234 - ETA: 4:49 - loss: 0.234 - ETA: 4:46 - loss: 0.233 - ETA: 4:43 - loss: 0.233 - ETA: 4:41 - loss: 0.232 - ETA: 4:38 - loss: 0.232 - ETA: 4:35 - loss: 0.231 - ETA: 4:33 - loss: 0.231 - ETA: 4:30 - loss: 0.231 - ETA: 4:27 - loss: 0.230 - ETA: 4:25 - loss: 0.230 - ETA: 4:22 - loss: 0.229 - ETA: 4:19 - loss: 0.229 - ETA: 4:17 - loss: 0.229 - ETA: 4:14 - loss: 0.228 - ETA: 4:11 - loss: 0.228 - ETA: 4:09 - loss: 0.227 - ETA: 4:06 - loss: 0.227 - ETA: 4:03 - loss: 0.226 - ETA: 4:01 - loss: 0.226 - ETA: 3:58 - loss: 0.225 - ETA: 3:55 - loss: 0.225 - ETA: 3:53 - loss: 0.225 - ETA: 3:50 - loss: 0.224 - ETA: 3:47 - loss: 0.224 - ETA: 3:45 - loss: 0.224 - ETA: 3:42 - loss: 0.223 - ETA: 3:40 - loss: 0.223 - ETA: 3:37 - loss: 0.223 - ETA: 3:34 - loss: 0.222 - ETA: 3:32 - loss: 0.222 - ETA: 3:29 - loss: 0.222 - ETA: 3:26 - loss: 0.221 - ETA: 3:24 - loss: 0.221 - ETA: 3:21 - loss: 0.220 - ETA: 3:19 - loss: 0.220 - ETA: 3:16 - loss: 0.220 - ETA: 3:13 - loss: 0.219 - ETA: 3:11 - loss: 0.219 - ETA: 3:08 - loss: 0.219 - ETA: 3:05 - loss: 0.218 - ETA: 3:03 - loss: 0.218 - ETA: 3:00 - loss: 0.218 - ETA: 2:58 - loss: 0.218 - ETA: 2:55 - loss: 0.217 - ETA: 2:52 - loss: 0.217 - ETA: 2:50 - loss: 0.217 - ETA: 2:47 - loss: 0.217 - ETA: 2:45 - loss: 0.216 - ETA: 2:42 - loss: 0.216 - ETA: 2:39 - loss: 0.216 - ETA: 2:37 - loss: 0.216 - ETA: 2:34 - loss: 0.215 - ETA: 2:31 - loss: 0.215 - ETA: 2:29 - loss: 0.215 - ETA: 2:26 - loss: 0.215 - ETA: 2:24 - loss: 0.214 - ETA: 2:21 - loss: 0.214 - ETA: 2:18 - loss: 0.214 - ETA: 2:16 - loss: 0.214 - ETA: 2:13 - loss: 0.213 - ETA: 2:11 - loss: 0.213 - ETA: 2:08 - loss: 0.213 - ETA: 2:06 - loss: 0.212 - ETA: 2:03 - loss: 0.212 - ETA: 2:00 - loss: 0.212 - ETA: 1:58 - loss: 0.212 - ETA: 1:55 - loss: 0.211 - ETA: 1:53 - loss: 0.211 - ETA: 1:50 - loss: 0.211 - ETA: 1:47 - loss: 0.211 - ETA: 1:45 - loss: 0.211 - ETA: 1:42 - loss: 0.210 - ETA: 1:40 - loss: 0.210 - ETA: 1:37 - loss: 0.210 - ETA: 1:34 - loss: 0.210 - ETA: 1:32 - loss: 0.209 - ETA: 1:29 - loss: 0.209 - ETA: 1:27 - loss: 0.209 - ETA: 1:24 - loss: 0.209 - ETA: 1:22 - loss: 0.208 - ETA: 1:19 - loss: 0.208 - ETA: 1:16 - loss: 0.208 - ETA: 1:14 - loss: 0.208 - ETA: 1:11 - loss: 0.207 - ETA: 1:09 - loss: 0.207 - ETA: 1:06 - loss: 0.207 - ETA: 1:03 - loss: 0.207 - ETA: 1:01 - loss: 0.207 - ETA: 58s - loss: 0.207 - ETA: 56s - loss: 0.20 - ETA: 53s - loss: 0.20 - ETA: 51s - loss: 0.20 - ETA: 48s - loss: 0.20 - ETA: 45s - loss: 0.20 - ETA: 43s - loss: 0.20 - ETA: 40s - loss: 0.20 - ETA: 38s - loss: 0.20 - ETA: 35s - loss: 0.20 - ETA: 33s - loss: 0.20 - ETA: 30s - loss: 0.20 - ETA: 27s - loss: 0.20 - ETA: 25s - loss: 0.20 - ETA: 22s - loss: 0.20 - ETA: 20s - loss: 0.20 - ETA: 17s - loss: 0.20 - ETA: 15s - loss: 0.20 - ETA: 12s - loss: 0.20 - ETA: 9s - loss: 0.2029 - ETA: 7s - loss: 0.202 - ETA: 4s - loss: 0.202 - ETA: 2s - loss: 0.202 - 700s 82ms/step - loss: 0.2018 - val_loss: 0.1090\n",
      "Epoch 2/5\n",
      "8540/8540 [==============================] - ETA: 11:11 - loss: 0.14 - ETA: 11:02 - loss: 0.14 - ETA: 10:59 - loss: 0.15 - ETA: 11:03 - loss: 0.16 - ETA: 11:02 - loss: 0.15 - ETA: 11:00 - loss: 0.15 - ETA: 10:58 - loss: 0.15 - ETA: 10:56 - loss: 0.15 - ETA: 10:54 - loss: 0.15 - ETA: 10:52 - loss: 0.15 - ETA: 10:49 - loss: 0.15 - ETA: 10:46 - loss: 0.15 - ETA: 10:44 - loss: 0.15 - ETA: 10:41 - loss: 0.15 - ETA: 10:39 - loss: 0.15 - ETA: 10:36 - loss: 0.15 - ETA: 10:33 - loss: 0.15 - ETA: 10:30 - loss: 0.15 - ETA: 10:27 - loss: 0.15 - ETA: 10:24 - loss: 0.15 - ETA: 10:21 - loss: 0.15 - ETA: 10:20 - loss: 0.15 - ETA: 10:18 - loss: 0.15 - ETA: 10:15 - loss: 0.15 - ETA: 10:12 - loss: 0.15 - ETA: 10:09 - loss: 0.15 - ETA: 10:07 - loss: 0.15 - ETA: 10:06 - loss: 0.15 - ETA: 10:03 - loss: 0.15 - ETA: 10:02 - loss: 0.15 - ETA: 10:02 - loss: 0.15 - ETA: 9:59 - loss: 0.1521 - ETA: 9:56 - loss: 0.153 - ETA: 9:53 - loss: 0.153 - ETA: 9:51 - loss: 0.152 - ETA: 9:48 - loss: 0.152 - ETA: 9:45 - loss: 0.153 - ETA: 9:43 - loss: 0.152 - ETA: 9:40 - loss: 0.151 - ETA: 9:37 - loss: 0.152 - ETA: 9:35 - loss: 0.152 - ETA: 9:32 - loss: 0.152 - ETA: 9:29 - loss: 0.152 - ETA: 9:26 - loss: 0.153 - ETA: 9:23 - loss: 0.153 - ETA: 9:21 - loss: 0.154 - ETA: 9:18 - loss: 0.154 - ETA: 9:16 - loss: 0.155 - ETA: 9:13 - loss: 0.156 - ETA: 9:11 - loss: 0.156 - ETA: 9:09 - loss: 0.156 - ETA: 9:07 - loss: 0.156 - ETA: 9:07 - loss: 0.156 - ETA: 9:06 - loss: 0.156 - ETA: 9:06 - loss: 0.155 - ETA: 9:05 - loss: 0.156 - ETA: 9:05 - loss: 0.156 - ETA: 9:04 - loss: 0.156 - ETA: 9:03 - loss: 0.155 - ETA: 9:02 - loss: 0.156 - ETA: 9:01 - loss: 0.157 - ETA: 9:00 - loss: 0.157 - ETA: 9:00 - loss: 0.157 - ETA: 8:58 - loss: 0.157 - ETA: 8:57 - loss: 0.157 - ETA: 8:55 - loss: 0.158 - ETA: 8:52 - loss: 0.158 - ETA: 8:50 - loss: 0.158 - ETA: 8:47 - loss: 0.158 - ETA: 8:44 - loss: 0.158 - ETA: 8:41 - loss: 0.158 - ETA: 8:38 - loss: 0.158 - ETA: 8:35 - loss: 0.158 - ETA: 8:32 - loss: 0.157 - ETA: 8:29 - loss: 0.157 - ETA: 8:26 - loss: 0.157 - ETA: 8:23 - loss: 0.157 - ETA: 8:20 - loss: 0.157 - ETA: 8:18 - loss: 0.156 - ETA: 8:15 - loss: 0.156 - ETA: 8:12 - loss: 0.156 - ETA: 8:09 - loss: 0.156 - ETA: 8:06 - loss: 0.156 - ETA: 8:03 - loss: 0.156 - ETA: 8:00 - loss: 0.157 - ETA: 7:57 - loss: 0.157 - ETA: 7:54 - loss: 0.157 - ETA: 7:51 - loss: 0.157 - ETA: 7:48 - loss: 0.157 - ETA: 7:45 - loss: 0.156 - ETA: 7:42 - loss: 0.156 - ETA: 7:39 - loss: 0.156 - ETA: 7:36 - loss: 0.156 - ETA: 7:34 - loss: 0.156 - ETA: 7:31 - loss: 0.156 - ETA: 7:28 - loss: 0.156 - ETA: 7:25 - loss: 0.156 - ETA: 7:22 - loss: 0.156 - ETA: 7:19 - loss: 0.156 - ETA: 7:17 - loss: 0.155 - ETA: 7:14 - loss: 0.155 - ETA: 7:11 - loss: 0.155 - ETA: 7:08 - loss: 0.155 - ETA: 7:05 - loss: 0.155 - ETA: 7:02 - loss: 0.155 - ETA: 7:00 - loss: 0.154 - ETA: 6:57 - loss: 0.154 - ETA: 6:54 - loss: 0.154 - ETA: 6:51 - loss: 0.154 - ETA: 6:49 - loss: 0.154 - ETA: 6:46 - loss: 0.154 - ETA: 6:43 - loss: 0.154 - ETA: 6:40 - loss: 0.154 - ETA: 6:38 - loss: 0.154 - ETA: 6:35 - loss: 0.154 - ETA: 6:32 - loss: 0.154 - ETA: 6:29 - loss: 0.154 - ETA: 6:27 - loss: 0.154 - ETA: 6:24 - loss: 0.154 - ETA: 6:21 - loss: 0.154 - ETA: 6:19 - loss: 0.154 - ETA: 6:16 - loss: 0.154 - ETA: 6:13 - loss: 0.154 - ETA: 6:10 - loss: 0.154 - ETA: 6:08 - loss: 0.154 - ETA: 6:05 - loss: 0.154 - ETA: 6:02 - loss: 0.155 - ETA: 6:00 - loss: 0.155 - ETA: 5:57 - loss: 0.155 - ETA: 5:54 - loss: 0.154 - ETA: 5:52 - loss: 0.155 - ETA: 5:49 - loss: 0.154 - ETA: 5:46 - loss: 0.154 - ETA: 5:44 - loss: 0.154 - ETA: 5:41 - loss: 0.154 - ETA: 5:38 - loss: 0.154 - ETA: 5:36 - loss: 0.154 - ETA: 5:33 - loss: 0.153 - ETA: 5:30 - loss: 0.153 - ETA: 5:28 - loss: 0.153 - ETA: 5:25 - loss: 0.153 - ETA: 5:22 - loss: 0.153 - ETA: 5:20 - loss: 0.153 - ETA: 5:17 - loss: 0.153 - ETA: 5:14 - loss: 0.153 - ETA: 5:12 - loss: 0.153 - ETA: 5:09 - loss: 0.153 - ETA: 5:07 - loss: 0.153 - ETA: 5:04 - loss: 0.153 - ETA: 5:01 - loss: 0.153 - ETA: 4:59 - loss: 0.153 - ETA: 4:56 - loss: 0.153 - ETA: 4:53 - loss: 0.153 - ETA: 4:51 - loss: 0.153 - ETA: 4:49 - loss: 0.153 - ETA: 4:48 - loss: 0.153 - ETA: 4:46 - loss: 0.153 - ETA: 4:45 - loss: 0.153 - ETA: 4:43 - loss: 0.153 - ETA: 4:41 - loss: 0.153 - ETA: 4:40 - loss: 0.153 - ETA: 4:38 - loss: 0.153 - ETA: 4:36 - loss: 0.153 - ETA: 4:34 - loss: 0.153 - ETA: 4:33 - loss: 0.153 - ETA: 4:31 - loss: 0.153 - ETA: 4:29 - loss: 0.153 - ETA: 4:27 - loss: 0.153 - ETA: 4:25 - loss: 0.153 - ETA: 4:23 - loss: 0.153 - ETA: 4:21 - loss: 0.153 - ETA: 4:19 - loss: 0.153 - ETA: 4:17 - loss: 0.153 - ETA: 4:15 - loss: 0.153 - ETA: 4:13 - loss: 0.153 - ETA: 4:11 - loss: 0.152 - ETA: 4:09 - loss: 0.152 - ETA: 4:07 - loss: 0.152 - ETA: 4:05 - loss: 0.152 - ETA: 4:02 - loss: 0.152 - ETA: 4:00 - loss: 0.152 - ETA: 3:58 - loss: 0.152 - ETA: 3:56 - loss: 0.152 - ETA: 3:54 - loss: 0.152 - ETA: 3:51 - loss: 0.152 - ETA: 3:49 - loss: 0.152 - ETA: 3:47 - loss: 0.152 - ETA: 3:44 - loss: 0.152 - ETA: 3:42 - loss: 0.152 - ETA: 3:40 - loss: 0.152 - ETA: 3:37 - loss: 0.152 - ETA: 3:35 - loss: 0.152 - ETA: 3:33 - loss: 0.152 - ETA: 3:30 - loss: 0.152 - ETA: 3:28 - loss: 0.152 - ETA: 3:25 - loss: 0.152 - ETA: 3:23 - loss: 0.152 - ETA: 3:20 - loss: 0.152 - ETA: 3:18 - loss: 0.152 - ETA: 3:15 - loss: 0.152 - ETA: 3:13 - loss: 0.152 - ETA: 3:10 - loss: 0.152 - ETA: 3:08 - loss: 0.152 - ETA: 3:05 - loss: 0.152 - ETA: 3:02 - loss: 0.151 - ETA: 3:00 - loss: 0.152 - ETA: 2:57 - loss: 0.151 - ETA: 2:55 - loss: 0.151 - ETA: 2:52 - loss: 0.151 - ETA: 2:49 - loss: 0.151 - ETA: 2:47 - loss: 0.151 - ETA: 2:44 - loss: 0.151 - ETA: 2:41 - loss: 0.152 - ETA: 2:38 - loss: 0.152 - ETA: 2:36 - loss: 0.152 - ETA: 2:33 - loss: 0.152 - ETA: 2:30 - loss: 0.152 - ETA: 2:27 - loss: 0.152 - ETA: 2:25 - loss: 0.151 - ETA: 2:22 - loss: 0.151 - ETA: 2:19 - loss: 0.151 - ETA: 2:16 - loss: 0.151 - ETA: 2:13 - loss: 0.151 - ETA: 2:10 - loss: 0.152 - ETA: 2:08 - loss: 0.151 - ETA: 2:05 - loss: 0.151 - ETA: 2:02 - loss: 0.151 - ETA: 1:59 - loss: 0.151 - ETA: 1:56 - loss: 0.151 - ETA: 1:53 - loss: 0.151 - ETA: 1:50 - loss: 0.151 - ETA: 1:47 - loss: 0.151 - ETA: 1:44 - loss: 0.151 - ETA: 1:41 - loss: 0.151 - ETA: 1:38 - loss: 0.151 - ETA: 1:35 - loss: 0.152 - ETA: 1:32 - loss: 0.151 - ETA: 1:29 - loss: 0.151 - ETA: 1:26 - loss: 0.151 - ETA: 1:23 - loss: 0.151 - ETA: 1:20 - loss: 0.152 - ETA: 1:17 - loss: 0.152 - ETA: 1:14 - loss: 0.152 - ETA: 1:11 - loss: 0.152 - ETA: 1:08 - loss: 0.152 - ETA: 1:05 - loss: 0.152 - ETA: 1:02 - loss: 0.151 - ETA: 59s - loss: 0.151 - ETA: 56s - loss: 0.15 - ETA: 53s - loss: 0.15 - ETA: 50s - loss: 0.15 - ETA: 47s - loss: 0.15 - ETA: 44s - loss: 0.15 - ETA: 40s - loss: 0.15 - ETA: 37s - loss: 0.15 - ETA: 34s - loss: 0.15 - ETA: 31s - loss: 0.15 - ETA: 28s - loss: 0.15 - ETA: 25s - loss: 0.15 - ETA: 21s - loss: 0.15 - ETA: 18s - loss: 0.15 - ETA: 15s - loss: 0.15 - ETA: 12s - loss: 0.15 - ETA: 9s - loss: 0.1519 - ETA: 5s - loss: 0.151 - ETA: 2s - loss: 0.151 - 873s 102ms/step - loss: 0.1519 - val_loss: 0.1099\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540/8540 [==============================] - ETA: 14:50 - loss: 0.14 - ETA: 15:15 - loss: 0.14 - ETA: 15:29 - loss: 0.14 - ETA: 15:33 - loss: 0.14 - ETA: 15:53 - loss: 0.15 - ETA: 16:00 - loss: 0.15 - ETA: 15:53 - loss: 0.15 - ETA: 15:59 - loss: 0.15 - ETA: 15:59 - loss: 0.15 - ETA: 15:48 - loss: 0.15 - ETA: 15:37 - loss: 0.15 - ETA: 15:32 - loss: 0.15 - ETA: 15:22 - loss: 0.15 - ETA: 15:13 - loss: 0.15 - ETA: 15:06 - loss: 0.15 - ETA: 15:03 - loss: 0.15 - ETA: 15:03 - loss: 0.15 - ETA: 15:05 - loss: 0.15 - ETA: 14:57 - loss: 0.15 - ETA: 14:49 - loss: 0.15 - ETA: 14:38 - loss: 0.15 - ETA: 14:28 - loss: 0.15 - ETA: 14:17 - loss: 0.15 - ETA: 14:08 - loss: 0.15 - ETA: 14:01 - loss: 0.15 - ETA: 13:51 - loss: 0.14 - ETA: 13:43 - loss: 0.14 - ETA: 13:35 - loss: 0.14 - ETA: 13:28 - loss: 0.14 - ETA: 13:20 - loss: 0.14 - ETA: 13:14 - loss: 0.15 - ETA: 13:07 - loss: 0.15 - ETA: 13:01 - loss: 0.15 - ETA: 12:56 - loss: 0.15 - ETA: 12:51 - loss: 0.15 - ETA: 12:43 - loss: 0.15 - ETA: 12:35 - loss: 0.14 - ETA: 12:28 - loss: 0.14 - ETA: 12:22 - loss: 0.14 - ETA: 12:16 - loss: 0.14 - ETA: 12:10 - loss: 0.14 - ETA: 12:04 - loss: 0.14 - ETA: 11:57 - loss: 0.15 - ETA: 11:51 - loss: 0.14 - ETA: 11:45 - loss: 0.14 - ETA: 11:39 - loss: 0.14 - ETA: 11:34 - loss: 0.14 - ETA: 11:28 - loss: 0.14 - ETA: 11:22 - loss: 0.14 - ETA: 11:16 - loss: 0.14 - ETA: 11:10 - loss: 0.14 - ETA: 11:04 - loss: 0.14 - ETA: 10:59 - loss: 0.14 - ETA: 10:53 - loss: 0.14 - ETA: 10:48 - loss: 0.14 - ETA: 10:43 - loss: 0.14 - ETA: 10:38 - loss: 0.14 - ETA: 10:33 - loss: 0.14 - ETA: 10:29 - loss: 0.14 - ETA: 10:26 - loss: 0.14 - ETA: 10:21 - loss: 0.14 - ETA: 10:17 - loss: 0.14 - ETA: 10:13 - loss: 0.14 - ETA: 10:09 - loss: 0.14 - ETA: 10:06 - loss: 0.14 - ETA: 10:01 - loss: 0.14 - ETA: 9:57 - loss: 0.1495 - ETA: 9:53 - loss: 0.149 - ETA: 9:48 - loss: 0.149 - ETA: 9:44 - loss: 0.149 - ETA: 9:40 - loss: 0.149 - ETA: 9:36 - loss: 0.148 - ETA: 9:33 - loss: 0.149 - ETA: 9:29 - loss: 0.148 - ETA: 9:26 - loss: 0.149 - ETA: 9:22 - loss: 0.149 - ETA: 9:18 - loss: 0.148 - ETA: 9:15 - loss: 0.148 - ETA: 9:11 - loss: 0.149 - ETA: 9:07 - loss: 0.148 - ETA: 9:03 - loss: 0.148 - ETA: 9:00 - loss: 0.148 - ETA: 8:56 - loss: 0.148 - ETA: 8:52 - loss: 0.147 - ETA: 8:49 - loss: 0.148 - ETA: 8:45 - loss: 0.148 - ETA: 8:41 - loss: 0.148 - ETA: 8:38 - loss: 0.148 - ETA: 8:34 - loss: 0.148 - ETA: 8:31 - loss: 0.148 - ETA: 8:28 - loss: 0.148 - ETA: 8:24 - loss: 0.148 - ETA: 8:21 - loss: 0.148 - ETA: 8:17 - loss: 0.149 - ETA: 8:14 - loss: 0.149 - ETA: 8:11 - loss: 0.149 - ETA: 8:08 - loss: 0.149 - ETA: 8:05 - loss: 0.149 - ETA: 8:02 - loss: 0.149 - ETA: 7:58 - loss: 0.149 - ETA: 7:55 - loss: 0.149 - ETA: 7:52 - loss: 0.149 - ETA: 7:49 - loss: 0.149 - ETA: 7:46 - loss: 0.149 - ETA: 7:43 - loss: 0.149 - ETA: 7:40 - loss: 0.149 - ETA: 7:36 - loss: 0.149 - ETA: 7:33 - loss: 0.149 - ETA: 7:30 - loss: 0.149 - ETA: 7:27 - loss: 0.148 - ETA: 7:24 - loss: 0.149 - ETA: 7:21 - loss: 0.149 - ETA: 7:18 - loss: 0.149 - ETA: 7:14 - loss: 0.149 - ETA: 7:11 - loss: 0.149 - ETA: 7:08 - loss: 0.149 - ETA: 7:05 - loss: 0.149 - ETA: 7:02 - loss: 0.149 - ETA: 6:59 - loss: 0.149 - ETA: 6:57 - loss: 0.149 - ETA: 6:53 - loss: 0.149 - ETA: 6:50 - loss: 0.149 - ETA: 6:47 - loss: 0.149 - ETA: 6:44 - loss: 0.149 - ETA: 6:41 - loss: 0.148 - ETA: 6:38 - loss: 0.148 - ETA: 6:35 - loss: 0.148 - ETA: 6:32 - loss: 0.148 - ETA: 6:29 - loss: 0.149 - ETA: 6:26 - loss: 0.149 - ETA: 6:23 - loss: 0.149 - ETA: 6:20 - loss: 0.149 - ETA: 6:17 - loss: 0.149 - ETA: 6:14 - loss: 0.149 - ETA: 6:11 - loss: 0.149 - ETA: 6:08 - loss: 0.150 - ETA: 6:05 - loss: 0.149 - ETA: 6:02 - loss: 0.149 - ETA: 5:59 - loss: 0.149 - ETA: 5:56 - loss: 0.149 - ETA: 5:54 - loss: 0.150 - ETA: 5:51 - loss: 0.149 - ETA: 5:48 - loss: 0.149 - ETA: 5:45 - loss: 0.149 - ETA: 5:42 - loss: 0.149 - ETA: 5:39 - loss: 0.149 - ETA: 5:36 - loss: 0.149 - ETA: 5:33 - loss: 0.149 - ETA: 5:30 - loss: 0.149 - ETA: 5:27 - loss: 0.149 - ETA: 5:24 - loss: 0.149 - ETA: 5:22 - loss: 0.149 - ETA: 5:19 - loss: 0.149 - ETA: 5:16 - loss: 0.149 - ETA: 5:13 - loss: 0.149 - ETA: 5:10 - loss: 0.149 - ETA: 5:07 - loss: 0.149 - ETA: 5:04 - loss: 0.149 - ETA: 5:01 - loss: 0.149 - ETA: 4:58 - loss: 0.148 - ETA: 4:56 - loss: 0.148 - ETA: 4:53 - loss: 0.148 - ETA: 4:50 - loss: 0.148 - ETA: 4:47 - loss: 0.148 - ETA: 4:44 - loss: 0.148 - ETA: 4:41 - loss: 0.148 - ETA: 4:38 - loss: 0.148 - ETA: 4:36 - loss: 0.148 - ETA: 4:33 - loss: 0.148 - ETA: 4:30 - loss: 0.148 - ETA: 4:27 - loss: 0.148 - ETA: 4:24 - loss: 0.148 - ETA: 4:21 - loss: 0.148 - ETA: 4:19 - loss: 0.148 - ETA: 4:16 - loss: 0.148 - ETA: 4:13 - loss: 0.148 - ETA: 4:10 - loss: 0.148 - ETA: 4:07 - loss: 0.148 - ETA: 4:04 - loss: 0.148 - ETA: 4:02 - loss: 0.148 - ETA: 3:59 - loss: 0.148 - ETA: 3:56 - loss: 0.148 - ETA: 3:53 - loss: 0.148 - ETA: 3:50 - loss: 0.148 - ETA: 3:47 - loss: 0.148 - ETA: 3:45 - loss: 0.148 - ETA: 3:42 - loss: 0.148 - ETA: 3:39 - loss: 0.148 - ETA: 3:36 - loss: 0.148 - ETA: 3:33 - loss: 0.148 - ETA: 3:30 - loss: 0.148 - ETA: 3:28 - loss: 0.148 - ETA: 3:25 - loss: 0.148 - ETA: 3:22 - loss: 0.148 - ETA: 3:19 - loss: 0.148 - ETA: 3:17 - loss: 0.148 - ETA: 3:14 - loss: 0.148 - ETA: 3:12 - loss: 0.148 - ETA: 3:09 - loss: 0.148 - ETA: 3:06 - loss: 0.148 - ETA: 3:04 - loss: 0.148 - ETA: 3:01 - loss: 0.148 - ETA: 2:58 - loss: 0.148 - ETA: 2:55 - loss: 0.148 - ETA: 2:53 - loss: 0.149 - ETA: 2:50 - loss: 0.149 - ETA: 2:47 - loss: 0.149 - ETA: 2:44 - loss: 0.149 - ETA: 2:42 - loss: 0.149 - ETA: 2:39 - loss: 0.149 - ETA: 2:36 - loss: 0.149 - ETA: 2:33 - loss: 0.149 - ETA: 2:30 - loss: 0.149 - ETA: 2:27 - loss: 0.149 - ETA: 2:25 - loss: 0.148 - ETA: 2:22 - loss: 0.148 - ETA: 2:19 - loss: 0.148 - ETA: 2:16 - loss: 0.149 - ETA: 2:13 - loss: 0.148 - ETA: 2:11 - loss: 0.148 - ETA: 2:08 - loss: 0.148 - ETA: 2:05 - loss: 0.148 - ETA: 2:02 - loss: 0.148 - ETA: 1:59 - loss: 0.148 - ETA: 1:56 - loss: 0.148 - ETA: 1:54 - loss: 0.148 - ETA: 1:51 - loss: 0.148 - ETA: 1:48 - loss: 0.148 - ETA: 1:45 - loss: 0.148 - ETA: 1:42 - loss: 0.148 - ETA: 1:39 - loss: 0.148 - ETA: 1:37 - loss: 0.148 - ETA: 1:34 - loss: 0.148 - ETA: 1:31 - loss: 0.148 - ETA: 1:28 - loss: 0.148 - ETA: 1:25 - loss: 0.148 - ETA: 1:22 - loss: 0.148 - ETA: 1:20 - loss: 0.148 - ETA: 1:17 - loss: 0.148 - ETA: 1:14 - loss: 0.148 - ETA: 1:11 - loss: 0.148 - ETA: 1:09 - loss: 0.149 - ETA: 1:06 - loss: 0.149 - ETA: 1:03 - loss: 0.148 - ETA: 1:00 - loss: 0.148 - ETA: 57s - loss: 0.148 - ETA: 55s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 41s - loss: 0.14 - ETA: 38s - loss: 0.14 - ETA: 35s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 16s - loss: 0.14 - ETA: 13s - loss: 0.14 - ETA: 10s - loss: 0.14 - ETA: 7s - loss: 0.1490 - ETA: 5s - loss: 0.148 - ETA: 2s - loss: 0.148 - 749s 88ms/step - loss: 0.1491 - val_loss: 0.1096\n",
      "Epoch 4/5\n",
      "8540/8540 [==============================] - ETA: 11:06 - loss: 0.15 - ETA: 11:04 - loss: 0.14 - ETA: 11:02 - loss: 0.14 - ETA: 11:02 - loss: 0.14 - ETA: 10:59 - loss: 0.15 - ETA: 10:56 - loss: 0.15 - ETA: 10:54 - loss: 0.15 - ETA: 10:51 - loss: 0.14 - ETA: 10:49 - loss: 0.14 - ETA: 10:46 - loss: 0.14 - ETA: 10:45 - loss: 0.14 - ETA: 10:42 - loss: 0.14 - ETA: 10:39 - loss: 0.14 - ETA: 10:37 - loss: 0.14 - ETA: 10:35 - loss: 0.14 - ETA: 10:32 - loss: 0.14 - ETA: 10:29 - loss: 0.14 - ETA: 10:26 - loss: 0.14 - ETA: 10:23 - loss: 0.14 - ETA: 10:21 - loss: 0.14 - ETA: 10:18 - loss: 0.14 - ETA: 10:15 - loss: 0.14 - ETA: 10:13 - loss: 0.14 - ETA: 10:10 - loss: 0.14 - ETA: 10:08 - loss: 0.14 - ETA: 10:05 - loss: 0.14 - ETA: 10:03 - loss: 0.14 - ETA: 10:00 - loss: 0.14 - ETA: 9:57 - loss: 0.1445 - ETA: 9:55 - loss: 0.145 - ETA: 9:52 - loss: 0.145 - ETA: 9:50 - loss: 0.144 - ETA: 9:47 - loss: 0.145 - ETA: 9:44 - loss: 0.146 - ETA: 9:42 - loss: 0.147 - ETA: 9:39 - loss: 0.147 - ETA: 9:37 - loss: 0.146 - ETA: 9:34 - loss: 0.146 - ETA: 9:32 - loss: 0.146 - ETA: 9:29 - loss: 0.146 - ETA: 9:27 - loss: 0.145 - ETA: 9:24 - loss: 0.146 - ETA: 9:22 - loss: 0.145 - ETA: 9:19 - loss: 0.146 - ETA: 9:16 - loss: 0.145 - ETA: 9:14 - loss: 0.145 - ETA: 9:11 - loss: 0.145 - ETA: 9:09 - loss: 0.144 - ETA: 9:06 - loss: 0.145 - ETA: 9:04 - loss: 0.145 - ETA: 9:01 - loss: 0.146 - ETA: 8:59 - loss: 0.146 - ETA: 8:56 - loss: 0.146 - ETA: 8:53 - loss: 0.146 - ETA: 8:51 - loss: 0.147 - ETA: 8:48 - loss: 0.147 - ETA: 8:46 - loss: 0.148 - ETA: 8:43 - loss: 0.148 - ETA: 8:41 - loss: 0.148 - ETA: 8:38 - loss: 0.148 - ETA: 8:36 - loss: 0.148 - ETA: 8:34 - loss: 0.148 - ETA: 8:31 - loss: 0.147 - ETA: 8:29 - loss: 0.147 - ETA: 8:26 - loss: 0.147 - ETA: 8:24 - loss: 0.147 - ETA: 8:21 - loss: 0.147 - ETA: 8:19 - loss: 0.147 - ETA: 8:16 - loss: 0.147 - ETA: 8:14 - loss: 0.147 - ETA: 8:11 - loss: 0.147 - ETA: 8:09 - loss: 0.147 - ETA: 8:06 - loss: 0.147 - ETA: 8:04 - loss: 0.147 - ETA: 8:01 - loss: 0.147 - ETA: 7:59 - loss: 0.147 - ETA: 7:56 - loss: 0.147 - ETA: 7:53 - loss: 0.146 - ETA: 7:51 - loss: 0.147 - ETA: 7:48 - loss: 0.147 - ETA: 7:46 - loss: 0.147 - ETA: 7:43 - loss: 0.147 - ETA: 7:41 - loss: 0.147 - ETA: 7:39 - loss: 0.147 - ETA: 7:36 - loss: 0.147 - ETA: 7:33 - loss: 0.147 - ETA: 7:31 - loss: 0.147 - ETA: 7:29 - loss: 0.147 - ETA: 7:26 - loss: 0.147 - ETA: 7:24 - loss: 0.147 - ETA: 7:21 - loss: 0.147 - ETA: 7:19 - loss: 0.147 - ETA: 7:16 - loss: 0.147 - ETA: 7:14 - loss: 0.147 - ETA: 7:11 - loss: 0.147 - ETA: 7:08 - loss: 0.148 - ETA: 7:06 - loss: 0.148 - ETA: 7:03 - loss: 0.148 - ETA: 7:01 - loss: 0.148 - ETA: 6:58 - loss: 0.148 - ETA: 6:56 - loss: 0.148 - ETA: 6:53 - loss: 0.148 - ETA: 6:51 - loss: 0.148 - ETA: 6:48 - loss: 0.147 - ETA: 6:46 - loss: 0.147 - ETA: 6:43 - loss: 0.147 - ETA: 6:41 - loss: 0.147 - ETA: 6:38 - loss: 0.148 - ETA: 6:36 - loss: 0.147 - ETA: 6:33 - loss: 0.148 - ETA: 6:31 - loss: 0.148 - ETA: 6:28 - loss: 0.148 - ETA: 6:26 - loss: 0.148 - ETA: 6:23 - loss: 0.148 - ETA: 6:21 - loss: 0.148 - ETA: 6:18 - loss: 0.148 - ETA: 6:16 - loss: 0.148 - ETA: 6:13 - loss: 0.148 - ETA: 6:11 - loss: 0.148 - ETA: 6:08 - loss: 0.148 - ETA: 6:06 - loss: 0.148 - ETA: 6:03 - loss: 0.148 - ETA: 6:01 - loss: 0.148 - ETA: 5:58 - loss: 0.148 - ETA: 5:56 - loss: 0.148 - ETA: 5:53 - loss: 0.148 - ETA: 5:51 - loss: 0.148 - ETA: 5:48 - loss: 0.148 - ETA: 5:46 - loss: 0.148 - ETA: 5:43 - loss: 0.148 - ETA: 5:41 - loss: 0.148 - ETA: 5:38 - loss: 0.148 - ETA: 5:36 - loss: 0.147 - ETA: 5:33 - loss: 0.147 - ETA: 5:31 - loss: 0.147 - ETA: 5:28 - loss: 0.147 - ETA: 5:26 - loss: 0.147 - ETA: 5:23 - loss: 0.147 - ETA: 5:21 - loss: 0.147 - ETA: 5:18 - loss: 0.147 - ETA: 5:16 - loss: 0.147 - ETA: 5:13 - loss: 0.147 - ETA: 5:11 - loss: 0.147 - ETA: 5:08 - loss: 0.147 - ETA: 5:06 - loss: 0.147 - ETA: 5:03 - loss: 0.147 - ETA: 5:01 - loss: 0.147 - ETA: 4:58 - loss: 0.147 - ETA: 4:56 - loss: 0.147 - ETA: 4:53 - loss: 0.147 - ETA: 4:51 - loss: 0.147 - ETA: 4:48 - loss: 0.147 - ETA: 4:45 - loss: 0.147 - ETA: 4:43 - loss: 0.147 - ETA: 4:40 - loss: 0.147 - ETA: 4:38 - loss: 0.147 - ETA: 4:35 - loss: 0.147 - ETA: 4:33 - loss: 0.147 - ETA: 4:30 - loss: 0.147 - ETA: 4:28 - loss: 0.147 - ETA: 4:25 - loss: 0.147 - ETA: 4:23 - loss: 0.147 - ETA: 4:20 - loss: 0.147 - ETA: 4:18 - loss: 0.147 - ETA: 4:15 - loss: 0.147 - ETA: 4:13 - loss: 0.148 - ETA: 4:10 - loss: 0.147 - ETA: 4:08 - loss: 0.147 - ETA: 4:05 - loss: 0.148 - ETA: 4:03 - loss: 0.148 - ETA: 4:00 - loss: 0.147 - ETA: 3:58 - loss: 0.148 - ETA: 3:55 - loss: 0.148 - ETA: 3:53 - loss: 0.148 - ETA: 3:50 - loss: 0.148 - ETA: 3:48 - loss: 0.148 - ETA: 3:45 - loss: 0.148 - ETA: 3:43 - loss: 0.148 - ETA: 3:40 - loss: 0.148 - ETA: 3:38 - loss: 0.148 - ETA: 3:35 - loss: 0.148 - ETA: 3:33 - loss: 0.148 - ETA: 3:30 - loss: 0.148 - ETA: 3:28 - loss: 0.148 - ETA: 3:25 - loss: 0.148 - ETA: 3:22 - loss: 0.148 - ETA: 3:20 - loss: 0.148 - ETA: 3:17 - loss: 0.148 - ETA: 3:15 - loss: 0.148 - ETA: 3:12 - loss: 0.148 - ETA: 3:10 - loss: 0.148 - ETA: 3:07 - loss: 0.148 - ETA: 3:05 - loss: 0.148 - ETA: 3:02 - loss: 0.148 - ETA: 3:00 - loss: 0.148 - ETA: 2:57 - loss: 0.148 - ETA: 2:55 - loss: 0.148 - ETA: 2:52 - loss: 0.148 - ETA: 2:50 - loss: 0.147 - ETA: 2:47 - loss: 0.147 - ETA: 2:45 - loss: 0.147 - ETA: 2:42 - loss: 0.147 - ETA: 2:40 - loss: 0.147 - ETA: 2:37 - loss: 0.148 - ETA: 2:35 - loss: 0.147 - ETA: 2:32 - loss: 0.147 - ETA: 2:30 - loss: 0.147 - ETA: 2:27 - loss: 0.147 - ETA: 2:25 - loss: 0.147 - ETA: 2:22 - loss: 0.147 - ETA: 2:20 - loss: 0.147 - ETA: 2:17 - loss: 0.147 - ETA: 2:15 - loss: 0.147 - ETA: 2:12 - loss: 0.147 - ETA: 2:10 - loss: 0.147 - ETA: 2:07 - loss: 0.147 - ETA: 2:05 - loss: 0.147 - ETA: 2:02 - loss: 0.147 - ETA: 2:00 - loss: 0.147 - ETA: 1:57 - loss: 0.147 - ETA: 1:55 - loss: 0.147 - ETA: 1:52 - loss: 0.147 - ETA: 1:50 - loss: 0.147 - ETA: 1:47 - loss: 0.147 - ETA: 1:45 - loss: 0.147 - ETA: 1:42 - loss: 0.147 - ETA: 1:40 - loss: 0.147 - ETA: 1:37 - loss: 0.147 - ETA: 1:35 - loss: 0.147 - ETA: 1:32 - loss: 0.147 - ETA: 1:30 - loss: 0.147 - ETA: 1:27 - loss: 0.147 - ETA: 1:25 - loss: 0.147 - ETA: 1:22 - loss: 0.147 - ETA: 1:20 - loss: 0.147 - ETA: 1:17 - loss: 0.147 - ETA: 1:15 - loss: 0.147 - ETA: 1:12 - loss: 0.147 - ETA: 1:10 - loss: 0.147 - ETA: 1:07 - loss: 0.147 - ETA: 1:05 - loss: 0.147 - ETA: 1:02 - loss: 0.147 - ETA: 1:00 - loss: 0.147 - ETA: 57s - loss: 0.147 - ETA: 55s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 50s - loss: 0.14 - ETA: 47s - loss: 0.14 - ETA: 44s - loss: 0.14 - ETA: 42s - loss: 0.14 - ETA: 39s - loss: 0.14 - ETA: 37s - loss: 0.14 - ETA: 34s - loss: 0.14 - ETA: 32s - loss: 0.14 - ETA: 29s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 22s - loss: 0.14 - ETA: 19s - loss: 0.14 - ETA: 17s - loss: 0.14 - ETA: 14s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 9s - loss: 0.1475 - ETA: 7s - loss: 0.147 - ETA: 4s - loss: 0.147 - ETA: 2s - loss: 0.147 - 686s 80ms/step - loss: 0.1474 - val_loss: 0.1106\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8540/8540 [==============================] - ETA: 11:28 - loss: 0.14 - ETA: 11:19 - loss: 0.15 - ETA: 11:11 - loss: 0.15 - ETA: 11:07 - loss: 0.15 - ETA: 11:04 - loss: 0.15 - ETA: 11:01 - loss: 0.15 - ETA: 10:57 - loss: 0.15 - ETA: 10:55 - loss: 0.14 - ETA: 10:52 - loss: 0.15 - ETA: 10:49 - loss: 0.14 - ETA: 10:47 - loss: 0.15 - ETA: 10:44 - loss: 0.15 - ETA: 10:41 - loss: 0.15 - ETA: 10:38 - loss: 0.15 - ETA: 10:35 - loss: 0.15 - ETA: 10:33 - loss: 0.15 - ETA: 10:30 - loss: 0.14 - ETA: 10:27 - loss: 0.14 - ETA: 10:25 - loss: 0.14 - ETA: 10:22 - loss: 0.14 - ETA: 10:19 - loss: 0.14 - ETA: 10:16 - loss: 0.14 - ETA: 10:14 - loss: 0.14 - ETA: 10:11 - loss: 0.14 - ETA: 10:09 - loss: 0.14 - ETA: 10:06 - loss: 0.14 - ETA: 10:04 - loss: 0.14 - ETA: 10:02 - loss: 0.14 - ETA: 9:59 - loss: 0.1437 - ETA: 9:57 - loss: 0.144 - ETA: 9:54 - loss: 0.145 - ETA: 9:52 - loss: 0.144 - ETA: 9:52 - loss: 0.144 - ETA: 9:49 - loss: 0.145 - ETA: 9:46 - loss: 0.145 - ETA: 9:44 - loss: 0.145 - ETA: 9:41 - loss: 0.145 - ETA: 9:49 - loss: 0.145 - ETA: 9:56 - loss: 0.145 - ETA: 10:01 - loss: 0.14 - ETA: 10:08 - loss: 0.14 - ETA: 10:13 - loss: 0.14 - ETA: 10:17 - loss: 0.14 - ETA: 10:22 - loss: 0.14 - ETA: 10:26 - loss: 0.14 - ETA: 10:29 - loss: 0.14 - ETA: 10:33 - loss: 0.14 - ETA: 10:36 - loss: 0.14 - ETA: 10:39 - loss: 0.14 - ETA: 10:41 - loss: 0.14 - ETA: 10:43 - loss: 0.14 - ETA: 10:45 - loss: 0.14 - ETA: 10:47 - loss: 0.14 - ETA: 10:49 - loss: 0.14 - ETA: 10:50 - loss: 0.14 - ETA: 10:51 - loss: 0.14 - ETA: 10:52 - loss: 0.14 - ETA: 10:52 - loss: 0.14 - ETA: 10:53 - loss: 0.14 - ETA: 10:53 - loss: 0.14 - ETA: 10:53 - loss: 0.14 - ETA: 10:53 - loss: 0.14 - ETA: 10:53 - loss: 0.14 - ETA: 10:53 - loss: 0.14 - ETA: 10:53 - loss: 0.14 - ETA: 10:53 - loss: 0.14 - ETA: 10:52 - loss: 0.14 - ETA: 10:51 - loss: 0.14 - ETA: 10:51 - loss: 0.14 - ETA: 10:50 - loss: 0.14 - ETA: 10:49 - loss: 0.14 - ETA: 10:47 - loss: 0.14 - ETA: 10:47 - loss: 0.14 - ETA: 10:46 - loss: 0.14 - ETA: 10:45 - loss: 0.14 - ETA: 10:43 - loss: 0.14 - ETA: 10:42 - loss: 0.14 - ETA: 10:40 - loss: 0.14 - ETA: 10:39 - loss: 0.14 - ETA: 10:37 - loss: 0.14 - ETA: 10:36 - loss: 0.14 - ETA: 10:34 - loss: 0.14 - ETA: 10:32 - loss: 0.14 - ETA: 10:31 - loss: 0.14 - ETA: 10:29 - loss: 0.14 - ETA: 10:27 - loss: 0.14 - ETA: 10:25 - loss: 0.14 - ETA: 10:23 - loss: 0.14 - ETA: 10:20 - loss: 0.14 - ETA: 10:18 - loss: 0.14 - ETA: 10:16 - loss: 0.14 - ETA: 10:14 - loss: 0.14 - ETA: 10:08 - loss: 0.14 - ETA: 10:04 - loss: 0.14 - ETA: 9:59 - loss: 0.1455 - ETA: 9:55 - loss: 0.145 - ETA: 9:51 - loss: 0.145 - ETA: 9:47 - loss: 0.145 - ETA: 9:43 - loss: 0.145 - ETA: 9:38 - loss: 0.145 - ETA: 9:33 - loss: 0.145 - ETA: 9:29 - loss: 0.145 - ETA: 9:24 - loss: 0.145 - ETA: 9:20 - loss: 0.145 - ETA: 9:15 - loss: 0.145 - ETA: 9:10 - loss: 0.145 - ETA: 9:05 - loss: 0.145 - ETA: 9:01 - loss: 0.145 - ETA: 8:56 - loss: 0.145 - ETA: 8:51 - loss: 0.145 - ETA: 8:47 - loss: 0.145 - ETA: 8:42 - loss: 0.145 - ETA: 8:38 - loss: 0.145 - ETA: 8:33 - loss: 0.145 - ETA: 8:29 - loss: 0.145 - ETA: 8:25 - loss: 0.145 - ETA: 8:21 - loss: 0.145 - ETA: 8:18 - loss: 0.145 - ETA: 8:14 - loss: 0.145 - ETA: 8:10 - loss: 0.145 - ETA: 8:05 - loss: 0.145 - ETA: 8:01 - loss: 0.145 - ETA: 7:57 - loss: 0.145 - ETA: 7:53 - loss: 0.145 - ETA: 7:49 - loss: 0.145 - ETA: 7:45 - loss: 0.145 - ETA: 7:41 - loss: 0.145 - ETA: 7:36 - loss: 0.144 - ETA: 7:33 - loss: 0.144 - ETA: 7:29 - loss: 0.144 - ETA: 7:25 - loss: 0.144 - ETA: 7:21 - loss: 0.144 - ETA: 7:17 - loss: 0.145 - ETA: 7:14 - loss: 0.145 - ETA: 7:10 - loss: 0.145 - ETA: 7:06 - loss: 0.144 - ETA: 7:02 - loss: 0.144 - ETA: 6:58 - loss: 0.144 - ETA: 6:55 - loss: 0.145 - ETA: 6:51 - loss: 0.145 - ETA: 6:47 - loss: 0.145 - ETA: 6:43 - loss: 0.145 - ETA: 6:40 - loss: 0.145 - ETA: 6:38 - loss: 0.145 - ETA: 6:35 - loss: 0.145 - ETA: 6:31 - loss: 0.145 - ETA: 6:28 - loss: 0.145 - ETA: 6:24 - loss: 0.145 - ETA: 6:20 - loss: 0.145 - ETA: 6:17 - loss: 0.145 - ETA: 6:13 - loss: 0.145 - ETA: 6:10 - loss: 0.145 - ETA: 6:06 - loss: 0.145 - ETA: 6:03 - loss: 0.145 - ETA: 5:59 - loss: 0.144 - ETA: 5:56 - loss: 0.144 - ETA: 5:52 - loss: 0.144 - ETA: 5:48 - loss: 0.145 - ETA: 5:45 - loss: 0.145 - ETA: 5:41 - loss: 0.145 - ETA: 5:38 - loss: 0.145 - ETA: 5:34 - loss: 0.145 - ETA: 5:30 - loss: 0.145 - ETA: 5:27 - loss: 0.145 - ETA: 5:23 - loss: 0.145 - ETA: 5:20 - loss: 0.145 - ETA: 5:16 - loss: 0.145 - ETA: 5:12 - loss: 0.145 - ETA: 5:09 - loss: 0.145 - ETA: 5:05 - loss: 0.145 - ETA: 5:02 - loss: 0.145 - ETA: 4:58 - loss: 0.145 - ETA: 4:55 - loss: 0.145 - ETA: 4:51 - loss: 0.145 - ETA: 4:48 - loss: 0.145 - ETA: 4:45 - loss: 0.145 - ETA: 4:41 - loss: 0.145 - ETA: 4:38 - loss: 0.145 - ETA: 4:34 - loss: 0.145 - ETA: 4:31 - loss: 0.145 - ETA: 4:27 - loss: 0.145 - ETA: 4:24 - loss: 0.145 - ETA: 4:21 - loss: 0.145 - ETA: 4:17 - loss: 0.145 - ETA: 4:14 - loss: 0.145 - ETA: 4:10 - loss: 0.145 - ETA: 4:07 - loss: 0.145 - ETA: 4:04 - loss: 0.145 - ETA: 4:00 - loss: 0.145 - ETA: 3:57 - loss: 0.145 - ETA: 3:54 - loss: 0.145 - ETA: 3:50 - loss: 0.145 - ETA: 3:47 - loss: 0.145 - ETA: 3:44 - loss: 0.145 - ETA: 3:41 - loss: 0.145 - ETA: 3:37 - loss: 0.145 - ETA: 3:34 - loss: 0.145 - ETA: 3:31 - loss: 0.145 - ETA: 3:27 - loss: 0.145 - ETA: 3:24 - loss: 0.145 - ETA: 3:21 - loss: 0.145 - ETA: 3:18 - loss: 0.145 - ETA: 3:15 - loss: 0.145 - ETA: 3:12 - loss: 0.145 - ETA: 3:08 - loss: 0.145 - ETA: 3:05 - loss: 0.145 - ETA: 3:02 - loss: 0.145 - ETA: 2:59 - loss: 0.145 - ETA: 2:56 - loss: 0.146 - ETA: 2:53 - loss: 0.146 - ETA: 2:49 - loss: 0.146 - ETA: 2:46 - loss: 0.146 - ETA: 2:43 - loss: 0.146 - ETA: 2:40 - loss: 0.146 - ETA: 2:37 - loss: 0.146 - ETA: 2:34 - loss: 0.146 - ETA: 2:30 - loss: 0.146 - ETA: 2:27 - loss: 0.146 - ETA: 2:24 - loss: 0.146 - ETA: 2:21 - loss: 0.146 - ETA: 2:18 - loss: 0.146 - ETA: 2:15 - loss: 0.146 - ETA: 2:12 - loss: 0.146 - ETA: 2:09 - loss: 0.146 - ETA: 2:05 - loss: 0.146 - ETA: 2:02 - loss: 0.146 - ETA: 1:59 - loss: 0.146 - ETA: 1:56 - loss: 0.146 - ETA: 1:54 - loss: 0.146 - ETA: 1:51 - loss: 0.146 - ETA: 1:48 - loss: 0.146 - ETA: 1:45 - loss: 0.146 - ETA: 1:42 - loss: 0.146 - ETA: 1:39 - loss: 0.146 - ETA: 1:37 - loss: 0.146 - ETA: 1:34 - loss: 0.146 - ETA: 1:31 - loss: 0.146 - ETA: 1:28 - loss: 0.145 - ETA: 1:25 - loss: 0.145 - ETA: 1:22 - loss: 0.145 - ETA: 1:19 - loss: 0.145 - ETA: 1:16 - loss: 0.146 - ETA: 1:13 - loss: 0.146 - ETA: 1:10 - loss: 0.146 - ETA: 1:07 - loss: 0.146 - ETA: 1:04 - loss: 0.146 - ETA: 1:01 - loss: 0.146 - ETA: 58s - loss: 0.146 - ETA: 55s - loss: 0.14 - ETA: 52s - loss: 0.14 - ETA: 49s - loss: 0.14 - ETA: 46s - loss: 0.14 - ETA: 43s - loss: 0.14 - ETA: 40s - loss: 0.14 - ETA: 36s - loss: 0.14 - ETA: 33s - loss: 0.14 - ETA: 30s - loss: 0.14 - ETA: 27s - loss: 0.14 - ETA: 24s - loss: 0.14 - ETA: 21s - loss: 0.14 - ETA: 18s - loss: 0.14 - ETA: 15s - loss: 0.14 - ETA: 12s - loss: 0.14 - ETA: 9s - loss: 0.1468 - ETA: 5s - loss: 0.146 - ETA: 2s - loss: 0.146 - 861s 101ms/step - loss: 0.1468 - val_loss: 0.1116\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "history = model.fit(padded_docs_train, y_train,\n",
    "                    class_weight='balanced',\n",
    "                    epochs=5,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2110, Recall: 0.5624, F1-measure: 0.3069\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2842, Recall: 0.2910, F1-measure: 0.2875\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3304, Recall: 0.2299, F1-measure: 0.2711\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0338, Recall: 0.0013, F1-measure: 0.0026\n",
      "For threshold:  0.5\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0148, Recall: 0.0004, F1-measure: 0.0009\n",
      "For threshold:  0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict([padded_docs_test])\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning:\n",
      "\n",
      "Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=71)`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_dim = (128,)     \n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1micro', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "#convolutional layer 1\n",
    "# classifier.add(Convolution1D(32, kernel_size=3,input_shape = 1200, activation = 'relu'))\n",
    "model.add(Embedding(vocab_size, 71, input_length=1200))\n",
    "# classifier.add(MaxPooling1D(pool_size=2)) \n",
    "classifier.add(Dropout(.20))\n",
    "\n",
    "#convolutional layer 2\n",
    "classifier.add(Convolution1D(32, kernel_size=3, activation = 'sigmoid'))\n",
    "classifier.add(MaxPooling1D(pool_size=2))\n",
    "classifier.add(Dropout(.20))\n",
    "\n",
    "# classifier.add(Convolution1D(32, kernel_size=3, activation = 'sigmoid'))\n",
    "# classifier.add(MaxPooling1D(pool_size=2))\n",
    "# classifier.add(Dropout(.20))\n",
    "\n",
    "#flatten \n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(output_dim =  71, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# history = classifier.fit(padded_docs_train, y_train,\n",
    "#                     class_weight='balanced',\n",
    "#                     epochs=5,\n",
    "#                     batch_size=32,\n",
    "#                     validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1200, 50)          6109750   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 71)                7171      \n",
      "=================================================================\n",
      "Total params: 6,122,021\n",
      "Trainable params: 6,122,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, output_dim=50, input_length=1200))\n",
    "# model.add(layers.Embedding(vocab_size, embedding_dim,  \n",
    "#                            input_length=1200, \n",
    "#                            trainable=True))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(100, activation='sigmoid'))\n",
    "# classifier.add(Dropout(.70))\n",
    "\n",
    "model.add(layers.Dense(71, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam')\n",
    "\n",
    "# history = model.fit(padded_docs_train, y_train,\n",
    "#                     class_weight='balanced',\n",
    "#                     epochs=5,\n",
    "#                     batch_size=32,\n",
    "#                     validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e610965048>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs_train, y_train,\n",
    "                        epochs=10,\n",
    "                        verbose=False,\n",
    "                        validation_data=(padded_docs_test, y_test),\n",
    "                        batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2298, Recall: 0.4838, F1-measure: 0.3116\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3091, Recall: 0.3638, F1-measure: 0.3342\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3663, Recall: 0.2902, F1-measure: 0.3238\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4177, Recall: 0.2345, F1-measure: 0.3004\n",
      "For threshold:  0.5\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4738, Recall: 0.1933, F1-measure: 0.2746\n",
      "For threshold:  0.6\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5327, Recall: 0.1572, F1-measure: 0.2427\n",
      "For threshold:  0.7\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5785, Recall: 0.1246, F1-measure: 0.2050\n",
      "For threshold:  0.8\n",
      "Micro-average quality numbers\n",
      "Precision: 0.6372, Recall: 0.0927, F1-measure: 0.1618\n",
      "For threshold:  0.9\n",
      "Micro-average quality numbers\n",
      "Precision: 0.6852, Recall: 0.0601, F1-measure: 0.1105\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict([padded_docs_test])\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1200, 50)          6109750   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 71)                7171      \n",
      "=================================================================\n",
      "Total params: 6,132,121\n",
      "Trainable params: 6,132,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, output_dim=50, input_length=1200))\n",
    "# model.add(layers.Embedding(vocab_size, embedding_dim,  \n",
    "#                            input_length=1200, \n",
    "#                            trainable=True))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(100, activation='sigmoid'))\n",
    "model.add(Dropout(.70))\n",
    "model.add(layers.Dense(100, activation='sigmoid'))\n",
    "model.add(layers.Dense(71, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e610853470>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs_train, y_train,\n",
    "                        epochs=10,\n",
    "                        verbose=False,\n",
    "                        validation_data=(padded_docs_test, y_test),\n",
    "                        batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2299, Recall: 0.5640, F1-measure: 0.3266\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3369, Recall: 0.3674, F1-measure: 0.3515\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4548, Recall: 0.2483, F1-measure: 0.3212\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5458, Recall: 0.1745, F1-measure: 0.2644\n",
      "For threshold:  0.5\n",
      "Micro-average quality numbers\n",
      "Precision: 0.6115, Recall: 0.1343, F1-measure: 0.2203\n",
      "For threshold:  0.6\n",
      "Micro-average quality numbers\n",
      "Precision: 0.6531, Recall: 0.0927, F1-measure: 0.1623\n",
      "For threshold:  0.7\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7085, Recall: 0.0488, F1-measure: 0.0913\n",
      "For threshold:  0.8\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7125, Recall: 0.0126, F1-measure: 0.0248\n",
      "For threshold:  0.9\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict([padded_docs_test])\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1micro', verbose=0, save_best_only=True, mode='max')\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 71, input_length=1200))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(300, 3, activation='relu'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "# model.add(Conv1D(200, 3, activation='relu'))\n",
    "# model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(71))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1200, 71)          8675845   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1198, 64)          13696     \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1196, 100)         19300     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1194, 100)         30100     \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 1192, 48)          14448     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 57216)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 71)                4062407   \n",
      "=================================================================\n",
      "Total params: 12,815,796\n",
      "Trainable params: 12,815,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_f1micro', verbose=0, save_best_only=True, mode='max')\n",
    "model = Sequential()\n",
    "# model.add(Conv1D(180, 3, input_shape=(timesteps, input_dim), activation='relu'))\n",
    "model.add(Embedding(vocab_size, 71, input_length=1200))\n",
    "model.add(Conv1D(64, 3, activation='sigmoid'))\n",
    "# model.add(Dropout(0.70))\n",
    "model.add(Conv1D(100, 3, activation='sigmoid'))\n",
    "model.add(Conv1D(100, 3, activation='sigmoid'))\n",
    "model.add(Conv1D(48, 3, activation='sigmoid'))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "model.add(Dense(71))\n",
    "# model.add(Dropout(0.70))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e60f7e2978>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs_train, y_train,\n",
    "                        epochs=10,\n",
    "                        verbose=False,\n",
    "                        validation_data=(padded_docs_test, y_test),\n",
    "                        batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1717, Recall: 0.3387, F1-measure: 0.2279\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1717, Recall: 0.3387, F1-measure: 0.2279\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1717, Recall: 0.3387, F1-measure: 0.2279\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1717, Recall: 0.3387, F1-measure: 0.2279\n",
      "For threshold:  0.5\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1717, Recall: 0.3387, F1-measure: 0.2279\n",
      "For threshold:  0.6\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1717, Recall: 0.3387, F1-measure: 0.2279\n",
      "For threshold:  0.7\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1717, Recall: 0.3387, F1-measure: 0.2279\n",
      "For threshold:  0.8\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1717, Recall: 0.3387, F1-measure: 0.2279\n",
      "For threshold:  0.9\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1717, Recall: 0.3387, F1-measure: 0.2279\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict([padded_docs_test])\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1200, 71)          8675845   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1198, 64)          13696     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1198, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 1196, 100)         19300     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1196, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 1194, 100)         30100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1194, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 1192, 48)          14448     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 57216)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 71)                4062407   \n",
      "=================================================================\n",
      "Total params: 12,815,796\n",
      "Trainable params: 12,815,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# model.add(Conv1D(180, 3, input_shape=(timesteps, input_dim), activation='relu'))\n",
    "model.add(Embedding(vocab_size, 71, input_length=1200))\n",
    "model.add(Conv1D(64, 3, activation='sigmoid'))\n",
    "model.add(Dropout(0.70))\n",
    "model.add(Conv1D(100, 3, activation='sigmoid'))\n",
    "model.add(Dropout(0.70))\n",
    "model.add(Conv1D(100, 3, activation='sigmoid'))\n",
    "model.add(Dropout(0.70))\n",
    "model.add(Conv1D(48, 3, activation='sigmoid'))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "model.add(Dense(71))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e6243ca080>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2200, Recall: 0.2893, F1-measure: 0.2499\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2200, Recall: 0.2893, F1-measure: 0.2499\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2200, Recall: 0.2893, F1-measure: 0.2499\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2200, Recall: 0.2893, F1-measure: 0.2499\n",
      "For threshold:  0.5\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2200, Recall: 0.2893, F1-measure: 0.2499\n",
      "For threshold:  0.6\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2200, Recall: 0.2893, F1-measure: 0.2499\n",
      "For threshold:  0.7\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2200, Recall: 0.2893, F1-measure: 0.2499\n",
      "For threshold:  0.8\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2200, Recall: 0.2893, F1-measure: 0.2499\n",
      "For threshold:  0.9\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2200, Recall: 0.2893, F1-measure: 0.2499\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_docs_train, y_train,\n",
    "                        epochs=10,\n",
    "                        verbose=False,\n",
    "                        validation_data=(padded_docs_test, y_test),\n",
    "                        batch_size=16)\n",
    "\n",
    "\n",
    "predictions=model.predict([padded_docs_test])\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1200, 50)          6109750   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1200, 128)         91648     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1200, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 71)                4615      \n",
      "=================================================================\n",
      "Total params: 6,255,421\n",
      "Trainable params: 6,255,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "# model.add(LSTM(6119, input_shape=(timesteps, input_dim)))\n",
    "model.add(Embedding(vocab_size, output_dim=50, input_length=1200))\n",
    "# model.add(LSTM(128))\n",
    "model.add(LSTM(128, return_sequences=True))  \n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(71, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1955d206fd0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2137, Recall: 0.5620, F1-measure: 0.3096\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3483, Recall: 0.2290, F1-measure: 0.2763\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.3894, Recall: 0.1280, F1-measure: 0.1927\n",
      "For threshold:  0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "model.fit(padded_docs_train, y_train,\n",
    "                        epochs=10,\n",
    "                        verbose=False,\n",
    "                        validation_data=(padded_docs_test, y_test),\n",
    "                        batch_size=16)\n",
    "\n",
    "\n",
    "predictions=model.predict([padded_docs_test])\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1200, 71)          8675845   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1198, 64)          13696     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1198, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1196, 100)         19300     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1196, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1194, 100)         30100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1194, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1192, 48)          14448     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 57216)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 71)                4062407   \n",
      "=================================================================\n",
      "Total params: 12,815,796\n",
      "Trainable params: 12,815,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# model.add(Conv1D(180, 3, input_shape=(timesteps, input_dim), activation='relu'))\n",
    "model.add(Embedding(vocab_size, 71, input_length=1200))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Dropout(0.70))\n",
    "model.add(Conv1D(100, 3, activation='relu'))\n",
    "model.add(Dropout(0.70))\n",
    "model.add(Conv1D(100, 3, activation='relu'))\n",
    "model.add(Dropout(0.70))\n",
    "model.add(Conv1D(48, 3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "model.add(Dense(71))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1954ae09a58>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "model.fit(padded_docs_train, y_train,\n",
    "                        epochs=10,\n",
    "                        verbose=False,\n",
    "                        validation_data=(padded_docs_test, y_test),\n",
    "                        batch_size=16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0849, Recall: 0.0824, F1-measure: 0.0836\n",
      "For threshold:  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n",
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning:\n",
      "\n",
      "F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict([padded_docs_test])\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
